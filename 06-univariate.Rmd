# Dynamic Time Series Models {-}

# Linear Autoregression

```{r echo=FALSE, include=FALSE, message=FALSE}
library(ggplot2)
library(data.table)
library(cowplot)
```

Economic time series are often characterized by stochastic cycles. A cycle is a pattern of periodic fluctuations, not contained within a calendar year. A stochastic cycle is one generated by random variables. In general terms, the process is given by:
$$Y_t = f(Y_{t-1},Y_{t-2},\ldots;\mathbf{\theta})+\varepsilon_t.\;~~t=1,\ldots,T$$

An autoregressive process (or, simply, an autoregression) is a regression in which the dependent variable and the regressors belong to the same stochastic process.

An autoregression of order $p$, denoted as $AR(p)$, has the following functional form:
$$y_t = \alpha + \beta_1 y_{t-1}+\beta_2 y_{t-2}+ \cdots + \beta_p y_{t-p}+\varepsilon_t$$

The sum of the autoregressive parameters, $\beta_1,\ldots,\beta_p$, depicts the persistence of the series. The larger is the persistence (i.e., closer it is to one), the longer it takes for the effect of a shock to dissolve. The effect will, eventually, dissolve so long as the series are covariance-stationary. 

The autocorrelation, $\rho$, and partial autocorrelation, $\pi$, functions of the covariance-stationary $AR(p)$ process have the following distinctive features:

- $\rho_1 = \pi_1$, and $\pi_p = \beta_p$.
- The values of $\beta_1,\ldots,\beta_p$ determine the shape of the autocorrelation function (ACF); in any case, the smaller (in absolute terms) is the persistence measure, the faster the ACF decays toward zero.
- The partial autocorrelation function (PACF) is characterized by "statistically significant" first $p$ spikes $\pi_1 \neq 0,\ldots,\pi_p \neq 0$, and the remaining $\pi_k = 0$, $\forall k > p$.


## Modeling

### AR(1)

The first-order autoregression is given by: $$y_t = \alpha + \beta_1 y_{t-1} + \varepsilon_t,$$ where $\alpha$ is a constant term; $\beta_1$ is the *persistence* parameter; and $\varepsilon_t$ is a white noise process.

A necessary and sufficient condition for an $AR(1)$ process to be covariance stationary is that $|\beta_1| < 1$. We can see this by substituting recursively the lagged equations into the lagged dependent variables:
$$
\begin{aligned}
y_t &= \alpha + \beta_1 y_{t-1} + \varepsilon_t \notag \\
y_t &= \alpha + \beta_1 (\alpha + \beta_1 y_{t-2} + \varepsilon_{t-1}) + \varepsilon_t \notag \\
&= \alpha(1+\beta_1) + \beta_1^2 (\alpha + \beta_1 y_{t-3} + \varepsilon_{t-2}) + \beta_1\varepsilon_{t-1} + \varepsilon_t \notag \\
&\vdots  \notag \\
&= \alpha\sum_{i=0}^{k-1}\beta_1^i + \beta_1^k y_{t-k} + \sum_{i=0}^{k-1}\beta_1^i\varepsilon_{t-i}
\end{aligned}
$$
The end-result is a general linear process with geometrically declining coefficients. Here, $|\beta_1| < 1$ is required for convergence.

Assuming $|\beta_1| < 1$, as $k \to \infty$ the process converges to: $$y_t = \frac{\alpha}{1-\beta_1} + \sum_{i=0}^{\infty}\beta_1^i\varepsilon_{t-i}$$

The *unconditional mean* of this process is: $$\mu = E\left(y_t\right) = E\left(\frac{\alpha}{1-\beta_1} + \sum_{i=0}^{\infty}\beta_1^i\varepsilon_{t-i}\right) = \frac{\alpha}{1-\beta_1}$$

The *unconditional variance* of this process is: $$\gamma_0 = Var\left(y_t\right) = Var\left(\frac{\alpha}{1-\beta_1} + \sum_{i=0}^{\infty}\beta_1^i\varepsilon_{t-i}\right) = \frac{\sigma_{\varepsilon}^2}{1-\beta_1^2}$$

The *Autocovariance* is simply the covariance between $y_t$ and $y_{t-k}$, that is: $$\gamma_k = Cov(y_t,y_{t-k}) = E[(y_t - \mu)(y_{t-k} - \mu)] = E(y_t y_{t-k}) - \mu^2$$

Some algebraic manipulation can help us show that: $$\gamma_k = \beta_1\gamma_{k-1},$$ and that: $$\rho_{k} = \beta_1\rho_{k-1}$$ (recall, $\rho_k = \gamma_k/\gamma_0$ is the autocorrelation coefficient).

In fact, for AR(1), an autocorrelation coefficient of some lag can be represented as the autoregression parameter (which in this instance is equivalent to the persistence measure) to that power. That is:
$$
\begin{aligned}
\rho_1 &= \beta_1\rho_0 = \beta_1 \notag \\
\rho_2 &= \beta_1\rho_1 = \beta_1^2 \notag \\
&\vdots \notag \\
\rho_k &= \beta_1\rho_{k-1} = \beta_1^k
\end{aligned}
$$

It follows that the autocorrelation function of a covariance stationary AR(1) is a geometric decay; the smaller is $|\beta_1|$ the more rapid is the decay.

By imposing certain restrictions, the AR(1) will reduce to other already known models:

- If $\beta_1 = 0$, $y_t$ is equivalent to a white noise.
- If $\beta_1 = 1$ and $\alpha = 0$, $y_t$ is a random walk.
- If $\beta_1 = 1$ and $\alpha \neq 0$, $y_t$ is a random walk with drift.

In general, a smaller persistence parameter results in a quicker adjustment to the *unconditional mean* of the process.

The autocorrelation and partial autocorrelation functions of the AR(1) process have three distinctive features:

- $\rho_1 = \pi_1 = \beta_1$. That is, the persistence parameter is also the autocorrelation and the partial autocorrelation coefficient.
- The autocorrelation function decreases exponentially toward zero, and the decay is faster when the persistence parameter is smaller.
- The partial autocorrelation function is characterized by only one spike $\pi_1 \neq 0$, and the remaining $\pi_k = 0$, $\forall k > 1$.

To illustrate the foregoing, let's generate a series of 100 observations that follow the process: $y_t=0.8y_{t-1}+\varepsilon_t$, where $y_0=0$ and $\varepsilon\sim N(0,1)$, and plot the ACF and PACF of this series.

```{r echo=FALSE, message=FALSE}
n <- 100

set.seed(n)
e <- rnorm(n)

y <- e
for(i in 2:n){
  y[i] <- 0.8*y[i-1]+e[i]
}

dt <- data.table(t=c(1:n),y=y)

maxlag <- 12
dt2 <- data.table(k=c(1:maxlag),rhat=c(acf(y,plot=F)[1:maxlag]$acf),phat=c(pacf(y,plot=F)[1:maxlag]$acf))

gg_ts <- ggplot(dt,aes(x=t,y=y))+
  geom_line(size=.8,col="steelblue")+
  labs(y=expression(paste(y[t],sep="")))+
  theme_classic()+
  theme(axis.title = element_text(size=14),axis.text = element_text(size=12))

gg_acf <- ggplot(dt2,aes(x=k,y=rhat))+
  geom_segment(aes(xend=k,yend=0))+
  geom_hline(yintercept=0,size=1)+
  geom_hline(yintercept=c(-1.96/sqrt(n),1.96/sqrt(n)),size=.8,linetype=5,col="indianred")+
  scale_x_continuous(breaks=c(1:maxlag),labels=c(1:maxlag))+
  labs(x="k",y=expression(paste(rho[k],sep="")))+
  coord_cartesian(ylim=c(-1,1))+
  theme_classic()+
  theme(axis.title = element_text(size=14),axis.text = element_text(size=12))

gg_pacf <- ggplot(dt2,aes(x=k,y=phat))+
  geom_segment(aes(xend=k,yend=0))+
  geom_hline(yintercept=0,size=1)+
  geom_hline(yintercept=c(-1.96/sqrt(n),1.96/sqrt(n)),size=.8,linetype=5,col="indianred")+
  scale_x_continuous(breaks=c(1:maxlag),labels=c(1:maxlag))+
  labs(x="k",y=expression(paste(pi[k],sep="")))+
  coord_cartesian(ylim=c(-1,1))+
  theme_classic()+
  theme(axis.title = element_text(size=14),axis.text = element_text(size=12))

plot_grid(gg_ts,gg_acf,gg_pacf,ncol=1)

```


### AR(2)

Now consider the second-order autoregression: $$y_t = \alpha + \beta_1 y_{t-1} + \beta_2 y_{t-2} + \varepsilon_t$$

where $\alpha$ is a constant term; $\beta_1+\beta_2$ is the persistence measure; and $\varepsilon_t$ is a white noise process.

In what follows, the necessary (1 and 2) and sufficient (3 and 4) conditions for an $AR(2)$ process to be covariance stationary are:

1. $|\beta_2| < 1$
2. $|\beta_1| < 2$
3. $\beta_1 + \beta_2 < 1$
4. $\beta_2 - \beta_1 < 1$

The autocorrelation functions of the AR(2) process have the following distinctive features:

- $\rho_1 = \pi_1$ (which is true for any $AR(p)$ process), and $\pi_2 = \beta_2$.
- The autocorrelation function decreases toward zero. The path, however, varies depending on the values of $\beta_1$ and $\beta_2$. Nonetheless, the decay is faster when the persistence measure is smaller.
- The partial autocorrelation function is characterized by only two spikes $\pi_1 \neq 0$ and $\pi_2 \neq 0$, and the remaining $\pi_k = 0$, $\forall k > 2$.

Again, to illustrate, let's generate a series of 100 observations that follow the process: $y_t=1.1y_{t-1}-0.4y_{t-2}+\varepsilon_t$, where $y_{-1}=y_0=0$ and $\varepsilon\sim N(0,1)$, and plot the ACF and PACF of this series.

```{r echo=FALSE, message=FALSE}
n <- 120

set.seed(n)
e <- rnorm(n)

y <- e
y[2] <- 1.1*y[1]+e[1]
for(i in 3:n){
  y[i] <- 1.1*y[i-1]-0.4*y[i-2]+e[i]
}

dt <- data.table(t=c(1:n),y=y)

maxlag <- 12
dt2 <- data.table(k=c(1:maxlag),rhat=c(acf(y,plot=F)[1:maxlag]$acf),phat=c(pacf(y,plot=F)[1:maxlag]$acf))

gg_ts <- ggplot(dt,aes(x=t,y=y))+
  geom_line(size=.8,col="steelblue")+
  labs(y=expression(paste(y[t],sep="")))+
  theme_classic()+
  theme(axis.title = element_text(size=14),axis.text = element_text(size=12))

gg_acf <- ggplot(dt2,aes(x=k,y=rhat))+
  geom_segment(aes(xend=k,yend=0))+
  geom_hline(yintercept=0,size=1)+
  geom_hline(yintercept=c(-1.96/sqrt(n),1.96/sqrt(n)),size=.8,linetype=5,col="indianred")+
  scale_x_continuous(breaks=c(1:maxlag),labels=c(1:maxlag))+
  labs(x="k",y=expression(paste(rho[k],sep="")))+
  coord_cartesian(ylim=c(-1,1))+
  theme_classic()+
  theme(axis.title = element_text(size=14),axis.text = element_text(size=12))

gg_pacf <- ggplot(dt2,aes(x=k,y=phat))+
  geom_segment(aes(xend=k,yend=0))+
  geom_hline(yintercept=0,size=1)+
  geom_hline(yintercept=c(-1.96/sqrt(n),1.96/sqrt(n)),size=.8,linetype=5,col="indianred")+
  scale_x_continuous(breaks=c(1:maxlag),labels=c(1:maxlag))+
  labs(x="k",y=expression(paste(pi[k],sep="")))+
  coord_cartesian(ylim=c(-1,1))+
  theme_classic()+
  theme(axis.title = element_text(size=14),axis.text = element_text(size=12))

plot_grid(gg_ts,gg_acf,gg_pacf,ncol=1)

```


## Forecasting

Making forecasts for some future period, $t+h$, from an AR(p) model that has been fit to the data up to and including period $t+h-1$ can be a straightforward exercise, so long as we have access to the relevant information set. For one-step-ahead forecasts, the information set is readily available. For multi-step-ahead forecasts, we need to 'come up' with the value of the variable that has not been realized yet. For example, when making a two-step-ahead forecast for period $t+2$, we need data from period $t+1$, which is not available at the time when the forecast is made. Instead, we need to use our forecast for period $t+1$. The same applies to forecasts for any subsequent periods in the future. This approach is known as an *iterative* method of forecasting, wherein we make forecast for some period using the available data, then iterate forward by one period and use the most recent forecast to make the next period's forecast, and so on and so forth.


### One-step-ahead forecast from AR(1)

The realization of the random variable in period $t+1$ is: $$y_{t+1} = \alpha + \beta_1 y_{t} + \varepsilon_{t+1}$$

The optimal one-step-ahead forecast is: $$y_{t+1|t} = E(y_{t+1}|\Omega_t) = E(\alpha + \beta_1 y_{t} + \varepsilon_{t+1}) = \alpha + \beta_1 y_{t}$$

The one-step-ahead forecast error is: $$e_{t+1|t} = y_{t+1} - y_{t+1|t} = \alpha + \beta_1 y_t + \varepsilon_{t+1} - (\alpha + \beta_1 y_t) = \varepsilon_{t+1}$$

The one-step-ahead forecast variance: $$\sigma_{t+1|t}^2 = Var(y_{t+1}|\Omega_t) = E(e_{t+1|t}^2) = E(\varepsilon_{t+1}^2) = \sigma_{\varepsilon}^2$$

The one-step-ahead (95%) interval forecast: $$y_{t+1|t} \pm z_{.025}\sigma_{t+1|t} = y_{t+1|t} \pm 1.96\sigma_{\varepsilon}$$


### Two-step-ahead forecast from AR(1)

The realization of the random variable in period $t+2$ is: $$y_{t+2} = \alpha + \beta_1 y_{t+1} + \varepsilon_{t+2}$$

The optimal two-step-ahead forecast is: $$y_{t+2|t} = E(y_{t+2}|\Omega_t) = E(\alpha + \beta_1 y_{t+1} + \varepsilon_{t+2}) = \alpha(1+\beta_1) + \beta_1^2 y_t$$

Note, that here we substituted $y_{t+1}$ with $\alpha + \beta_1 y_t + \varepsilon_{t+1}$.

The two-step-ahead forecast error is:
$$\begin{aligned}
e_{t+2|t} &= y_{t+2} - y_{t+2|t} \\
&= \alpha(1+\beta_1) + \beta_1^2 y_t + \beta_1\varepsilon_{t+1} + \varepsilon_{t+2} - [\alpha(1+\beta_1) + \beta_1^2 y_t] \\
&= \beta_1\varepsilon_{t+1} + \varepsilon_{t+2}
\end{aligned}$$

The two-step-ahead forecast variance is:
$$\begin{aligned}
\sigma_{t+2|t}^2 &= Var(y_{t+2}|\Omega_t) \\
&= E(e_{t+2|t}^2) = E(\beta_1\varepsilon_{t+1} + \varepsilon_{t+2})^2 = \sigma_{\varepsilon}^2(1+\beta_1^2)
\end{aligned}$$

The two-step-ahead (95%) interval forecast: $$y_{t+2|t} \pm z_{.025}\sigma_{t+2|t} = y_{t+2|t} \pm 1.96\sigma_{\varepsilon}\sqrt{1+\beta_1^2}$$


### h-step-ahead forecast from AR(1)

The realization of the random variable in period $t+h$ is: $$y_{t+h} = \alpha + \beta_1 y_{t+h-1} + \varepsilon_{t+h}$$

The optimal h-step-ahead forecast: $$y_{t+h|t} = E(y_{t+h}|\Omega_t) = E(\alpha + \beta_1 y_{t+h-1} + \varepsilon_{t+1}) = \alpha\textstyle\sum_{j=0}^{h-1}\beta_1^j + \beta_1^h y_t$$

The h-step-ahead forecast error: $$e_{t+h|t} = y_{t+h} - y_{t+h|t} = \textstyle\sum_{j=0}^{h-1}\beta_1^j\varepsilon_{t+h-j}$$

The h-step-ahead forecast variance: $$\sigma_{t+h|t}^2 = Var(y_{t+h}|\Omega_t) = E(e_{t+h|t}^2) = \sigma_{\varepsilon}^2\textstyle\sum_{j=0}^{h-1}\beta_1^{2j}$$

The h-step-ahead (95%) interval forecast: $$y_{t+h|t} \pm z_{.025}\sigma_{t+h|t} = y_{t+1|t} \pm 1.96\sigma_{\varepsilon}\sqrt{\textstyle\sum_{j=0}^{h-1}\beta_1^{2j}}$$

If the series represent a covariance-stationary process, i.e. when $|\beta_1| < 1$, as $h \to \infty$:

The optimal point forecast converges to: $$y_{t+h|t} = \frac{\alpha}{1-\beta_1}$$

The forecast variance converges to: $$\sigma_{t+h|t}^2 = \frac{\sigma_{\varepsilon}^2}{1-\beta_1^2}$$

The (95%) interval forecast converges to: $$y_{t+h|t} \pm z_{.025}\sigma_{t+h|t} = \frac{\alpha}{1-\beta_1} \pm 1.96\frac{\sigma_{\varepsilon}}{\sqrt{1-\beta_1^2}}$$


### One-step-ahead forecast from AR(2)

The realization of the random variable in period $t+1$ is: $$y_{t+1} = \alpha + \beta_1 y_{t} + \beta_2 y_{t-1} + \varepsilon_{t+1}$$

The optimal one-step-ahead forecast:
$$\begin{aligned}
y_{t+1|t} &= E(y_{t+1}|\Omega_t) \\
&= E(\alpha + \beta_1 y_{t} + \beta_2 y_{t-1} + \varepsilon_{t+1}) = \alpha + \beta_1 y_{t} + \beta_2 y_{t-1}
\end{aligned}$$

The one-step-ahead forecast error:
$$\begin{aligned}
e_{t+1|t} &= y_{t+1} - y_{t+1|t} \\
&= \alpha + \beta_1 y_t + \beta_2 y_{t-1} + \varepsilon_{t+1} - (\alpha + \beta_1 y_t + \beta_2 y_{t-1}) = \varepsilon_{t+1}
\end{aligned}$$

The one-step-ahead forecast variance: $$\sigma_{t+1|t}^2 = Var(y_{t+1}|\Omega_t) = E(e_{t+1|t}^2) = E(\varepsilon_{t+1}^2) = \sigma_{\varepsilon}^2$$

The one-step-ahead (95%) interval forecast: $$y_{t+1|t} \pm z_{.025}\sigma_{t+1|t} = y_{t+1|t} \pm 1.96\sigma_{\varepsilon}$$


### Two-step-ahead forecast from AR(2)

The realization of the random variable in period $t+2$ is: $$y_{t+2} = \alpha + \beta_1 y_{t+1} + \beta_2 y_{t} + \varepsilon_{t+2}$$

The optimal two-step-ahead forecast:
$$\begin{aligned}
y_{t+2|t} = E(y_{t+2}|\Omega_t) &= E(\alpha + \beta_1 y_{t+1} + \beta_2 y_{t} + \varepsilon_{t+1}) \\
&= \alpha(1+\beta_1) + (\beta_1^2+\beta_2) y_{t} + \beta_1\beta_2 y_{t-1}
\end{aligned}$$

The two-step-ahead forecast error:
$$\begin{aligned}
e_{t+2|t} = y_{t+2} - y_{t+2|t} =& \alpha + \beta_1 y_{t+1} + \beta_2 y_{t} + \varepsilon_{t+2} \\
&- (\alpha + \beta_1 y_{t+1|t} + \beta_2 y_{t}) = \beta_1\varepsilon_{t+1} + \varepsilon_{t+2}
\end{aligned}$$

The two-step-ahead forecast variance:
$$\sigma_{t+2|t}^2 = Var(y_{t+2}|\Omega_t) = E(e_{t+2|t}^2) = E(\beta_1\varepsilon_{t+1} + \varepsilon_{t+2})^2 = \sigma_{\varepsilon}^2(1+\beta_1^2)$$

The two-step-ahead (95%) interval forecast: $$y_{t+2|t} \pm z_{.025}\sigma_{t+2|t} = y_{t+2|t} \pm 1.96\sigma_{\varepsilon}\sqrt{1+\beta_1^2}$$


### One-step-ahead forecast from AR(2)

The realization of the random variable in period $t+h$ is: $$y_{t+h} = \alpha + \beta_1 y_{t+h-1} + \beta_2 y_{t+h-2} + \varepsilon_{t+h}$$

The optimal h-step-ahead forecast (iterated method):
$$\begin{aligned}
y_{t+1|t} &= \alpha + \beta_1 y_t + \beta_2 y_{t-1} \\
y_{t+2|t} &= \alpha + \beta_1 y_{t+1|t} + \beta_2 y_{t} \\
y_{t+3|t} &= \alpha + \beta_1 y_{t+2|t} + \beta_2 y_{t+1|t} \\
&\vdots \\
y_{t+h|t} &= \alpha + \beta_1 y_{t+h-1|t} + \beta_2 y_{t+h-2|t}
\end{aligned}$$

The h-step-ahead forecast error: $$e_{t+h|t} = y_{t+h} - y_{t+h|t} = \varepsilon_{t+h}+\beta_1 e_{t+h-1|t}+\beta_2 e_{t+h-2|t}$$

The h-step-ahead forecast variance:
$$\begin{aligned}
\sigma_{t+h|t}^2 &= Var(y_{t+h}|\Omega_t) = E(e_{t+h|t}^2) \\
&= \sigma_{\varepsilon}^2+\beta_1^2 Var(e_{t+h-1|t})+\beta_2^2 Var(e_{t+h-2|t}) \\
&+2\beta_1\beta_2Cov(e_{t+h-1|t},e_{t+h-2|t})
\end{aligned}$$

(Note: the formulas for $\sigma_{t+1|t}^2,\sigma_{t+2|t}^2,\ldots,\sigma_{t+h|t}^2$ are the same for any $AR(p)$, $p \geq h-1$).

The h-step-ahead (95%) interval forecast: $$y_{t+h|t} \pm z_{.025}\sigma_{t+h|t} = y_{t+1|t} \pm 1.96\sigma_{t+h|t}$$

The optimal h-step-ahead forecast: $$y_{t+h|t} = E(y_{t+h}|\Omega_t) = \alpha + \beta_1 y_{t+h-1|t} + \beta_2 y_{t+h-2|t} + \cdots + \beta_p y_{t+h-p|t}$$

The h-step-ahead forecast error: $$e_{t+h|t} = \varepsilon_{t+h} + \beta_1 e_{t+h-1|t} + \beta_2 e_{t+h-2|t} + \cdots + \beta_p e_{t+h-p|t}$$

The h-step-ahead forecast variance:
$$\begin{aligned}
\sigma_{t+h|t}^2 & = Var(y_{t+h}|\Omega_t) = E(e_{t+2|t}^2) \\
&= \sigma_{\varepsilon}^2 + \sum_{i=1}^{p}\beta_i^2 Var(e_{t+h-i|t}) + 2\sum_{i \neq j}\beta_i\beta_j Cov(e_{t+h-i|t},e_{t+h-j|t})
\end{aligned}$$

The h-step-ahead (95%) interval forecast: $$y_{t+h|t} \pm z_{.025}\sigma_{t+h|t} = y_{t+h|t} \pm 1.96\sigma_{t+h|t}$$
