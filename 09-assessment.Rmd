# Forecast Assessment {-}

# Forecast Evaluation

```{r echo=FALSE, include=FALSE, message=FALSE}
library(ggplot2)
library(data.table)
```

## The Need for the Forecast Evaluation

We typically have several candidate econometric models or methods to forecast an economic variable of interest. Among these, we tend to select the most adequate using in-sample goodness of fit measures (e.g., AIC or SIC). 

A more sensible approach, at least from the standpoint of a forecaster, would be to assess goodness of fit in an out-of-sample setting. Recall that models that offer the superior in-sample fit don't necessarily produce the most accurate out-of-sample forecasts. That is, while a better in-sample fit can be achieved by incorporating additional parameters in the model, such more complex models extrapolate the estimated parameter uncertainty into the forecasts, thus sabotaging their accuracy.

Thus far we have applied the following algorithm to identify 'the best' among the competing forecasts:

- Decide on a loss function (e.g., quadratic loss).
- Obtain forecasts, the forecast errors, and the corresponding sample expected loss (e.g., root mean squared forecast error) for each model in consideration.
- Rank the models according to their sample expected loss values.
- Select the model with the lowest sample expected loss.

But the loss function is a function of a random variable, and in practice we deal with sample information, so sampling variation needs to be taken into the account. Statistical methods of evaluation are, therefore, desirable (at the very least).

## Relative Forecast Accuracy Tests

Here we will cover two tests for the hypothesis that two forecasts are equivalent, in the sense that the associated loss differential is not statistically significantly different from zero.

Consider a time series of length $T$. Suppose $h$-step-ahead forecasts for periods $R+h$ through $T$ have been generated from two competing models $i$ and $j$: $y_{t+h|t,i}$ and $y_{t+h|t,j}$, with corresponding forecast errors: $e_{t+h,i}$ and $e_{t+h,j}$. The null hypothesis of equal predictive ability can be given in terms of the unconditional expectation of the loss differential: $$H_0: E\left[d(e_{t+h,ij})\right] = 0,$$ where $d(e_{t+h,ij}) = L(e_{t+h,i})-L(e_{t+h,j})$.


### The Morgan-Granger-Newbold Test

The Morgan-Granger-Newbold (MGN) test is based on auxiliary variables: $u_{1,t+h} = e_{t+h,i}-e_{t+h,j}$ and $u_{2,t+h} = e_{t+h,i}+e_{t+h,j}$. It follows that: $$E(u_{1,t+h},u_{2,t+h}) = MSFE(i,t+h)-MSFE(j,t+h).$$ Thus, the hypothesis of interest is equivalent to testing whether the two auxiliary variables are correlated.

The MGN test statistic is: $$\frac{r}{\sqrt{(P-1)^{-1}(1-r^2)}}\sim t_{P-1},$$ where $t_{P-1}$ is a Student t distribution with $P-1$ degrees of freedom, $P$ is the number of out-of-sample forecasts, and $$r=\frac{\sum_{t=R}^{T-h}{u_{1,t+h}u_{2,t+h}}}{\sqrt{\sum_{t=R}^{T-h}{u_{1,t+h}^2}\sum_{t=R}^{T-h}{u_{2,t+h}^2}}}$$

The MGN test relies on the assumption that forecast errors (of the forecasts to be compared) are unbiased, normally distributed, and uncorrelated (with each other). These are rather strict assumptions that are, often, violated in empirical applications.
		

### The Diebold-Mariano Test

The Diebold-Mariano (DM) test relaxes the aforementioned requirements on the forecast errors. The DM test statistic is: $$\frac{\bar{d}_h}{\sqrt{\sigma_d^2/P}} \sim N(0,1),$$ where $\bar{d}_h=P^{-1}\sum_{t=R}^{T-h} d(e_{t+h,ij})$, and where $P=T-R-h+1$ is the total number of forecasts generated using a rolling or recursive widnow scheme, for example.

A modified version of the DM statistic, due to Harvey, Leybourne, and Newbold (1998), addresses the finite sample properties of the test, so that: $$\sqrt{\frac{P+1-2h+P^{-1}h(h-1)}{P}}DM\sim t_{P-1},$$ where $t_{P-1}$ is a Student t distribution with $P-1$ degrees of freedom.

In practice, the test of equal predictive ability can be applied within the framework of a regression model as follows: $$d_{t+h} = \delta + \upsilon_{t+h}\;~~t = R,\ldots,T-h,$$ where $d_{t+h} \equiv d(e_{t+h,ij})$. The null of equal predictive ability is equivalent of testing $H_0: \delta = 0$ in the OLS setting. Moreover, because $d_{t+h}$ may be serially correlated, autocorrelation consistent standard errors should be used for inference.