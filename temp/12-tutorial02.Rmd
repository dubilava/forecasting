# Tutorial 2: Data Management and Visualisation{-}

```{r echo=FALSE, include=FALSE, message=FALSE}
library(ggplot2)
library(data.table)
```

In this tutorial, we will introduce several functions of the the `data.table` and `ggplot2` packages, which allow data management and plotting, respectively, in an efficient and aesthetically pleasing way. These two packages are not part of the base R, so we need to load them in the beginning of each session (for the very first time, we will need to download these packages from a repository).

```{r echo=TRUE, message=FALSE}
library(data.table)
library(ggplot2)
```

One of the comparative advantages of R is in its graphing aesthetics. Currently, the best graphs are plotted via the `ggplot2` package. Notably, this package requires that the data are maintained in the `data.frame` or the `data.table` format (i.e., if your data are stored as a matrix, you will need to convert it to one of the two aforementioned formats; the conversion is pretty much seamless).

Let's create a `data.table` object and observe its few lines:
```{r echo=TRUE}
set.seed(1)
x <- runif(120,0,2)
y <- 0.2+0.7*x+rnorm(120)

dt <- data.table(y=y,x=x)
dt
```

So, we have generated^[Notice that prior to sampling we set seed to some value (to `1` in this instance). We do so to ensure that we can exactly replicate the sample in the future.] a dataset that contains two variables (`x` which is a realization of the uniformly distributed random variable, and `y` which consists of a deterministic component, function of `x`, and of an idiosyncratic component), stored in two columns, each of length 120. To get a feel whether there is any relationship between these two variables, we can generate a simple scatterplot.
```{r echo=TRUE, message=FALSE}
ggplot(dt,aes(x=x,y=y))+
  geom_point()
```

We can augment this plot in a number of different ways. Here we change the point color, add the fitted regression line to the plot, add labels to the figure, and change the background theme:
```{r echo=TRUE}
ggplot(dt,aes(x=x,y=y))+
  geom_point(color="coral")+
  geom_smooth(method="lm",formula=y~x,se=F,color="darkgray")+
  labs(title="A scatterplot with a fitted regression line", 
       x="Treatment Variable", 
       y="Outcome Variable", 
       caption="Caption: in case if needed.")+
  theme_classic()
```

As another example, let's generate a histogram (of `y` from the same data):
```{r echo=TRUE}
ggplot(dt,aes(x=y))+
  geom_histogram(color="white",fill="coral",binwidth=.5)+
  labs(title="A basic histogram")+
  theme_classic()
```

It is customary to illustrate a time series via a line plot. So, we add a date column to our dataset, and then plot `y` in the chronological order:
```{r echo=TRUE}
dt$date <- seq(from=as.Date("2000-01-01"),by="month",along.with=y)

ggplot(dt,aes(x=date,y=y))+
  geom_line(color="coral",size=1)+
  labs(title="A basic time series plot",  
       x="Year", 
       y="Outcome Variable")+
  theme_classic()
```


<!-- Let's generate a sequence of 200 iid random variables with mean zero and variance 4, call it e. -->

<!-- ```{r echo=TRUE} -->
<!-- set.seed(1) -->
<!-- e <- rnorm(200,0,2) -->
<!-- ``` -->



<!-- Next, generate a sequence of 200 binary variables, call it x. -->

<!-- ```{r echo=TRUE} -->
<!-- set.seed(2) -->
<!-- x <- sample(c(0,1),200,replace=T) -->
<!-- ``` -->

<!-- Construct a dependent variable, y, using the following formula: $y=2+0.5x+e$. -->

<!-- ```{r echo=TRUE} -->
<!-- y <- 2+0.5*x+e -->
<!-- ``` -->

<!-- Regress y on x, using the `lm()` function, to obtain estimates of the intercept and slope parameters.  -->

<!-- ```{r echo=TRUE} -->
<!-- ols <- lm(y~x) -->
<!-- ols -->
<!-- ``` -->

<!-- Generate some "future" realizations (100 observations) of y. -->

<!-- ```{r echo=TRUE} -->
<!-- set.seed(3) -->
<!-- e <- rnorm(100,0,2) -->

<!-- set.seed(4) -->
<!-- x <- sample(c(0,1),100,replace=T) -->

<!-- y <- 2+0.5*x+e -->

<!-- ``` -->

<!-- Note that these represent actual realizations of the variable; these not forecasts. -->

<!-- Suppose we think that in the considered forecast period, x only takes on 1 (below we will refer to this as the Model 1). Based on this, and using parameter estimates from above, let's generate forecasts for this period. -->

<!-- ```{r echo=TRUE} -->
<!-- y_f1 <- ols$coefficients[1]+ols$coefficients[2]*rep(1,100) -->
<!-- ``` -->

<!-- At this point, we have actual realisations of y and its forecasts. Thus, we can obtain forecast errors, mean absolute forecast errors, and root mean square forecast errors. -->

<!-- ```{r echo=TRUE} -->
<!-- e_f1 <- y-y_f1 -->

<!-- mafe1 <- mean(abs(e_f1)) -->
<!-- rmsfe1 <- sqrt(mean(e_f1^2)) -->

<!-- mafe1 -->
<!-- rmsfe1 -->
<!-- ``` -->


<!-- Suppose, instead, we think that in the considered forecast period x only takes on 0 (below we will refer to this as the Model 2). Based on this, and using parameter estimates from above, let's generate forecasts for this period. -->

<!-- ```{r echo=TRUE} -->
<!-- y_f0 <- ols$coefficients[1]+ols$coefficients[2]*rep(0,100) -->
<!-- ``` -->

<!-- Using these forecasts, obtain forecast errors, mean absolute forecast errors, and root mean square forecast errors. -->

<!-- ```{r echo=TRUE} -->
<!-- e_f0 <- y-y_f0 -->

<!-- mafe0 <- mean(abs(e_f0)) -->
<!-- rmsfe0 <- sqrt(mean(e_f0^2)) -->

<!-- mafe0 -->
<!-- rmsfe0 -->
<!-- ``` -->

<!-- By comparing the two sets of forecasts, we can observe a somewhat rare and yet not an unlikely scenario: MAFE points to the Model 1 as more accurate of the two models, while RMSFE suggests the Model 2 as the more accurate one. More often than not, however, these two accuracy measures tend to agree. -->