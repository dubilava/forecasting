# Deterministic Time Series Models {-}

# Trends

```{r echo=FALSE, include=FALSE, message=FALSE}
library(ggplot2)
library(data.table)
```

Economic time series usually are characterized by trending behavior, and often present a seasonal pattern as well. Trend is a unidirectional change of time series over an extended period of time that arises from the accumulation of information over time. Seasonality is a repeating pattern *within a calendar year* that arises from the links of technologies, preferences, and institutions to the calendar. Modeling and forecasting these time series features is a fairly straightforward task. But before we get to it, let's discuss what may happen if we were to ignore the presence of trends and/or seasonality when analyzing the time series data.

## Spurious Relationship

Nothing about trending time series necessarily violates the classical linear regression model assumptions. The issue may arise, however, if an unobserved trending variable is simultaneously correlated with the dependent variable as well as one of the independent variables in a time series regression. In such case, we may find a (statistically significant) relationship between two or more unrelated economic variables simply because they are all trending. Such relationship is referred to a *spurious relationship*.

To illustrate, consider two trending variables: $$y_t = \gamma t + \nu_t,\;~~\nu\sim N(0,\sigma_{\nu}^2),$$ and $$x_t = \delta t + \upsilon_t,\;~~\upsilon\sim N(0,\sigma_{\upsilon}^2),$$ where $Cov(\nu_t,\upsilon_t) = 0$. For simplicity, we can assume $\sigma_{\nu}^2=\sigma_{\upsilon}^2=1$. Suppose, $\gamma$ and $\delta$ are some positive scalars, say, $0.3$ and $0.5$, respectively. That is, $y$ and $x$ are trending in the same direction. Below is an example of such time series:
```{r echo=FALSE, message=FALSE, cache=TRUE}
r <- 1000
n <- 120
tr <- 1:n

set.seed(1)
y <- 0.3*tr+rnorm(n)
set.seed(n)
x <- 0.5*tr+rnorm(n)

dt <- data.table(tr=tr,x=x,y=y)

ggplot(dt,aes(x=tr)) +
  geom_line(aes(y=x),color="goldenrod",size=.8) +
  geom_line(aes(y=y),color="black",size=.8,linetype=5) +
  labs(x="t",y=expression(paste(y[t],", ",x[t],sep=""))) +
  theme_classic()+
  theme(axis.title = element_text(size=14),axis.text = element_text(size=12))
```

If we were to estimate $$y_t = \alpha+\beta x_t + \varepsilon_t,$$ we are likely to find the relationship between the two -- in this case $\beta>0$ -- even though, we know, the two are not related. To illustrate this, we will generate 1000 samples of size 120 for $y$ and $x$, and in each case we will estimate the parameter $\beta$. The following graph illustrates the empirical distribution of these parameter estimates:
```{r echo=FALSE, message=FALSE, cache=TRUE}
bmat <- matrix(NA,r,1)
for(i in 1:r){
  set.seed(i)
  y <- 0.3*tr+rnorm(n)
  set.seed(r+i)
  x <- 0.5*tr+rnorm(n)
  spur <- lm(y~x)
  bmat[i,] <- spur$coef[2]
}
bmat <- data.table(bmat)
colnames(bmat) <- "b"
ggplot(bmat,aes(x=b)) +
  geom_histogram(color="white",fill="darkgray",alpha=.8,binwidth=0.003) +
  labs(x="b") +
  theme_classic()+
  theme(axis.title = element_text(size=14),axis.text = element_text(size=12))
```

Luckily, we can easily "fix" the issue, by incorporating a trend in the regression: $$y_t = \alpha+\beta x_t + \eta t + \varepsilon_t.$$ Once the trend is accounted for, the previously illustrated "bias" disappears. Using a similar simulation exercise as before, the following graph illustrates the empirical distribution of these parameter estimates:
```{r echo=FALSE, message=FALSE, cache=TRUE}
amat <- matrix(NA,r,1)
for(i in 1:r){
  set.seed(i)
  y <- 0.3*tr+rnorm(n)
  set.seed(r+i)
  x <- 0.5*tr+rnorm(n)
  spur <- lm(y~x+tr)
  amat[i,] <- spur$coef[2]
}
amat <- data.table(amat)
colnames(amat) <- "b"
ggplot(amat,aes(x=b)) +
  geom_histogram(color="white",fill="goldenrod",alpha=.8,binwidth=0.04) +
  labs(x="b") +
  theme_classic()+
  theme(axis.title = element_text(size=14),axis.text = element_text(size=12))
```

In fact, this "fix" is equivalent to regressing a de-trended $y$ on a de-trended $x$. To de-trend a variable, we first run a regression: $y_t = \gamma_0 + \gamma_1 t + \nu_t$, and then obtain the fitted values for some fixed trend (typically zero), that is: $\tilde{y}_t = \hat{\gamma}_0+\hat{\nu}_t$, where $\hat{\gamma}_0$ and $\hat{\nu}_t$ are the parameter estimate and the residuals from the foregoing regression.


## Modeling

As seen, accounting for trends in a time series can help us resolve some regression issues. But a trend in and of itself can be an inherent feature of a times series. To that end, we can apply deterministic trends to forecast time series. 

The simplest (and perhaps most frequently applied) model to account for the trending time series is a *linear* trend model: $$y_t = \alpha + \beta t$$

Other likely candidate trend specifications are *polynomial* (e.g. quadratic, cubic, etc.), *exponential*, and *shifting* (or *switching*) trend models, respectively given by:
$$\begin{aligned}
	y_t &= \alpha + \beta_1 t + \beta_2 t^2 + \ldots + \beta_p t^p \\
	y_t &= e^{\alpha + \beta t}\;~~\mbox{or}\;~~\ln{y_t} = \alpha + \beta t \\
	y_t &= \alpha + \beta_1 t + \beta_2 (t-\tau)I(t>\tau),\;~~\tau\in\mathsf{T}	
	\end{aligned}$$
	
Of these, here we will primarily consider linear and quadratic trends. An exponential trend, from the standpoint of modeling and forecasting, is equivalent to a linear trend fitted to natural logarithm of the series. For a time series $\{y_t: t=1,\ldots,T\}$, the natural logarithm is: $z_t = \ln{y_t}$. Some of the benefits of such a transformation are that:

- they are easier to interpret (relative/percentage change).
- they homogenizes the variance of the time series. 
- they may result in improved forecasting accuracy. 

Exponential trends are suitable when a time series is characterized by a stable relative change over time (e.g., when economic time series grow by 2% every year). 

We will cover the shifting/switching trend models in another chapter.

Trends are (relatively) easy to model and forecast. Caution is needed, however, with (higher order) polynomial trends, as they may fit well in-sample, but cause major problems out-of-sample.

Consider a linear trend model with an additive error term: $$y_t = \alpha + \beta t + \varepsilon_t$$ We estimate the model parameters, $\mathbf{\theta}=\{\alpha,\beta\}$, by fitting the trend model to a time series using the least-squares regression: $$\hat{\theta} = \operatorname*{argmin}_{\mathbf{\theta}} \sum_{t=1}^{T}\big(y_t - \alpha - \beta t\big)^2.$$ Fitted values are then given by: $$\hat{y}_t = \hat{\alpha} + \hat{\beta} t$$


## Forecasting

If a linear trend model is fitted to the data, then any future realization of the stochastic process is assumed to follow the linear trend model: $$y_{t+h} = \alpha + \beta (t+h) + \varepsilon_{t+h}.$$ 

An optimal forecast of $y_{t+h}$, therefore, is given by: $$y_{t+h|t} = E(y_{t+h}|\Omega_t) = E[\alpha + \beta (t+h) + \varepsilon_{t+h}] = \alpha + \beta (t+h).$$ 

The forecast error is: $$e_{t+h|t} = y_{t+h} - y_{t+h|t} = \varepsilon_{t+h}$$ 

The forecast variance, then, is: $$\sigma_{t+h|t}^2 = E(e_{t+h|t}^2) =  E(\varepsilon_{t+h}^2) = \hat{\sigma}^2,\;~~\forall\;h$$

From this, we can obtain interval forecast at any horizon, which is: $$y_{t+h|t} \pm 1.96 \hat{\sigma}.$$

A few features of trend forecasts to note:

- they tend to understate uncertainty (at long horizons as the forecast interval doesn not widen with the horizon);
- short-term trend forecasts can perform poorly; long-term trend forecasts typically perform poorly;
- sometimes it may be beneficial, from the standpoint of achieving better accuracy, to forecast growth rates, and then reconstruct level forecasts.

