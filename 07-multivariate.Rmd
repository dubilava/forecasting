# Vector Autoregression

```{r echo=FALSE, include=FALSE, message=FALSE}
library(ggplot2)
library(data.table)
```

Many economic variables are interrelated. For example, changes to household income impact their consumption levels; changes to interest rates impact investments in the economy. Often (albeit not always) the relationship between the variables goes in both directions. For example, higher wages (and, therefore, income) result in higher prices (inflation), which in turn puts an upward pressure on wages. 

The foregoing implies that a shock to a variable may propagate a dynamic response not only of that variable, but also of related variables. The dynamic linkages between two (or more) economic variables can be modeled as a *system of equations*, represented by a vector autoregressive (VAR) process.


## Modeling

To begin, consider a bivariate (two-dimensional) VAR of order one, VAR(1). 

Let $\{X_{1,t}\}$ and $\{X_{2,t}\}$ be the stationary stochastic processes. A bivariate VAR(1), is then given by:
$$\begin{aligned}
x_{1,t} &= \alpha_1 + \pi_{11}x_{1,t-1} + \pi_{12}x_{2,t-1} + \varepsilon_{1,t} \\
x_{2,t} &= \alpha_2 + \pi_{21}x_{1,t-1} + \pi_{22}x_{2,t-1} + \varepsilon_{2,t}
\end{aligned}$$

where $\varepsilon_{1,t} \sim iid(0,\sigma_1^2)$ and $\varepsilon_{2,t} \sim iid(0,\sigma_2^2)$, and the two can be correlated, i.e., $Cov(\varepsilon_{1,t},\varepsilon_{2,t}) \neq 0$.

To generalize, consider a multivariate ($n$-dimensional) VAR of order $p$, VAR(p), presented in matrix notation: $$\mathbf{x}_t = \mathbf{\alpha} + \Pi_1 \mathbf{x}_{t-1} + \ldots + \Pi_p \mathbf{x}_{t-p} + \mathbf{\varepsilon}_t,$$ where $\mathbf{x}_t = (x_{1,t},\ldots,x_{n,t})'$ is a vector of $n$ (potentially) related variables; $\mathbf{\varepsilon}_t = (\varepsilon_{1,t},\ldots,\varepsilon_{n,t})'$ is a vector of error terms, such that $E(\mathbf{\varepsilon}_t) = \mathbf{0}$, $E(\mathbf{\varepsilon}_t^{}\mathbf{\varepsilon}_t^{\prime}) = \Sigma$, and $E(\mathbf{\varepsilon}_{t}^{}\mathbf{\varepsilon}_{s \neq t}^{\prime}) = 0$. $\Pi_1,\ldots,\Pi_p$ are $n$-dimensional parameter matrices: 
$$\Pi_j = 
	    \left[ 
		\begin{array}{cccc} 
		\pi_{11j} & \pi_{12j} & \cdots &  \pi_{1nj} \\ 
		\pi_{21j} & \pi_{22j} & \cdots &  \pi_{2nj} \\  
		\vdots & \vdots & \ddots &  \vdots \\  
		\pi_{n1j} & \pi_{n2j} & \cdots &  \pi_{nnj} \\  
		\end{array} 
		\right],\;~~j=1,\ldots,p$$
		
When two or more variables are modeled in this way, the implies assumption is that these variables are endogenous to each other; that is, each of the variables affects and is affected by other variables. 

There are three forms of vector autoregressions: structural, recursive, and reduced-form. The structural VAR uses economic theory to impose the 'structure' on correlations of the error terms in the system, thus, facilitate their 'causal' interpretation. The recursive VAR also introduces a structure of some sort, which primarily involves ordering the equations in the system in a specific way so that the error terms in each equation are uncorrelated with those in the preceding equations. To the extent that the 'identifying assumptions' are satisfied, some contemporaneous values (of other variables) appear in the equation of a given variable. The reduced-form VAR makes no claims of causality, instead it only includes lagged values of all the variables in each equation of the system. To the extent that the variables entering the system are, indeed, correlated among each other, the error terms of the reduced-form VAR (typically) are contemporaneously correlated. Here, unless otherwise specified, VAR refers to the reduced-form VAR.

Some of the features of the VAR are that:

- only lagged values of the dependent variables are considered as the right-hand-side variables (although, trend and seasonal variables might also be included in higher-frequency data analysis);
- each equation has the same set of right-hand-side variables (however, it is possible to impose a different lag structure across the equations, especially when $p$ is relatively large, to preserve the degrees of freedom, particularly when the sample size is relatively small and when there are several variables in the system).
- The autregressive order of the VAR, $p$, is determined by the maximum lag of a variable across all equations.

The order of a VAR, $p$, can be determined using system-wide information criteria:

$$\begin{aligned}
& AIC = \ln\left|\Sigma_{\varepsilon}\right| + \frac{2}{T}(pn^2+n) \\
& SIC = \ln\left|\Sigma_{\varepsilon}\right| + \frac{\ln{T}}{T}(pn^2+n)
\end{aligned}$$

where $\left|\Sigma_{\varepsilon}\right|$ is the determinant of the residual covariance matrix; $n$ is the number of equations, and $T$ is the effective sample size.

We can estimate each equation of the VAR individually using OLS.


### In-Sample Granger Causality

A test of joint significance of parameters associated with all the lags of a variable entering the equation of another variable is known as the 'Granger causality' test. The use of the term 'causality' in this context has been criticized. That one variable can help explain the movement of another variable does not necessarily mean that the former causes the latter. To that end the use of the term is misleading, indeed. Rather, it simply means that the former helps predict the latter, and that is, in effect, the meaning of causality in Granger sense.

To illustrate the testing framework, consider a bivariate VAR(p): 
$$\begin{aligned}
x_{1,t} &= \alpha_1 + \pi_{111} x_{1,t-1} + \cdots + \pi_{11p} x_{1,t-p} \\
&+ \pi_{121} x_{2,t-1} + \cdots + \pi_{12p} x_{2,t-p} +\varepsilon_{1,t}  \\
x_{2,t} &= \alpha_1 + \pi_{211} x_{1,t-1} + \cdots + \pi_{21p} x_{1,t-p} \\
&+ \pi_{221} x_{2,t-1} + \cdots + \pi_{22p} x_{2,t-p} +\varepsilon_{2,t} 
\end{aligned}$$

It is said that:
- $\{X_2\}$ does not Granger cause $\{X_1\}$ if $\pi_{121}=\cdots=\pi_{12p}=0$
- $\{X_1\}$ does not Granger cause $\{X_2\}$ if $\pi_{211}=\cdots=\pi_{21p}=0$

So long as the variables of the system are covariance-stationarity, we can apply a F test for the hypotheses testing. If $p=1$, a t test is equivalently applicable for hypotheses testing.


## Forecasting

Generating forecasts for each of the variable comprising the VAR(p) model can be a straightforward exercise, so long as we have access to the relevant information set. As it was the case with autoregressive models, we make one-step-ahead forecasts based on the readily available data; and we make multi-step-ahead forecasts iteratively, using the forecast in periods for which the data are not present.

### One-step-ahead forecast from bivariate VAR(1)

The realizations of the variables in period $t+1$ are: 
$$\begin{aligned}
x_{1,t+1} &= \alpha_1 + \pi_{11} x_{1,t} + \pi_{12} x_{2,t} + \varepsilon_{1,t+1} \\
x_{2,t+1} &= \alpha_2 + \pi_{21} x_{1,t} + \pi_{22} x_{2,t} + \varepsilon_{2,t+1}
\end{aligned}$$

The optimal one-step-ahead forecasts are:
$$\begin{aligned}
x_{1,t+1|t} &= E(x_{1,t+1}|\Omega_t) = \alpha_1 + \pi_{11} x_{1,t} + \pi_{12} x_{2,t} \\
x_{2,t+1|t} &= E(x_{2,t+1}|\Omega_t) = \alpha_2 + \pi_{21} x_{1,t} + \pi_{22} x_{2,t}
\end{aligned}$$


The one-step-ahead forecast errors are: 
$$\begin{aligned}
e_{1,t+1|t} &= x_{1,t+1} - x_{1,t+1|t} = \varepsilon_{1,t+1} \\
e_{2,t+1|t} &= x_{2,t+1} - x_{2,t+1|t} = \varepsilon_{2,t+1}
\end{aligned}$$

The one-step-ahead forecast variances are:
$$\begin{aligned}
\sigma_{1,t+1|t}^2 &= E(x_{1,t+1} - x_{1,t+1|t}|\Omega_t)^2 = E(\varepsilon_{1,t+1}^2) = \sigma_{1}^2 \\
\sigma_{2,t+1|t}^2 &= E(x_{2,t+1} - x_{2,t+1|t}|\Omega_t)^2 = E(\varepsilon_{2,t+1}^2) = \sigma_{2}^2
\end{aligned}$$

The one-step-ahead (95%) interval forecasts are:
$$\begin{aligned}
x_{1,t+1|t} \pm z_{.025}\sigma_{1,t+1|t} = x_{1,t+1|t} \pm 1.96\sigma_{\varepsilon_1} \\
x_{2,t+1|t} \pm z_{.025}\sigma_{2,t+1|t} = x_{2,t+1|t} \pm 1.96\sigma_{\varepsilon_2}
\end{aligned}$$


### Two-step-ahead forecast from bivariate VAR(1)

The realizations of the variables in period $t+2$ are: 
$$\begin{aligned}
x_{1,t+2} &= \alpha_1 + \pi_{11} x_{1,t+1} + \pi_{12} x_{2,t+1} + \varepsilon_{1,t+2} \\
x_{2,t+2} &= \alpha_2 + \pi_{21} x_{1,t+1} + \pi_{22} x_{2,t+1} + \varepsilon_{2,t+2}
\end{aligned}$$

The optimal two-step-ahead forecasts are:
$$\begin{aligned}
x_{1,t+2|t} &= E(x_{1,t+2}|\Omega_t) = \alpha_1 + \pi_{11} x_{1,t+1|t} + \pi_{12} x_{2,t+1|t} \\
x_{2,t+2|t} &= E(x_{2,t+2}|\Omega_t) = \alpha_2 + \pi_{21} x_{1,t+1|t} + \pi_{22} x_{2,t+1|t}
\end{aligned}$$

The two-step-ahead forecast errors are:
$$\begin{aligned}
e_{1,t+2|t} &= x_{1,t+2} - x_{1,t+2|t} = \pi_{11} e_{1,t+1|t} + \pi_{12} e_{2,t+1|t} + \varepsilon_{1,t+2} \\
e_{2,t+2|t} &= x_{2,t+2} - x_{2,t+2|t} = \pi_{21} e_{1,t+1|t} + \pi_{22} e_{2,t+1|t} + \varepsilon_{2,t+2}
\end{aligned}$$


The two-step-ahead forecast variances are:
$$\begin{aligned}
\sigma_{1,t+2|t}^2 &= E(x_{1,t+2} - x_{1,t+2|t}|\Omega_t)^2 \\ 
&= \sigma_{1}^2(1+\pi_{11}^2) + \sigma_{2}^2\pi_{12}^2 + 2\pi_{11}\pi_{12} Cov(\varepsilon_{1},\varepsilon_{2})\\
\sigma_{2,t+2|t}^2 &= E(x_{2,t+2} - x_{2,t+2|t}|\Omega_t)^2 \\ 
&= \sigma_{2}^2(1+\pi_{22}^2) + \sigma_{1}^2\pi_{21}^2 + 2\pi_{21}\pi_{22} Cov(\varepsilon_{1},\varepsilon_{2})
\end{aligned}$$

The two-step-ahead (95%) interval forecasts are:
$$\begin{aligned}
x_{1,t+2|t} \pm z_{.025}\sigma_{1,t+2|t} \\
x_{2,t+2|t} \pm z_{.025}\sigma_{2,t+2|t}
\end{aligned}$$


### Out-of-Sample Granger Causality

The previously discussed (in sample) test of causality in Granger sense is frequently performed in practice. But as noted above, the term 'causality' may be misleading somewhat. Indeed, the 'true spirit' of such test is to assess the ability of a variable to help predict another variable in an out-of-sample setting.

Consider the restricted and unrestricted information sets, where the former only contains information on variable $X_1$, while the latter contains the same information as well as information on variable $X_2$:
$$\begin{aligned}
&\Omega_{t,r} \equiv \Omega_{t}(X_1) = \{x_{1,t},x_{1,t-1},\ldots\} \\
&\Omega_{t,u} \equiv \Omega_{t}(X_1,X_2) = \{x_{1,t},x_{1,t-1},\ldots,x_{2,t},x_{2,t-1},\ldots\}
\end{aligned}$$

Following Granger's definition of causality: $\{X_2\}$ is said to cause $\{X_1\}$ if $\sigma_{x_1}^2\left(\Omega_{t,u}\right) < \sigma_{x_1}^2\left(\Omega_{t,r}\right)$, meaning that we can better predict $X_1$ using all available information on $X_1$ and $X_2$, rather than that on $X_1$ only.


Let the forecasts based on each of the information sets be:
$$\begin{aligned}
	&x_{1,t+h|t,r} = E\left(x_{1,t+h}|\Omega_{t,r}\right) \\
	&x_{1,t+h|t,u} = E\left(x_{1,t+h}|\Omega_{t,u}\right)
\end{aligned}$$

For these forecasts, the corresponding forecast errors are:
$$\begin{aligned}
	& e_{1,t+h|t,r} = x_{1,t+h} - x_{1,t+h|t,r}\\
	& e_{1,t+h|t,u} = x_{1,t+h} - x_{1,t+h|t,u}
\end{aligned}$$

The out-of-sample forecast errors are then evaluated by comparing the loss functions based on these forecasts errors. For example, assuming quadratic loss, and $P$ out-of-sample forecasts:
$$\begin{aligned}
RMSFE_{r} &= \sqrt{\frac{1}{P}\sum_{s=1}^{P}\left(e_{1,R+s|R-1+s,r}\right)^2} \\
RMSFE_{u} &= \sqrt{\frac{1}{P}\sum_{s=1}^{P}\left(e_{1,R+s|R-1+s,u}\right)^2}
\end{aligned}$$
where $R$ is the size of the (first) estimation window.

$\{X_2\}$ is said to cause *in Granger sense* $\{X_1\}$ if $RMSFE_{u} < RMSFE_{r}$.
