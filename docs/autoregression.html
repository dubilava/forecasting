<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Chapter 6 – Autoregression | Educated Guess" />
<meta property="og:type" content="book" />
<meta property="og:image" content="/forecasting-logo.png" />
<meta property="og:description" content="Forecasting With Time Series Models Using R" />


<meta name="author" content="David Ubilava" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Forecasting With Time Series Models Using R">

<title>Chapter 6 – Autoregression | Educated Guess</title>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<link href="libs/msmb-css-0/msmb.css" rel="stylesheet" />
<script>
function toggle_visibility(id1, id2) {
var e = document.getElementById(id1);
var f = document.getElementById(id2);

e.style.display = ((e.style.display!='none') ? 'none' : 'block');

if(f.classList.contains('fa-plus-square')) {
    f.classList.add('fa-minus-square')
    f.classList.remove('fa-plus-square')
} else {
    f.classList.add('fa-plus-square')
    f.classList.remove('fa-minus-square')
}

}
</script>
<script>
function copy_link(id) {
  var dummy = document.createElement('input'),
  text = window.location.href.split(/[?#]/)[0] + '#' + id;
  document.body.appendChild(dummy);
  dummy.value = text;
  dummy.select();
  document.execCommand('copy');
  document.body.removeChild(dummy);
  
  var tooltip = document.getElementById(id + '-tooltip');
  tooltip.innerHTML = 'Copied!';
}

function reset_tooltip(id) {
  var tooltip = document.getElementById(id);
  tooltip.innerHTML = 'Copy link';
}
</script>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
<script>
  $(document).ready(function () {
    var element_label= $("label[for *= 'tufte-sn-']");
    var count = $(element_label).length;
    $(element_label).each(function( index ) {
      //console.log( ++index + ": " + $( this ).text() );
      $(this).attr('for','tufte-sn-'+ ++index);
      $(this).text(index);
    });
  });
  $(document).ready(function () {
    var element_input= $("input[id *= 'tufte-sn-']");
    var count = $(element_input).length;
    $(element_input).each(function( index ) {
      //console.log( ++index + ": " + $( this ).text() );
      $(this).attr('id','tufte-sn-'+ ++index);
    }); 
  });
  $(document).ready(function () {
    var element_span= $("span[class *= 'sidenote-number']");
    var count = $(element_span).length;
    $(element_span).each(function( index ) {
      //console.log( ++index + ": " + $( this ).text() );
      $(this).text(++index);
    }); 
  });
</script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }

code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul class="navbar">
<li class="msmb"><p class="title">Educated Guess<p><p class="author">David Ubilava</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="index.html" id="toc-educated-guess-an-intuitive-guide-to-forecasting-with-time-series-models-using-r">Educated Guess: An Intuitive Guide to Forecasting With Time Series Models Using R</a>
<a href="forecasting-with-time-series-models.html" id="toc-forecasting-with-time-series-models">Forecasting With Time Series Models</a>
<a href="introduction-to-forecasting.html" id="toc-introduction-to-forecasting"><span class="toc-section-number">1</span> – Introduction to Forecasting</a>
<a href="features-of-time-series-data.html" id="toc-features-of-time-series-data"><span class="toc-section-number">2</span> – Features of Time Series Data</a>
<a href="forecasting-methods-and-routines.html" id="toc-forecasting-methods-and-routines"><span class="toc-section-number">3</span> – Forecasting Methods and Routines</a>
<a href="trends.html" id="toc-trends"><span class="toc-section-number">4</span> – Trends</a>
<a href="seasonality.html" id="toc-seasonality"><span class="toc-section-number">5</span> – Seasonality</a>
<a id="active-page" href="autoregression.html" id="toc-autoregression"><span class="toc-section-number">6</span> – Autoregression</a><ul class="toc-sections">
<li class="toc"><a href="#stochastic-cycles"> Stochastic Cycles</a></li>
<li class="toc"><a href="#modeling-2"> Modeling</a></li>
<li class="toc"><a href="#forecasting-2"> Forecasting</a></li>
</ul>
<a href="vector-autoregression.html" id="toc-vector-autoregression"><span class="toc-section-number">7</span> – Vector Autoregression</a>
<a href="threshold-autoregression.html" id="toc-threshold-autoregression"><span class="toc-section-number">8</span> – Threshold Autoregression</a>
<a href="forecast-evaluation.html" id="toc-forecast-evaluation"><span class="toc-section-number">9</span> – Forecast Evaluation</a>
<a href="forecast-combination.html" id="toc-forecast-combination"><span class="toc-section-number">10</span> – Forecast Combination</a>
<a href="forecasting-using-r.html" id="toc-forecasting-using-r">Forecasting Using R</a>
<a href="tutorial-1-introduction-to-r.html" id="toc-tutorial-1-introduction-to-r">Tutorial 1: Introduction to R</a>
<a href="tutorial-2-data-management-and-visualisation.html" id="toc-tutorial-2-data-management-and-visualisation">Tutorial 2: Data Management and Visualisation</a>
<a href="tutorial-3-forecasting-methods-and-routines.html" id="toc-tutorial-3-forecasting-methods-and-routines">Tutorial 3: Forecasting Methods and Routines</a>
<a href="tutorial-4-trends.html" id="toc-tutorial-4-trends">Tutorial 4: Trends</a>
<a href="tutorial-5-seasonality.html" id="toc-tutorial-5-seasonality">Tutorial 5: Seasonality</a>
<a href="tutorial-6-autoregression.html" id="toc-tutorial-6-autoregression">Tutorial 6: Autoregression</a>
<a href="tutorial-7-moving-average.html" id="toc-tutorial-7-moving-average">Tutorial 7: Moving Average</a>
<a href="tutorial-8-vector-autoregression.html" id="toc-tutorial-8-vector-autoregression">Tutorial 8: Vector Autoregression</a>
<a href="tutorial-9-threshold-autoregression.html" id="toc-tutorial-9-threshold-autoregression">Tutorial 9: Threshold Autoregression</a>
<a href="tutorial-10-forecast-evaluation.html" id="toc-tutorial-10-forecast-evaluation">Tutorial 10: Forecast Evaluation</a>
<a href="tutorial-10-forecast-combination.html" id="toc-tutorial-10-forecast-combination">Tutorial 10: Forecast Combination</a>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="autoregression" class="section level1" number="6">
<h1>
<span class="header-section-number">Chapter 6</span> – Autoregression</h1>
<p><img src="art/autoregression.png"></p>
<div id="stochastic-cycles" class="section level2" number="6.1">
<h2>
<span class="header-section-number">6.1</span> Stochastic Cycles<div class="tooltip"><button class="internal-link-btn" onclick="copy_link('stochastic-cycles')" onmouseout="reset_tooltip('stochastic-cycles-tooltip')"><span class="tooltiptext" id="stochastic-cycles-tooltip">Copy link</span><i class="fa fa-link"></i></button></div>
</h2>
<p><span class="newthought">Often there is</span> a cyclical pattern in economic time series. Cycles are characterized by a sequence of expansions and contractions, almost like in the case of seasonality. Unlike seasonality, however, a cycle is not contained within a calendar year and, moreover, the amplitude and length of cycles may vary from one another. Such cycles, which are (assumed to be) generated by random variables, are referred to as <em>stochastic cycles</em>.</p>
<p>Autoregressive stochastic cycle is a special and widely applied case of a stochastic cycle, where a random variable in a given period of a stochastic process is expressed as a function of random variables, of the same stochastic process, from preceding periods. That is:
<span class="math display">\[Y_t = f(Y_{t-1},Y_{t-2},\ldots),\;~~t=1,\ldots,T.\]</span> Autoregressive models, or simply <em>autoregressions</em> are deployed to approximate dynamics of such stochastic cycles.</p>
</div>
<div id="modeling-2" class="section level2" number="6.2">
<h2>
<span class="header-section-number">6.2</span> Modeling<div class="tooltip"><button class="internal-link-btn" onclick="copy_link('modeling-2')" onmouseout="reset_tooltip('modeling-2-tooltip')"><span class="tooltiptext" id="modeling-2-tooltip">Copy link</span><i class="fa fa-link"></i></button></div>
</h2>
<p>An autoregression of order <span class="math inline">\(p\)</span>, denoted as <span class="math inline">\(AR(p)\)</span>, has the following functional form:
<span class="math display">\[y_t = \alpha + \beta_1 y_{t-1}+\beta_2 y_{t-2}+ \cdots + \beta_p y_{t-p}+\varepsilon_t,\]</span> where <span class="math inline">\(\varepsilon\sim iid~\text{N}\left(0,\sigma^2_{\varepsilon}\right)\)</span></p>
<p>The sum of the autoregressive parameters, <span class="math inline">\(\beta_1,\ldots,\beta_p\)</span>, depicts the persistence of the series. The larger is the persistence (i.e., closer it is to one), the longer it takes for the effect of a shock to dissolve. The effect will, eventually, dissolve so long as the series are covariance-stationary.</p>
<p>The autocorrelation, <span class="math inline">\(\rho\)</span>, and partial autocorrelation, <span class="math inline">\(\pi\)</span>, functions of the covariance-stationary <span class="math inline">\(AR(p)\)</span> process have the following distinctive features:</p>
<ul>
<li>
<span class="math inline">\(\rho_1 = \pi_1\)</span>, and <span class="math inline">\(\pi_p = \beta_p\)</span>.</li>
<li>The values of <span class="math inline">\(\beta_1,\ldots,\beta_p\)</span> determine the shape of the autocorrelation function (ACF); in any case, the smaller (in absolute terms) is the persistence measure, the faster the ACF decays toward zero.</li>
<li>The partial autocorrelation function (PACF) is characterized by “statistically significant” first <span class="math inline">\(p\)</span> spikes <span class="math inline">\(\pi_1 \neq 0,\ldots,\pi_p \neq 0\)</span>, and the remaining <span class="math inline">\(\pi_k = 0\)</span>, <span class="math inline">\(\forall k &gt; p\)</span>.</li>
</ul>
<p>In what follows, we consider the cases of the first-order autoregression as an illustration.</p>
<div id="first-order-autoregression" class="section level3" number="6.2.1">
<h3>
<span class="header-section-number">6.2.1</span> First-order autoregression<div class="tooltip"><button class="internal-link-btn" onclick="copy_link('first-order-autoregression')" onmouseout="reset_tooltip('first-order-autoregression-tooltip')"><span class="tooltiptext" id="first-order-autoregression-tooltip">Copy link</span><i class="fa fa-link"></i></button></div>
</h3>
<p>A first-order autoregression is given by: <span class="math display">\[y_t = \alpha + \beta_1 y_{t-1} + \varepsilon_t,\]</span> where <span class="math inline">\(\alpha\)</span> is a constant term; <span class="math inline">\(\beta_1\)</span> is the <em>persistence</em> parameter; and <span class="math inline">\(\varepsilon_t\)</span> is a white noise process.</p>
<p>A necessary and sufficient condition for an <span class="math inline">\(AR(1)\)</span> process to be covariance stationary is that <span class="math inline">\(|\beta_1| &lt; 1\)</span>. We can see this by substituting recursively the lagged equations into the lagged dependent variables:
<span class="math display">\[
\begin{aligned}
y_t &amp;= \alpha + \beta_1 y_{t-1} + \varepsilon_t \notag \\
y_t &amp;= \alpha + \beta_1 (\alpha + \beta_1 y_{t-2} + \varepsilon_{t-1}) + \varepsilon_t \notag \\
&amp;= \alpha(1+\beta_1) + \beta_1^2 (\alpha + \beta_1 y_{t-3} + \varepsilon_{t-2}) + \beta_1\varepsilon_{t-1} + \varepsilon_t \notag \\
&amp;\vdots  \notag \\
&amp;= \alpha\sum_{i=0}^{k-1}\beta_1^i + \beta_1^k y_{t-k} + \sum_{i=0}^{k-1}\beta_1^i\varepsilon_{t-i}
\end{aligned}
\]</span>
The end-result is a general linear process with geometrically declining coefficients. Here, <span class="math inline">\(|\beta_1| &lt; 1\)</span> is required for convergence.</p>
<p>Assuming <span class="math inline">\(|\beta_1| &lt; 1\)</span>, as <span class="math inline">\(k \to \infty\)</span> the process converges to: <span class="math display">\[y_t = \frac{\alpha}{1-\beta_1} + \sum_{i=0}^{\infty}\beta_1^i\varepsilon_{t-i}\]</span></p>
<p>The <em>unconditional mean</em> of this process is: <span class="math display">\[\mu = E\left(y_t\right) = E\left(\frac{\alpha}{1-\beta_1} + \sum_{i=0}^{\infty}\beta_1^i\varepsilon_{t-i}\right) = \frac{\alpha}{1-\beta_1}\]</span></p>
<p>The <em>unconditional variance</em> of this process is: <span class="math display">\[\gamma_0 = Var\left(y_t\right) = Var\left(\frac{\alpha}{1-\beta_1} + \sum_{i=0}^{\infty}\beta_1^i\varepsilon_{t-i}\right) = \frac{\sigma_{\varepsilon}^2}{1-\beta_1^2}\]</span></p>
<p>The <em>Autocovariance</em> is simply the covariance between <span class="math inline">\(y_t\)</span> and <span class="math inline">\(y_{t-k}\)</span>, that is: <span class="math display">\[\gamma_k = Cov(y_t,y_{t-k}) = E[(y_t - \mu)(y_{t-k} - \mu)] = E(y_t y_{t-k}) - \mu^2\]</span></p>
<p>Some algebraic manipulation can help us show that: <span class="math display">\[\gamma_k = \beta_1\gamma_{k-1},\]</span> and that: <span class="math display">\[\rho_{k} = \beta_1\rho_{k-1}\]</span> (recall, <span class="math inline">\(\rho_k = \gamma_k/\gamma_0\)</span> is the autocorrelation coefficient).</p>
<p>In fact, for AR(1), an autocorrelation coefficient of some lag can be represented as the autoregression parameter (which in this instance is equivalent to the persistence measure) to that power. That is:
<span class="math display">\[
\begin{aligned}
\rho_1 &amp;= \beta_1\rho_0 = \beta_1 \notag \\
\rho_2 &amp;= \beta_1\rho_1 = \beta_1^2 \notag \\
&amp;\vdots \notag \\
\rho_k &amp;= \beta_1\rho_{k-1} = \beta_1^k
\end{aligned}
\]</span></p>
<p>It follows that the autocorrelation function of a covariance stationary AR(1) is a geometric decay; the smaller is <span class="math inline">\(|\beta_1|\)</span> the more rapid is the decay.</p>
<p>Moreover, a smaller persistence parameter results in a quicker adjustment to the <em>unconditional mean</em> of the process.</p>
</div>
<div id="unit-roots-and-non-stationarity" class="section level3" number="6.2.2">
<h3>
<span class="header-section-number">6.2.2</span> Unit Roots and Non-stationarity<div class="tooltip"><button class="internal-link-btn" onclick="copy_link('unit-roots-and-non-stationarity')" onmouseout="reset_tooltip('unit-roots-and-non-stationarity-tooltip')"><span class="tooltiptext" id="unit-roots-and-non-stationarity-tooltip">Copy link</span><i class="fa fa-link"></i></button></div>
</h3>
<p>A special case of an AR(1) process is a random walk with drift. The latter is obtained by setting <span class="math inline">\(\beta_1=1\)</span>. Note that the unconditional mean and variance are undefined under this restriction.</p>
<p>We cannot directly test the null of non-stationarity in an autoregression. That is, we cannot just estimate: <span class="math display">\[y_t = \alpha+\beta_1 y_{t-1}+\varepsilon_t\]</span> and test whether <span class="math inline">\(\hat{\beta}_1\)</span> is statistically significantly less than unity.</p>
<p>Instead, we can subtract <span class="math inline">\(y_{t-1}\)</span> from both sides of the equation, and estimate: <span class="math display">\[\Delta y_t = \alpha+\phi y_{t-1}+\varepsilon_t,\]</span> where <span class="math inline">\(\phi=\beta_1-1\)</span>, and test whether <span class="math inline">\(\hat{\phi}\)</span> is statistically significantly less than zero. This test is known as the Dickey-Fuller (DF) test.</p>
<p>In practice, we apply the augmented Dickey-Fuller (ADF) test, which is the same test as above, except the lags of the dependent variable, <span class="math inline">\(\Delta y_t\)</span>, are added to the regression to ensure that <span class="math inline">\(\varepsilon_t\)</span> is white noise.</p>
<p>It is important to note that distribution of the test statistic is non-standard. That is, we can not use t distribution to test the null hypothesis of non-stationarity after obtaining the test statistic associated with <span class="math inline">\(\hat{\phi}\)</span>. Instead, we use the relevant version of the Dickey-Fuller table.<label for="tufte-sn-9" class="margin-toggle sidenote-number">9</label><input type="checkbox" id="tufte-sn-9" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">9</span> The three versions of the test are (i) unit root, (ii) unit root with drift, and (iii) unit root with drift and trend.</span></p>
<p>To illustrate some of the foregoing, consider the USD/EUR exchange rates.</p>
<div class="figure">
<span style="display:block;" id="fig:exchange"></span>
<p class="caption marginnote shownote">
Figure 6.1: USD/EUR exchange rates
</p>
<img src="forecasting_files/figure-html/exchange-1.png" alt="USD/EUR exchange rates" width="624">
</div>
<p>First, let’s observe the autocorrelation and partial autocorrelation functions of the series.</p>
<div class="figure">
<span style="display:block;" id="fig:acfexchange"></span>
<p class="caption marginnote shownote">
Figure 6.2: Autocorrelation function
</p>
<img src="forecasting_files/figure-html/acfexchange-1.png" alt="Autocorrelation function" width="624">
</div>
<div class="figure">
<span style="display:block;" id="fig:pacfexchange"></span>
<p class="caption marginnote shownote">
Figure 6.3: Partial autocorrelation function
</p>
<img src="forecasting_files/figure-html/pacfexchange-1.png" alt="Partial autocorrelation function" width="624">
</div>
<p>That autocorrelations decrease gradually and eventually become statistically indistinguishable from zero is suggestive of a stationary process. That partial autocorrelations of the first two lags are statistically significantly different from zero is suggestive of the second-order autoregression.<label for="tufte-sn-10" class="margin-toggle sidenote-number">10</label><input type="checkbox" id="tufte-sn-10" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">10</span> More formally, the order of autoregression can be determined using an information criterion such as AIC or SIC.</span></p>
<p>Let’s check formally, using augmented Dickey-Fuller test, whether the series are non-stationary. For that, estimate <span class="math inline">\(\Delta y_t=\alpha+\phi y_{t-1}+\delta_1\Delta y_{t-1}+\varepsilon_t\)</span>, and obtain test statistics associated with <span class="math inline">\(\hat{\phi}\)</span>. The test statistic turns out to be -2.757 which lies between the critical values of -2.88 for 5% statistical significance and -2.57 for 10% statistical significance.</p>
</div>
</div>
<div id="forecasting-2" class="section level2" number="6.3">
<h2>
<span class="header-section-number">6.3</span> Forecasting<div class="tooltip"><button class="internal-link-btn" onclick="copy_link('forecasting-2')" onmouseout="reset_tooltip('forecasting-2-tooltip')"><span class="tooltiptext" id="forecasting-2-tooltip">Copy link</span><i class="fa fa-link"></i></button></div>
</h2>
<p>Making forecasts for some future period, <span class="math inline">\(t+h\)</span>, from an AR(p) model that has been fit to the data up to and including period <span class="math inline">\(t+h-1\)</span> can be a straightforward exercise, so long as we have access to such data. That is the case for one-step-ahead forecasts, that is when <span class="math inline">\(h=1\)</span>, for which the information set is readily available. For multi-step-ahead forecasts, that is when <span class="math inline">\(h&gt;1\)</span>, this no longer it the case. There are two approaches or methods of multi-step-ahead forecasting that can allowe us to circumvent the issue.</p>
<div id="iterative-method-of-multistep-forecasting" class="section level3" number="6.3.1">
<h3>
<span class="header-section-number">6.3.1</span> Iterative Method of Multistep Forecasting<div class="tooltip"><button class="internal-link-btn" onclick="copy_link('iterative-method-of-multistep-forecasting')" onmouseout="reset_tooltip('iterative-method-of-multistep-forecasting-tooltip')"><span class="tooltiptext" id="iterative-method-of-multistep-forecasting-tooltip">Copy link</span><i class="fa fa-link"></i></button></div>
</h3>
<p>One approach involves ‘coming up’ with the value of the variable that has not been realized yet. For example, when making a two-step-ahead forecast for period <span class="math inline">\(t+2\)</span>, we need data from period <span class="math inline">\(t+1\)</span>, which is not available at the time when the forecast is made. Instead, we need to use our forecast for period <span class="math inline">\(t+1\)</span>. The same applies to forecasts for any subsequent periods in the future. This approach is known as an <em>iterative</em> method of forecasting, wherein we make forecast for some period using the available data, then iterate forward by one period and use the most recent forecast to make the next period’s forecast, and so on and so forth.</p>
<p>Consider an AR(p) model. A future realization of the random variable is <span class="math display">\[y_{t+h} = \alpha + \beta_1 y_{t+h-1} + \cdots  + \beta_p y_{t+h-p}+\varepsilon_{t+h}\]</span></p>
<p>Point forecast (ignoring parameter uncertainty) is: <span class="math display">\[y_{t+h|t} = E(y_{t+h}|\Omega_t) = \alpha + \beta_1 y_{t+h-1|t} + \cdots  + \beta_p y_{t+h-p|t},\]</span> where <span class="math inline">\(y_{t+h-j|t}=y_{t+h-j}\)</span> when <span class="math inline">\(h-j\le 0\)</span>. That is, for a given time period, we use the realization of the random variable if it is observed, otherwise we use the point forecast of the realization.</p>
<p>Forecast error: <span class="math display">\[e_{t+h|t} = y_{t+h} - y_{t+h|t} = \beta_1 e_{t+h-1|t} + \cdots  + \beta_p e_{t+h-p|t} + \varepsilon_{t+h},\]</span> where <span class="math inline">\(e_{t+h-j|t}=0\)</span> when <span class="math inline">\(h-j\le 0\)</span>. So, when <span class="math inline">\(h=1\)</span>, <span class="math inline">\(e_{t+1|t}=\varepsilon_{t+1}\)</span>, which is the same as with previously described models (e.g., trend or seasonal models), but when <span class="math inline">\(h&gt;1\)</span>, forecast error becomes more complex.</p>
<p>The forecast variance: <span class="math display">\[\sigma_{t+h|t}^2 = \sigma_{\varepsilon}^2 + \sum_{i=1}^{p}\beta_i^2 Var(e_{t+h-i|t}) + 2\sum_{i \neq j}\beta_i\beta_j Cov(e_{t+h-i|t},e_{t+h-j|t})\]</span>
These variance and covariances of forecast errors from preceding horizons are some functions of the in-sample error variance and model parameters.</p>
<p>The 95% interval forecast is: <span class="math display">\[y_{t+h|t} \pm 1.96 \hat{\sigma}_{\varepsilon}.\]</span></p>
<p>To illustrate the foregoing, let’s revisit the USD/EUR exchange rate series, and obtain point and interval forecasts for periods from January 2011 onward based on parameter estimates of the second-order autoregression using data up to and including December 2010.</p>
<div class="figure">
<span style="display:block;" id="fig:forexchange"></span>
<p class="caption marginnote shownote">
Figure 6.4: Second-order autoregression
</p>
<img src="forecasting_files/figure-html/forexchange-1.png" alt="Second-order autoregression" width="624">
</div>
</div>
<div id="direct-method-of-multistep-forecasting" class="section level3" number="6.3.2">
<h3>
<span class="header-section-number">6.3.2</span> Direct Method of Multistep Forecasting<div class="tooltip"><button class="internal-link-btn" onclick="copy_link('direct-method-of-multistep-forecasting')" onmouseout="reset_tooltip('direct-method-of-multistep-forecasting-tooltip')"><span class="tooltiptext" id="direct-method-of-multistep-forecasting-tooltip">Copy link</span><i class="fa fa-link"></i></button></div>
</h3>
<p>The other approach entails directly obtaining multi-step-ahead forecasts. To illustrate, consider a first-order autoregression: <span class="math inline">\(y_t=\alpha+\beta_1 y_{t-1}+\varepsilon_t.\)</span></p>
<p>One-step-ahead point forecast is readily given by: <span class="math inline">\(y_{t+1|t}=\alpha+\beta_1 y_{t}\)</span>. That is, we observe all the variables on the right-hand side of the equation.</p>
<p>To generate the two-step-ahead forecast in a similar manner, that is by ensuring the observed variables on the right-hand side of the equation, we can substitute <span class="math inline">\(y_{t-1}=\alpha+\beta_1 y_{t-2}+\varepsilon_{t-1}\)</span> into the original equation to obtain: <span class="math display">\[y_t=\alpha(1+\beta_1)+\beta_1^2y_{t-2} + \varepsilon_t + \beta_1\varepsilon_{t-1} = \tilde{\alpha} + \tilde{\beta}_1 y_{t-2} + u_t,\]</span> where <span class="math inline">\(\tilde{\alpha}=\alpha(1+\beta_1)\)</span> and <span class="math inline">\(\tilde{\beta}_1=\beta_1^2\)</span>, and <span class="math inline">\(u_t=\varepsilon_t + \beta_1\varepsilon_{t-1}.\)</span></p>
<p>Thus, we can obtain two-step-ahead forecast in a manner similar to that when we obtain one-step-ahead forecast by regressing <span class="math inline">\(y_t\)</span> on <span class="math inline">\(y_{t-2}\)</span>, and then directly forecasting <span class="math inline">\(y_{t+2}\)</span> from <span class="math inline">\(y_{t}\)</span>.</p>
<p>This <em>direct</em> method of multi-step-ahead forecasting can be extended to higher order autoregression, as well as to any forecast horizon.</p>
<p>In the direct method, error terms are serially correlated (by construction).<label for="tufte-sn-11" class="margin-toggle sidenote-number">11</label><input type="checkbox" id="tufte-sn-11" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">11</span> Recall that multi-step-ahead forecast errors tend to be serially correlated. So, direct method merely maintains this feature of multistep forecasts.</span> For example, in the direct two-step-ahead forecast from a re-specified AR(1) model, as we saw: <span class="math inline">\(u_t = \varepsilon_t+\beta_1\varepsilon_{t-1}\)</span>. It then follows that: <span class="math display">\[\sigma^2_u = E\left[(\varepsilon_t+\beta_1\varepsilon_{t-1})^2\right] = \sigma^2_{\varepsilon}(1+\beta_1^2).\]</span></p>
<p>This is also the expression of the two-step-ahead forecast error variance under the iterated method.</p>
<p>Thus, when applying the direct method of forecasting, interval forecasts for a given horizon are obtained ‘directly,’ based on the standard deviation of the residuals.</p>
<p>The relative performance of forecasts from the considered two methods—iterative and direct—in terms of bias and efficiency depends on the bias and efficiency of the estimators of each method.</p>
<p>Assuming the autoregressive model is correctly specified, both methods are consistent, but the iterative method is more efficient. Thus, in large samples, the iterative forecast can be expected to perform better than the direct forecast.</p>
<p>In the case of a mis-specified model, however, the direct method may as well outperform the iterated method.</p>

</div>
</div>
</div>
<p style="text-align: center;">
<a href="seasonality.html"><button class="btn btn-default">Previous</button></a>
<a href="vector-autoregression.html"><button class="btn btn-default">Next</button></a>
</p>
<p class="build-date">Page built: 
2022-10-10
 using 
R version 4.1.2 (2021-11-01)
</p>
</div>
</div>



</body>
</html>
