<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Chapter 5 Autoregressive Models | Forecasting With Time Series Models Using R" />
<meta property="og:type" content="book" />
<meta property="og:image" content="/forecasting-logo.png" />
<meta property="og:description" content="Chapter 5 Autoregressive Models | Forecasting With Time Series Models Using R" />


<meta name="author" content="David Ubilava" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Chapter 5 Autoregressive Models | Forecasting With Time Series Models Using R">

<title>Chapter 5 Autoregressive Models | Forecasting With Time Series Models Using R</title>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<link href="libs/msmb-css-0/msmb.css" rel="stylesheet" />
<script>
function toggle_visibility(id1, id2) {
var e = document.getElementById(id1);
var f = document.getElementById(id2);

e.style.display = ((e.style.display!='none') ? 'none' : 'block');

if(f.classList.contains('fa-plus-square')) {
    f.classList.add('fa-minus-square')
    f.classList.remove('fa-plus-square')
} else {
    f.classList.add('fa-plus-square')
    f.classList.remove('fa-minus-square')
}

}
</script>
<script>
function copy_link(id) {
  var dummy = document.createElement('input'),
  text = window.location.href.split(/[?#]/)[0] + '#' + id;
  document.body.appendChild(dummy);
  dummy.value = text;
  dummy.select();
  document.execCommand('copy');
  document.body.removeChild(dummy);
  
  var tooltip = document.getElementById(id + '-tooltip');
  tooltip.innerHTML = 'Copied!';
}

function reset_tooltip(id) {
  var tooltip = document.getElementById(id);
  tooltip.innerHTML = 'Copy link';
}
</script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }

code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul class="navbar">
<li class="msmb"><p class="title">Forecasting With Time Series Models Using R<p><p class="author">David Ubilava</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="index.html" id="toc-foreword">Foreword</a>
<a href="forecasting-with-time-series-models.html" id="toc-forecasting-with-time-series-models">Forecasting With Time Series Models</a>
<a href="introduction-to-forecasting.html" id="toc-introduction-to-forecasting"><span class="toc-section-number">1</span> Introduction to Forecasting</a>
<a href="features-of-time-series-data.html" id="toc-features-of-time-series-data"><span class="toc-section-number">2</span> Features of Time Series Data</a>
<a href="forecasting-methods-and-routines.html" id="toc-forecasting-methods-and-routines"><span class="toc-section-number">3</span> Forecasting Methods and Routines</a>
<a href="trends-and-seasonality.html" id="toc-trends-and-seasonality"><span class="toc-section-number">4</span> Trends and Seasonality</a>
<a id="active-page" href="autoregressive-models.html" id="toc-autoregressive-models"><span class="toc-section-number">5</span> Autoregressive Models</a><ul class="toc-sections">
<li class="toc"><a href="#modeling-1"> Modeling</a></li>
<li class="toc"><a href="#forecasting-1"> Forecasting</a></li>
</ul>
<a href="vector-autoregressive-models.html" id="toc-vector-autoregressive-models"><span class="toc-section-number">6</span> Vector Autoregressive Models</a>
<a href="dynamic-factor-models.html" id="toc-dynamic-factor-models"><span class="toc-section-number">7</span> Dynamic Factor Models</a>
<a href="threshold-autoregressive-models.html" id="toc-threshold-autoregressive-models"><span class="toc-section-number">8</span> Threshold Autoregressive Models</a>
<a href="forecast-evaluation.html" id="toc-forecast-evaluation"><span class="toc-section-number">9</span> Forecast Evaluation</a>
<a href="forecast-combination.html" id="toc-forecast-combination"><span class="toc-section-number">10</span> Forecast Combination</a>
<a href="forecasting-using-r.html" id="toc-forecasting-using-r">Forecasting Using R</a>
<a href="tutorial-1-introduction-to-r.html" id="toc-tutorial-1-introduction-to-r">Tutorial 1: Introduction to R</a>
<a href="tutorial-2-data-management.html" id="toc-tutorial-2-data-management">Tutorial 2: Data Management</a>
<a href="tutorial-3-forecasting-methods-and-routines.html" id="toc-tutorial-3-forecasting-methods-and-routines">Tutorial 3: Forecasting Methods and Routines</a>
<a href="tutorial-4-trends-and-seasonality.html" id="toc-tutorial-4-trends-and-seasonality">Tutorial 4: Trends and Seasonality</a>
<a href="tutorial-5-autoregressive-models.html" id="toc-tutorial-5-autoregressive-models">Tutorial 5: Autoregressive Models</a>
<a href="tutorial-6-vector-autoregressive-models.html" id="toc-tutorial-6-vector-autoregressive-models">Tutorial 6: Vector Autoregressive Models</a>
<a href="tutorial-7-dynamic-factor-models.html" id="toc-tutorial-7-dynamic-factor-models">Tutorial 7: Dynamic Factor Models</a>
<a href="tutorial-8-threshold-autoregression.html" id="toc-tutorial-8-threshold-autoregression">Tutorial 8: Threshold Autoregression</a>
<a href="tutorial-9-forecast-evaluation.html" id="toc-tutorial-9-forecast-evaluation">Tutorial 9: Forecast Evaluation</a>
<a href="tutorial-10-forecast-combination.html" id="toc-tutorial-10-forecast-combination">Tutorial 10: Forecast Combination</a>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="autoregressive-models" class="section level1" number="5">
<h1>
<span class="header-section-number">Chapter 5</span> Autoregressive Models</h1>
<p>Economic time series are often characterized by stochastic cycles. A cycle is a pattern of periodic fluctuations, not contained within a calendar year. A stochastic cycle is one generated by random variables. In general terms, the process is given by:
<span class="math display">\[Y_t = f(Y_{t-1},Y_{t-2},\ldots;\mathbf{\theta})+\varepsilon_t.\;~~t=1,\ldots,T\]</span></p>
<p>An autoregressive process (or, simply, an autoregression) is a regression in which the dependent variable and the regressors belong to the same stochastic process.</p>
<p>An autoregression of order <span class="math inline">\(p\)</span>, denoted as <span class="math inline">\(AR(p)\)</span>, has the following functional form:
<span class="math display">\[y_t = \alpha + \beta_1 y_{t-1}+\beta_2 y_{t-2}+ \cdots + \beta_p y_{t-p}+\varepsilon_t\]</span></p>
<p>The sum of the autoregressive parameters, <span class="math inline">\(\beta_1,\ldots,\beta_p\)</span>, depicts the persistence of the series. The larger is the persistence (i.e., closer it is to one), the longer it takes for the effect of a shock to dissolve. The effect will, eventually, dissolve so long as the series are covariance-stationary.</p>
<p>The autocorrelation, <span class="math inline">\(\rho\)</span>, and partial autocorrelation, <span class="math inline">\(\pi\)</span>, functions of the covariance-stationary <span class="math inline">\(AR(p)\)</span> process have the following distinctive features:</p>
<ul>
<li>
<span class="math inline">\(\rho_1 = \pi_1\)</span>, and <span class="math inline">\(\pi_p = \beta_p\)</span>.</li>
<li>The values of <span class="math inline">\(\beta_1,\ldots,\beta_p\)</span> determine the shape of the autocorrelation function (ACF); in any case, the smaller (in absolute terms) is the persistence measure, the faster the ACF decays toward zero.</li>
<li>The partial autocorrelation function (PACF) is characterized by “statistically significant” first <span class="math inline">\(p\)</span> spikes <span class="math inline">\(\pi_1 \neq 0,\ldots,\pi_p \neq 0\)</span>, and the remaining <span class="math inline">\(\pi_k = 0\)</span>, <span class="math inline">\(\forall k &gt; p\)</span>.</li>
</ul>
<div id="modeling-1" class="section level2" number="5.1">
<h2>
<span class="header-section-number">5.1</span> Modeling<div class="tooltip"><button class="internal-link-btn" onclick="copy_link('modeling-1')" onmouseout="reset_tooltip('modeling-1-tooltip')"><span class="tooltiptext" id="modeling-1-tooltip">Copy link</span><i class="fa fa-link"></i></button></div>
</h2>
<div id="ar1" class="section level3" number="5.1.1">
<h3>
<span class="header-section-number">5.1.1</span> AR(1)<div class="tooltip"><button class="internal-link-btn" onclick="copy_link('ar1')" onmouseout="reset_tooltip('ar1-tooltip')"><span class="tooltiptext" id="ar1-tooltip">Copy link</span><i class="fa fa-link"></i></button></div>
</h3>
<p>The first-order autoregression is given by: <span class="math display">\[y_t = \alpha + \beta_1 y_{t-1} + \varepsilon_t,\]</span> where <span class="math inline">\(\alpha\)</span> is a constant term; <span class="math inline">\(\beta_1\)</span> is the <em>persistence</em> parameter; and <span class="math inline">\(\varepsilon_t\)</span> is a white noise process.</p>
<p>A necessary and sufficient condition for an <span class="math inline">\(AR(1)\)</span> process to be covariance stationary is that <span class="math inline">\(|\beta_1| &lt; 1\)</span>. We can see this by substituting recursively the lagged equations into the lagged dependent variables:
<span class="math display">\[
\begin{aligned}
y_t &amp;= \alpha + \beta_1 y_{t-1} + \varepsilon_t \notag \\
y_t &amp;= \alpha + \beta_1 (\alpha + \beta_1 y_{t-2} + \varepsilon_{t-1}) + \varepsilon_t \notag \\
&amp;= \alpha(1+\beta_1) + \beta_1^2 (\alpha + \beta_1 y_{t-3} + \varepsilon_{t-2}) + \beta_1\varepsilon_{t-1} + \varepsilon_t \notag \\
&amp;\vdots  \notag \\
&amp;= \alpha\sum_{i=0}^{k-1}\beta_1^i + \beta_1^k y_{t-k} + \sum_{i=0}^{k-1}\beta_1^i\varepsilon_{t-i}
\end{aligned}
\]</span>
The end-result is a general linear process with geometrically declining coefficients. Here, <span class="math inline">\(|\beta_1| &lt; 1\)</span> is required for convergence.</p>
<p>Assuming <span class="math inline">\(|\beta_1| &lt; 1\)</span>, as <span class="math inline">\(k \to \infty\)</span> the process converges to: <span class="math display">\[y_t = \frac{\alpha}{1-\beta_1} + \sum_{i=0}^{\infty}\beta_1^i\varepsilon_{t-i}\]</span></p>
<p>The <em>unconditional mean</em> of this process is: <span class="math display">\[\mu = E\left(y_t\right) = E\left(\frac{\alpha}{1-\beta_1} + \sum_{i=0}^{\infty}\beta_1^i\varepsilon_{t-i}\right) = \frac{\alpha}{1-\beta_1}\]</span></p>
<p>The <em>unconditional variance</em> of this process is: <span class="math display">\[\gamma_0 = Var\left(y_t\right) = Var\left(\frac{\alpha}{1-\beta_1} + \sum_{i=0}^{\infty}\beta_1^i\varepsilon_{t-i}\right) = \frac{\sigma_{\varepsilon}^2}{1-\beta_1^2}\]</span></p>
<p>The <em>Autocovariance</em> is simply the covariance between <span class="math inline">\(y_t\)</span> and <span class="math inline">\(y_{t-k}\)</span>, that is: <span class="math display">\[\gamma_k = Cov(y_t,y_{t-k}) = E[(y_t - \mu)(y_{t-k} - \mu)] = E(y_t y_{t-k}) - \mu^2\]</span></p>
<p>Some algebraic manipulation can help us show that: <span class="math display">\[\gamma_k = \beta_1\gamma_{k-1},\]</span> and that: <span class="math display">\[\rho_{k} = \beta_1\rho_{k-1}\]</span> (recall, <span class="math inline">\(\rho_k = \gamma_k/\gamma_0\)</span> is the autocorrelation coefficient).</p>
<p>In fact, for AR(1), an autocorrelation coefficient of some lag can be represented as the autoregression parameter (which in this instance is equivalent to the persistence measure) to that power. That is:
<span class="math display">\[
\begin{aligned}
\rho_1 &amp;= \beta_1\rho_0 = \beta_1 \notag \\
\rho_2 &amp;= \beta_1\rho_1 = \beta_1^2 \notag \\
&amp;\vdots \notag \\
\rho_k &amp;= \beta_1\rho_{k-1} = \beta_1^k
\end{aligned}
\]</span></p>
<p>It follows that the autocorrelation function of a covariance stationary AR(1) is a geometric decay; the smaller is <span class="math inline">\(|\beta_1|\)</span> the more rapid is the decay.</p>
<p>By imposing certain restrictions, the AR(1) will reduce to other already known models:</p>
<ul>
<li>If <span class="math inline">\(\beta_1 = 0\)</span>, <span class="math inline">\(y_t\)</span> is equivalent to a white noise.</li>
<li>If <span class="math inline">\(\beta_1 = 1\)</span> and <span class="math inline">\(\alpha = 0\)</span>, <span class="math inline">\(y_t\)</span> is a random walk.</li>
<li>If <span class="math inline">\(\beta_1 = 1\)</span> and <span class="math inline">\(\alpha \neq 0\)</span>, <span class="math inline">\(y_t\)</span> is a random walk with drift.</li>
</ul>
<p>In general, a smaller persistence parameter results in a quicker adjustment to the <em>unconditional mean</em> of the process.</p>
<p>The autocorrelation and partial autocorrelation functions of the AR(1) process have three distinctive features:</p>
<ul>
<li>
<span class="math inline">\(\rho_1 = \pi_1 = \beta_1\)</span>. That is, the persistence parameter is also the autocorrelation and the partial autocorrelation coefficient.</li>
<li>The autocorrelation function decreases exponentially toward zero, and the decay is faster when the persistence parameter is smaller.</li>
<li>The partial autocorrelation function is characterized by only one spike <span class="math inline">\(\pi_1 \neq 0\)</span>, and the remaining <span class="math inline">\(\pi_k = 0\)</span>, <span class="math inline">\(\forall k &gt; 1\)</span>.</li>
</ul>
<p>To illustrate the foregoing, let’s generate a series of 100 observations that follow the process: <span class="math inline">\(y_t=0.8y_{t-1}+\varepsilon_t\)</span>, where <span class="math inline">\(y_0=0\)</span> and <span class="math inline">\(\varepsilon\sim N(0,1)\)</span>, and plot the ACF and PACF of this series.</p>
<p><img src="forecasting_files/figure-html/unnamed-chunk-10-1.png" width="624"></p>
</div>
<div id="ar2" class="section level3" number="5.1.2">
<h3>
<span class="header-section-number">5.1.2</span> AR(2)<div class="tooltip"><button class="internal-link-btn" onclick="copy_link('ar2')" onmouseout="reset_tooltip('ar2-tooltip')"><span class="tooltiptext" id="ar2-tooltip">Copy link</span><i class="fa fa-link"></i></button></div>
</h3>
<p>Now consider the second-order autoregression: <span class="math display">\[y_t = \alpha + \beta_1 y_{t-1} + \beta_2 y_{t-2} + \varepsilon_t\]</span></p>
<p>where <span class="math inline">\(\alpha\)</span> is a constant term; <span class="math inline">\(\beta_1+\beta_2\)</span> is the persistence measure; and <span class="math inline">\(\varepsilon_t\)</span> is a white noise process.</p>
<p>In what follows, the necessary (1 and 2) and sufficient (3 and 4) conditions for an <span class="math inline">\(AR(2)\)</span> process to be covariance stationary are:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(|\beta_2| &lt; 1\)</span></li>
<li><span class="math inline">\(|\beta_1| &lt; 2\)</span></li>
<li><span class="math inline">\(\beta_1 + \beta_2 &lt; 1\)</span></li>
<li><span class="math inline">\(\beta_2 - \beta_1 &lt; 1\)</span></li>
</ol>
<p>The autocorrelation functions of the AR(2) process have the following distinctive features:</p>
<ul>
<li>
<span class="math inline">\(\rho_1 = \pi_1\)</span> (which is true for any <span class="math inline">\(AR(p)\)</span> process), and <span class="math inline">\(\pi_2 = \beta_2\)</span>.</li>
<li>The autocorrelation function decreases toward zero. The path, however, varies depending on the values of <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span>. Nonetheless, the decay is faster when the persistence measure is smaller.</li>
<li>The partial autocorrelation function is characterized by only two spikes <span class="math inline">\(\pi_1 \neq 0\)</span> and <span class="math inline">\(\pi_2 \neq 0\)</span>, and the remaining <span class="math inline">\(\pi_k = 0\)</span>, <span class="math inline">\(\forall k &gt; 2\)</span>.</li>
</ul>
<p>Again, to illustrate, let’s generate a series of 100 observations that follow the process: <span class="math inline">\(y_t=1.1y_{t-1}-0.4y_{t-2}+\varepsilon_t\)</span>, where <span class="math inline">\(y_{-1}=y_0=0\)</span> and <span class="math inline">\(\varepsilon\sim N(0,1)\)</span>, and plot the ACF and PACF of this series.</p>
<p><img src="forecasting_files/figure-html/unnamed-chunk-11-1.png" width="624"></p>
</div>
</div>
<div id="forecasting-1" class="section level2" number="5.2">
<h2>
<span class="header-section-number">5.2</span> Forecasting<div class="tooltip"><button class="internal-link-btn" onclick="copy_link('forecasting-1')" onmouseout="reset_tooltip('forecasting-1-tooltip')"><span class="tooltiptext" id="forecasting-1-tooltip">Copy link</span><i class="fa fa-link"></i></button></div>
</h2>
<p>Making forecasts for some future period, <span class="math inline">\(t+h\)</span>, from an AR(p) model that has been fit to the data up to and including period <span class="math inline">\(t+h-1\)</span> can be a straightforward exercise, so long as we have access to the relevant information set. For one-step-ahead forecasts, the information set is readily available. For multi-step-ahead forecasts, we need to ‘come up’ with the value of the variable that has not been realized yet. For example, when making a two-step-ahead forecast for period <span class="math inline">\(t+2\)</span>, we need data from period <span class="math inline">\(t+1\)</span>, which is not available at the time when the forecast is made. Instead, we need to use our forecast for period <span class="math inline">\(t+1\)</span>. The same applies to forecasts for any subsequent periods in the future. This approach is known as an <em>iterative</em> method of forecasting, wherein we make forecast for some period using the available data, then iterate forward by one period and use the most recent forecast to make the next period’s forecast, and so on and so forth.</p>
<div id="one-step-ahead-forecast-from-ar1" class="section level3" number="5.2.1">
<h3>
<span class="header-section-number">5.2.1</span> One-step-ahead forecast from AR(1)<div class="tooltip"><button class="internal-link-btn" onclick="copy_link('one-step-ahead-forecast-from-ar1')" onmouseout="reset_tooltip('one-step-ahead-forecast-from-ar1-tooltip')"><span class="tooltiptext" id="one-step-ahead-forecast-from-ar1-tooltip">Copy link</span><i class="fa fa-link"></i></button></div>
</h3>
<p>The realization of the random variable in period <span class="math inline">\(t+1\)</span> is: <span class="math display">\[y_{t+1} = \alpha + \beta_1 y_{t} + \varepsilon_{t+1}\]</span></p>
<p>The optimal one-step-ahead forecast is: <span class="math display">\[y_{t+1|t} = E(y_{t+1}|\Omega_t) = E(\alpha + \beta_1 y_{t} + \varepsilon_{t+1}) = \alpha + \beta_1 y_{t}\]</span></p>
<p>The one-step-ahead forecast error is: <span class="math display">\[e_{t+1|t} = y_{t+1} - y_{t+1|t} = \alpha + \beta_1 y_t + \varepsilon_{t+1} - (\alpha + \beta_1 y_t) = \varepsilon_{t+1}\]</span></p>
<p>The one-step-ahead forecast variance: <span class="math display">\[\sigma_{t+1|t}^2 = Var(y_{t+1}|\Omega_t) = E(e_{t+1|t}^2) = E(\varepsilon_{t+1}^2) = \sigma_{\varepsilon}^2\]</span></p>
<p>The one-step-ahead (95%) interval forecast: <span class="math display">\[y_{t+1|t} \pm z_{.025}\sigma_{t+1|t} = y_{t+1|t} \pm 1.96\sigma_{\varepsilon}\]</span></p>
</div>
<div id="two-step-ahead-forecast-from-ar1" class="section level3" number="5.2.2">
<h3>
<span class="header-section-number">5.2.2</span> Two-step-ahead forecast from AR(1)<div class="tooltip"><button class="internal-link-btn" onclick="copy_link('two-step-ahead-forecast-from-ar1')" onmouseout="reset_tooltip('two-step-ahead-forecast-from-ar1-tooltip')"><span class="tooltiptext" id="two-step-ahead-forecast-from-ar1-tooltip">Copy link</span><i class="fa fa-link"></i></button></div>
</h3>
<p>The realization of the random variable in period <span class="math inline">\(t+2\)</span> is: <span class="math display">\[y_{t+2} = \alpha + \beta_1 y_{t+1} + \varepsilon_{t+2}\]</span></p>
<p>The optimal two-step-ahead forecast is: <span class="math display">\[y_{t+2|t} = E(y_{t+2}|\Omega_t) = E(\alpha + \beta_1 y_{t+1} + \varepsilon_{t+2}) = \alpha(1+\beta_1) + \beta_1^2 y_t\]</span></p>
<p>Note, that here we substituted <span class="math inline">\(y_{t+1}\)</span> with <span class="math inline">\(\alpha + \beta_1 y_t + \varepsilon_{t+1}\)</span>.</p>
<p>The two-step-ahead forecast error is:
<span class="math display">\[\begin{aligned}
e_{t+2|t} &amp;= y_{t+2} - y_{t+2|t} \\
&amp;= \alpha(1+\beta_1) + \beta_1^2 y_t + \beta_1\varepsilon_{t+1} + \varepsilon_{t+2} - [\alpha(1+\beta_1) + \beta_1^2 y_t] \\
&amp;= \beta_1\varepsilon_{t+1} + \varepsilon_{t+2}
\end{aligned}\]</span></p>
<p>The two-step-ahead forecast variance is:
<span class="math display">\[\begin{aligned}
\sigma_{t+2|t}^2 &amp;= Var(y_{t+2}|\Omega_t) \\
&amp;= E(e_{t+2|t}^2) = E(\beta_1\varepsilon_{t+1} + \varepsilon_{t+2})^2 = \sigma_{\varepsilon}^2(1+\beta_1^2)
\end{aligned}\]</span></p>
<p>The two-step-ahead (95%) interval forecast: <span class="math display">\[y_{t+2|t} \pm z_{.025}\sigma_{t+2|t} = y_{t+2|t} \pm 1.96\sigma_{\varepsilon}\sqrt{1+\beta_1^2}\]</span></p>
</div>
<div id="h-step-ahead-forecast-from-ar1" class="section level3" number="5.2.3">
<h3>
<span class="header-section-number">5.2.3</span> h-step-ahead forecast from AR(1)<div class="tooltip"><button class="internal-link-btn" onclick="copy_link('h-step-ahead-forecast-from-ar1')" onmouseout="reset_tooltip('h-step-ahead-forecast-from-ar1-tooltip')"><span class="tooltiptext" id="h-step-ahead-forecast-from-ar1-tooltip">Copy link</span><i class="fa fa-link"></i></button></div>
</h3>
<p>The realization of the random variable in period <span class="math inline">\(t+h\)</span> is: <span class="math display">\[y_{t+h} = \alpha + \beta_1 y_{t+h-1} + \varepsilon_{t+h}\]</span></p>
<p>The optimal h-step-ahead forecast: <span class="math display">\[y_{t+h|t} = E(y_{t+h}|\Omega_t) = E(\alpha + \beta_1 y_{t+h-1} + \varepsilon_{t+1}) = \alpha\textstyle\sum_{j=0}^{h-1}\beta_1^j + \beta_1^h y_t\]</span></p>
<p>The h-step-ahead forecast error: <span class="math display">\[e_{t+h|t} = y_{t+h} - y_{t+h|t} = \textstyle\sum_{j=0}^{h-1}\beta_1^j\varepsilon_{t+h-j}\]</span></p>
<p>The h-step-ahead forecast variance: <span class="math display">\[\sigma_{t+h|t}^2 = Var(y_{t+h}|\Omega_t) = E(e_{t+h|t}^2) = \sigma_{\varepsilon}^2\textstyle\sum_{j=0}^{h-1}\beta_1^{2j}\]</span></p>
<p>The h-step-ahead (95%) interval forecast: <span class="math display">\[y_{t+h|t} \pm z_{.025}\sigma_{t+h|t} = y_{t+1|t} \pm 1.96\sigma_{\varepsilon}\sqrt{\textstyle\sum_{j=0}^{h-1}\beta_1^{2j}}\]</span></p>
<p>If the series represent a covariance-stationary process, i.e. when <span class="math inline">\(|\beta_1| &lt; 1\)</span>, as <span class="math inline">\(h \to \infty\)</span>:</p>
<p>The optimal point forecast converges to: <span class="math display">\[y_{t+h|t} = \frac{\alpha}{1-\beta_1}\]</span></p>
<p>The forecast variance converges to: <span class="math display">\[\sigma_{t+h|t}^2 = \frac{\sigma_{\varepsilon}^2}{1-\beta_1^2}\]</span></p>
<p>The (95%) interval forecast converges to: <span class="math display">\[y_{t+h|t} \pm z_{.025}\sigma_{t+h|t} = \frac{\alpha}{1-\beta_1} \pm 1.96\frac{\sigma_{\varepsilon}}{\sqrt{1-\beta_1^2}}\]</span></p>
</div>
<div id="one-step-ahead-forecast-from-ar2" class="section level3" number="5.2.4">
<h3>
<span class="header-section-number">5.2.4</span> One-step-ahead forecast from AR(2)<div class="tooltip"><button class="internal-link-btn" onclick="copy_link('one-step-ahead-forecast-from-ar2')" onmouseout="reset_tooltip('one-step-ahead-forecast-from-ar2-tooltip')"><span class="tooltiptext" id="one-step-ahead-forecast-from-ar2-tooltip">Copy link</span><i class="fa fa-link"></i></button></div>
</h3>
<p>The realization of the random variable in period <span class="math inline">\(t+1\)</span> is: <span class="math display">\[y_{t+1} = \alpha + \beta_1 y_{t} + \beta_2 y_{t-1} + \varepsilon_{t+1}\]</span></p>
<p>The optimal one-step-ahead forecast:
<span class="math display">\[\begin{aligned}
y_{t+1|t} &amp;= E(y_{t+1}|\Omega_t) \\
&amp;= E(\alpha + \beta_1 y_{t} + \beta_2 y_{t-1} + \varepsilon_{t+1}) = \alpha + \beta_1 y_{t} + \beta_2 y_{t-1}
\end{aligned}\]</span></p>
<p>The one-step-ahead forecast error:
<span class="math display">\[\begin{aligned}
e_{t+1|t} &amp;= y_{t+1} - y_{t+1|t} \\
&amp;= \alpha + \beta_1 y_t + \beta_2 y_{t-1} + \varepsilon_{t+1} - (\alpha + \beta_1 y_t + \beta_2 y_{t-1}) = \varepsilon_{t+1}
\end{aligned}\]</span></p>
<p>The one-step-ahead forecast variance: <span class="math display">\[\sigma_{t+1|t}^2 = Var(y_{t+1}|\Omega_t) = E(e_{t+1|t}^2) = E(\varepsilon_{t+1}^2) = \sigma_{\varepsilon}^2\]</span></p>
<p>The one-step-ahead (95%) interval forecast: <span class="math display">\[y_{t+1|t} \pm z_{.025}\sigma_{t+1|t} = y_{t+1|t} \pm 1.96\sigma_{\varepsilon}\]</span></p>
</div>
<div id="two-step-ahead-forecast-from-ar2" class="section level3" number="5.2.5">
<h3>
<span class="header-section-number">5.2.5</span> Two-step-ahead forecast from AR(2)<div class="tooltip"><button class="internal-link-btn" onclick="copy_link('two-step-ahead-forecast-from-ar2')" onmouseout="reset_tooltip('two-step-ahead-forecast-from-ar2-tooltip')"><span class="tooltiptext" id="two-step-ahead-forecast-from-ar2-tooltip">Copy link</span><i class="fa fa-link"></i></button></div>
</h3>
<p>The realization of the random variable in period <span class="math inline">\(t+2\)</span> is: <span class="math display">\[y_{t+2} = \alpha + \beta_1 y_{t+1} + \beta_2 y_{t} + \varepsilon_{t+2}\]</span></p>
<p>The optimal two-step-ahead forecast:
<span class="math display">\[\begin{aligned}
y_{t+2|t} = E(y_{t+2}|\Omega_t) &amp;= E(\alpha + \beta_1 y_{t+1} + \beta_2 y_{t} + \varepsilon_{t+1}) \\
&amp;= \alpha(1+\beta_1) + (\beta_1^2+\beta_2) y_{t} + \beta_1\beta_2 y_{t-1}
\end{aligned}\]</span></p>
<p>The two-step-ahead forecast error:
<span class="math display">\[\begin{aligned}
e_{t+2|t} = y_{t+2} - y_{t+2|t} =&amp; \alpha + \beta_1 y_{t+1} + \beta_2 y_{t} + \varepsilon_{t+2} \\
&amp;- (\alpha + \beta_1 y_{t+1|t} + \beta_2 y_{t}) = \beta_1\varepsilon_{t+1} + \varepsilon_{t+2}
\end{aligned}\]</span></p>
<p>The two-step-ahead forecast variance:
<span class="math display">\[\sigma_{t+2|t}^2 = Var(y_{t+2}|\Omega_t) = E(e_{t+2|t}^2) = E(\beta_1\varepsilon_{t+1} + \varepsilon_{t+2})^2 = \sigma_{\varepsilon}^2(1+\beta_1^2)\]</span></p>
<p>The two-step-ahead (95%) interval forecast: <span class="math display">\[y_{t+2|t} \pm z_{.025}\sigma_{t+2|t} = y_{t+2|t} \pm 1.96\sigma_{\varepsilon}\sqrt{1+\beta_1^2}\]</span></p>
</div>
<div id="one-step-ahead-forecast-from-ar2-1" class="section level3" number="5.2.6">
<h3>
<span class="header-section-number">5.2.6</span> One-step-ahead forecast from AR(2)<div class="tooltip"><button class="internal-link-btn" onclick="copy_link('one-step-ahead-forecast-from-ar2-1')" onmouseout="reset_tooltip('one-step-ahead-forecast-from-ar2-1-tooltip')"><span class="tooltiptext" id="one-step-ahead-forecast-from-ar2-1-tooltip">Copy link</span><i class="fa fa-link"></i></button></div>
</h3>
<p>The realization of the random variable in period <span class="math inline">\(t+h\)</span> is: <span class="math display">\[y_{t+h} = \alpha + \beta_1 y_{t+h-1} + \beta_2 y_{t+h-2} + \varepsilon_{t+h}\]</span></p>
<p>The optimal h-step-ahead forecast (iterated method):
<span class="math display">\[\begin{aligned}
y_{t+1|t} &amp;= \alpha + \beta_1 y_t + \beta_2 y_{t-1} \\
y_{t+2|t} &amp;= \alpha + \beta_1 y_{t+1|t} + \beta_2 y_{t} \\
y_{t+3|t} &amp;= \alpha + \beta_1 y_{t+2|t} + \beta_2 y_{t+1|t} \\
&amp;\vdots \\
y_{t+h|t} &amp;= \alpha + \beta_1 y_{t+h-1|t} + \beta_2 y_{t+h-2|t}
\end{aligned}\]</span></p>
<p>The h-step-ahead forecast error: <span class="math display">\[e_{t+h|t} = y_{t+h} - y_{t+h|t} = \varepsilon_{t+h}+\beta_1 e_{t+h-1|t}+\beta_2 e_{t+h-2|t}\]</span></p>
<p>The h-step-ahead forecast variance:
<span class="math display">\[\begin{aligned}
\sigma_{t+h|t}^2 &amp;= Var(y_{t+h}|\Omega_t) = E(e_{t+h|t}^2) \\
&amp;= \sigma_{\varepsilon}^2+\beta_1^2 Var(e_{t+h-1|t})+\beta_2^2 Var(e_{t+h-2|t}) \\
&amp;+2\beta_1\beta_2Cov(e_{t+h-1|t},e_{t+h-2|t})
\end{aligned}\]</span></p>
<p>(Note: the formulas for <span class="math inline">\(\sigma_{t+1|t}^2,\sigma_{t+2|t}^2,\ldots,\sigma_{t+h|t}^2\)</span> are the same for any <span class="math inline">\(AR(p)\)</span>, <span class="math inline">\(p \geq h-1\)</span>).</p>
<p>The h-step-ahead (95%) interval forecast: <span class="math display">\[y_{t+h|t} \pm z_{.025}\sigma_{t+h|t} = y_{t+1|t} \pm 1.96\sigma_{t+h|t}\]</span></p>
<p>The optimal h-step-ahead forecast: <span class="math display">\[y_{t+h|t} = E(y_{t+h}|\Omega_t) = \alpha + \beta_1 y_{t+h-1|t} + \beta_2 y_{t+h-2|t} + \cdots + \beta_p y_{t+h-p|t}\]</span></p>
<p>The h-step-ahead forecast error: <span class="math display">\[e_{t+h|t} = \varepsilon_{t+h} + \beta_1 e_{t+h-1|t} + \beta_2 e_{t+h-2|t} + \cdots + \beta_p e_{t+h-p|t}\]</span></p>
<p>The h-step-ahead forecast variance:
<span class="math display">\[\begin{aligned}
\sigma_{t+h|t}^2 &amp; = Var(y_{t+h}|\Omega_t) = E(e_{t+2|t}^2) \\
&amp;= \sigma_{\varepsilon}^2 + \sum_{i=1}^{p}\beta_i^2 Var(e_{t+h-i|t}) + 2\sum_{i \neq j}\beta_i\beta_j Cov(e_{t+h-i|t},e_{t+h-j|t})
\end{aligned}\]</span></p>
<p>The h-step-ahead (95%) interval forecast: <span class="math display">\[y_{t+h|t} \pm z_{.025}\sigma_{t+h|t} = y_{t+h|t} \pm 1.96\sigma_{t+h|t}\]</span></p>

</div>
</div>
</div>
<p style="text-align: center;">
<a href="trends-and-seasonality.html"><button class="btn btn-default">Previous</button></a>
<a href="vector-autoregressive-models.html"><button class="btn btn-default">Next</button></a>
</p>
<p class="build-date">Page built: 
2022-08-04
 using 
R version 4.1.2 (2021-11-01)
</p>
</div>
</div>



</body>
</html>
