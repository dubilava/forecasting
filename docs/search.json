[{"path":"index.html","id":"preamble","chapter":"Preamble","heading":"Preamble","text":"notes prepared teach undergraduate-level course economic forecasting. content presented four parts. first part, Preliminaries, introduces concept forecasting time series context. second part, Deterministic Time Series Models, covers trends seasonal models. third part, Dynamic Time Series Models, covers linear, multivariate, nonlinear autoregressive processes. fourth part, Forecast Assessment, goes forecast evaluation combination routines, forecast accuracy tests.","code":""},{"path":"preliminaries.html","id":"preliminaries","chapter":"Preliminaries","heading":"Preliminaries","text":"","code":""},{"path":"introduction-to-forecasting.html","id":"introduction-to-forecasting","chapter":"1 Introduction to Forecasting","heading":"1 Introduction to Forecasting","text":"Economic events tend co-occur, precede, succeed one another. Understanding essence relationships – , identifying causal mechanisms facilitate correlation among economic variables – core econometric analysis. Throughout relatively brief history study econometrics, numerous methods techniques proposed developed – aimed give empirical content economic models. methods techniques allow us test theories, evaluate policy outcomes, etc.core, econometric model aims correct (accurate) identification causal mechanism underlying process. econometric models also predictive nature. help us make economic forecasts even causal mechanism may well identified. words, correlation necessarily imply causation, goal analyst make forecast, mere correlation might well suffice.Roots forecasting extend much beginning human history. desire predict future, people attempted make forecasts , used services others. Fortunetellers, example, forecast experts sort, basing predictions magic. less common current age. Astrologers, rely astronomical phenomena foresee future, maintain relevance date. time, particularly development study econometrics, rigorous forecasting methods introduced developed. methods – primitive complex, spurious scientifically substantiated – one thing common: rely (, least, pretend rely) information.Information key forecasting. comes many forms, organized stored, end data. Information cumulative. contains everything know point time. , information set given \\(\\Omega_t = \\{x_t,x_{t-1},x_{t-2},\\ldots\\}\\), \\(\\{x_{t-j}:j=0,1,2,\\ldots\\}\\) observed data form time series.diverse set forecasting methods typically rely insights econometric analysis time series – historical data collected regular intervals stored chronologically. time series analysis, implied assumption past tends repeat , least extent. , well study past, may able forecast event degree accuracy.Accurate forecasting difficult. Indeed, forecasting difficult. , forecast, bound commit forecast errors. sources errors : model uncertainty (bias), parameter uncertainty (variance), forecast uncertainty. can try minimize first two, example, applying model mimics true model exceptionally well, using bigger nuanced data. matter well econometric model fits data, rich available data , still surprise element concerning forecast – something never happened past, specific future. , thing precise forecast, even fluke exactly predict outcome event. forecasts better others. search forecasts study time series econometrics evolved.","code":""},{"path":"stochastic-process-and-time-series.html","id":"stochastic-process-and-time-series","chapter":"2 Stochastic Process and Time Series","heading":"2 Stochastic Process and Time Series","text":"time series realization sequence random variables stored chronologically. sequence referred stochastic process. Thus, time series realization stochastic process. index time periods \\(1,2,\\ldots,T\\), stochastic process given \\(\\{Y_t:t=1,\\ldots,T\\}\\). time series represented set observations: \\(\\{y_1,\\ldots,y_T\\}\\). One can view time series finite sample underlying doubly–infinite sequence: \\(\\{\\ldots,y_{-1},y_{0},y_1,y_2,\\ldots,y_T,y_{T+1},y_{T+2},\\ldots\\}\\). say history extends beyond starting ending time periods sample hand.","code":""},{"path":"stochastic-process-and-time-series.html","id":"stationarity","chapter":"2 Stochastic Process and Time Series","heading":"2.1 Stationarity","text":"random variables, time series drawn, distribution, refer data stationary time series. Stationarity important feature, assumption, time series analysis heavily relies.diving concepts methods time series econometrics, consider simplest kind time series comprised realizations independent identically distributed normal random variable zero mean constant variance: \\(\\varepsilon_t \\sim iid\\left(0,\\sigma^2\\right)\\). following graph plots time series time.\nFigure 2.1: White noise: illustration\ntime series referred white noise. , \\(\\{y_t: t=1,\\ldots,T\\}\\), white noise process :\n\\[\\begin{align*}\n& E(y_t) = 0,\\;~\\forall~t\\\\\n& Var(y_t) = \\sigma^2,\\;~\\forall~t\\\\\n& Cov(y_t,y_{t-k}) = 0,\\;~\\forall~k \\ne 0\n\\end{align*}\\]observation drawn distribution, white noise stationary time series. Indeed, special type stationary time series insofar mean, variance, covariance time-invariant. Note, time series stationary, neither mean covariances need equal zero. Thus, \\(\\{y_t\\}\\) stationary mean variance independent \\(t\\), autocovariances independent \\(t\\) \\(k\\).","code":""},{"path":"stochastic-process-and-time-series.html","id":"serial-dependence","chapter":"2 Stochastic Process and Time Series","heading":"2.2 Serial Dependence","text":"norm rather exception time series correlated time. Indeed, sequential nature time series, commonly observe dependence among temporally adjacent time series. , economic time series, expect \\(y_t\\) \\(y_{t-1}\\) correlated. correlation, referred first order autocorrelations, given : \\(\\rho_1=Cor(y_{t},y_{t-1}) = \\frac{Cov(y_{t},y_{t-k})}{Var(y_t)}\\). general, \\(k^{th}\\) order autocorrelation given : \\[\\rho_k=Cor(y_{t},y_{t-k}) = \\frac{Cov(y_{t},y_{t-k})}{Var(y_{t})},\\;~~k=1,2,\\ldots\\]Autocorrelations commonly illustrated via -called autocorrelogram, plots sequence autocorrelation coefficients lags coefficients obtained. example, autocorrelogram previously illustrated white noise process follows:\nFigure 2.2: Autocorrelation\n\\(k\\), vertical line extending zero represents autocorrelation coefficient lag. red dashed lines denote 90% confidence interval, given \\(\\pm 1.96/\\sqrt{T}\\), \\(T\\) length time series.Another relevant measure time series dependence partial autocorrelation, correlation \\(y_t\\) \\(y_{t-k}\\) net correlations \\(y_t\\) \\(y_{t-k+j}\\), \\(j=1,\\ldots,k-1\\). Similar autocorrelations, partial autocorrelations can also illustrated using autocorrelograms:\nFigure 2.3: Partial Autocorrelation\n","code":""},{"path":"stochastic-process-and-time-series.html","id":"transformations","chapter":"2 Stochastic Process and Time Series","heading":"2.3 Transformations","text":"common transform time series taking logarithms, differences, differences logarithms (growth rates). transformations usually done work suitable variable desired econometric analysis. example, economic time series characterized apparent exponential growth (e.g., real GDP), taking natural logarithms time series ``flatten’’ fluctuations become proportionate. difference operator denoted \\(\\Delta\\), \\(\\Delta y_t = y_t-y_{t-1}\\). following three graphs illustrate () time series apparent exponential growth, (ii) natural logarithm time series, (iii) differences (.e., log-differences original series).\nFigure 2.4: time series transformations\n","code":""},{"path":"forecasting-methods-and-routines.html","id":"forecasting-methods-and-routines","chapter":"3 Forecasting Methods and Routines","heading":"3 Forecasting Methods and Routines","text":"","code":""},{"path":"forecasting-methods-and-routines.html","id":"optimal-forecast","chapter":"3 Forecasting Methods and Routines","heading":"3.1 Optimal Forecast","text":"forecast random variable distribution , thus, moments. simplest form forecast point forecast (usually mean distribution, can median , indeed, quantile).point forecast made period \\(t\\) horizon \\(h\\) can denoted \\(y_{t+h|t}\\); ‘best guess’, made period \\(t\\), actual realization random variable period \\(t+h\\), denoted \\(y_{t+h}\\). difference two forecast error. , \\[e_{t+h|t} = y_{t+h} - y_{t+h|t}\\]accurate forecast smaller forecast error.Three types uncertainty contribute forecast error:\n\\[\\begin{aligned}\n        e_{t+h|t} & = \\big[y_{t+h}-E(y_{t+h}|\\Omega_{t})\\big]\\;~~\\text{(forecast uncertainty)}  \\\\\n        & + \\big[E(y_{t+h}|\\Omega_{t}) - g(\\Omega_{t};\\theta)\\big]\\;~~\\text{(model uncertainty)}  \\\\\n        & + \\big[g(\\Omega_{t};\\theta)-g(\\Omega_{t};\\hat{\\theta})\\big]\\;~~\\text{(parameter uncertainty)}\n        \\end{aligned}\\]\\(\\Omega_t\\) denotes information set available time forecast made; \\(g(\\cdot)\\) functional form model applied data; \\(\\theta\\) set parameters model, \\(\\hat{\\theta}\\) estimatesBecause uncertainty avoided, forecaster bound commit forecast errors. goal forecaster minimize ‘cost’ associated forecast errors. achieved minimizing expected loss function.loss function, \\(L(e_{t+h|t})\\), can take many different forms, satisfy following properties:\n\\[\\begin{aligned}\n        & L(e_{t+h|t}) = 0,\\;~~\\forall\\;e_{t+h|t} = 0 \\\\\n        & L(e_{t+h|t}) \\geq 0,\\;~~\\forall\\;e_{t+h|t} \\neq 0 \\\\\n        & L(e_{t+h|t}^{()}) > L(e_{t+h|t}^{(j)}),\\;~~\\forall\\;|e_{t+h|t}^{()}| > |e_{t+h|t}^{(j)}|\n        \\end{aligned}\\]Two commonly used symmetric loss functions absolute quadratic loss functions:\n\\[\\begin{aligned}\n        & L{(e_{t+h|t})} = |e_{t+h|t}|\\;~~\\text{(absolute loss function)} \\\\\n        & L{(e_{t+h|t})} = (e_{t+h|t})^2\\;~~\\text{(quadratic loss function)}\n        \\end{aligned}\\]quadratic loss function popular, partly typically select models based ‘-sample’ quadratic loss (.e. minimizing sum squared residuals).Optimal forecast forecast minimizes expected loss:\n\\[\\min_{y_{t+h|t}} E\\left[L\\left(e_{t+h|t}\\right)\\right] = \\min_{y_{t+h|t}} E\\left[L\\left(y_{t+h}-y_{t+h|t}\\right)\\right]\\]\nexpected loss given :\n\\[E\\left[L\\left(y_{t+h}-y_{t+h|t}\\right)\\right]=\\int L\\left(y_{t+h}-y_{t+h|t}\\right) f(y_{t+h}|\\Omega_t)dy\\]can assume conditional density normal density mean \\(\\mu_{t+h} \\equiv E(y_{t+h})\\), variance \\(\\sigma_{t+h}^2 \\equiv Var(y_{t+h})\\).assumption quadratic loss function:\n\\[\\begin{aligned}\n        E\\left[L(e_{t+h|t})\\right] & = E(e_{t+h|t}^2) = E(y_{t+h} - \\hat{y}_{t+h|t})^2 \\\\\n        & = E(y_{t+h}^2)-2E(y_{t+h})\\hat{y}_{t+h|t} + \\hat{y}_{t+h|t}^2\n        \\end{aligned}\\]solving optimization problem follows : \\[\\hat{y}_{t+h|t} = E(y_{t+h}) \\equiv \\mu_{t+h}\\]Thus, optimal point forecast quadratic loss mean (reference, optimal point forecast absolute loss median).","code":""},{"path":"forecasting-methods-and-routines.html","id":"measuring-forecast-accuracy","chapter":"3 Forecasting Methods and Routines","heading":"3.2 Measuring Forecast Accuracy","text":"Forecast accuracy determined considering well model performs data used estimation. assess forecast accuracy need access data, typically future time periods, used estimation. leads -called ‘pseudo forecasting’ routine. routine involves splitting available data two segments referred ‘-sample’ ‘--sample’. -sample segment series also known ‘estimation set’ ‘training set’. --sample segment series also known ‘hold-set’ ‘test set’.Thus, make -called ‘genuine’ forecasts using information estimation set, assess accuracy forecasts --sample setting.forecasting often performed time series context, estimation set typically predates hold-set. non-dynamic settings chronological ordering may necessary, however.different forecasting schemes updating information set pseudo-forecasting routine. : recursive, rolling, fixed.recursive forecasting environment uses sequence expanding windows update model estimates information set.rolling forecasting environment uses sequence rolling windows size update model estimates information set.fixed forecasting environment uses one fixed window model estimates, updates information set.following animation illustrates distinctive features three routines:","code":""},{"path":"forecasting-methods-and-routines.html","id":"evaluating-time-series-forecasts","chapter":"3 Forecasting Methods and Routines","heading":"3.3 Evaluating Time Series Forecasts","text":"evaluate forecasts time series, \\(\\{y_t\\}\\), total \\(T\\) observations, divide sample two parts, -sample set total \\(R\\) observations, \\(R < T\\) (typically, \\(R \\approx 0.75T\\)), --sample set.example, interested one-step-ahead forecast assessment, way produce sequence forecasts: \\(\\{y_{R+1|R},y_{R+2|{R+1}},\\ldots,y_{T|{T-1}}\\}\\) \\(\\{Y_{R+1},Y_{R+2},\\ldots,Y_{T}\\}\\).Forecast errors, \\(e_{R+j} = y_{R+j} - y_{R+j|{R+j-1}}\\), can computed \\(j = 1,\\ldots,T-R\\).commonly applied accuracy measures mean absolute forecast error (MAFE) root mean squared forecast error (RMSFE):\n\\[\\begin{aligned}\n\\text{MAFE}  = & \\frac{1}{P}\\sum_{=1}^{P}|e_i|\\\\\n\\text{RMSFE} = & \\sqrt{\\frac{1}{P}\\sum_{=1}^{P}e_i^2}\n\\end{aligned}\\]\n\\(P\\) total number --sample forecasts. lower accuracy measure (choice), better given model performs generating accurate forecasts. noted earlier, ‘better’ mean ‘without errors’.Forecast errors ‘good’ forecasting method following properties:zero mean; otherwise, forecasts biased.correlation forecasts; otherwise, information left used computing forecasts.serial correlation among one-step-ahead forecast errors. Note \\(k\\)-step-ahead forecasts, \\(k>1\\), can , usually , serially correlated.forecasting method satisfy properties potential improved.","code":""},{"path":"forecasting-methods-and-routines.html","id":"unbiasedness","chapter":"3 Forecasting Methods and Routines","heading":"3.3.1 Unbiasedness","text":"Testing \\(E(e_{t+h|t})=0\\). Set regression: \\[e_{t+h|t} = \\alpha+\\upsilon_{t+h} \\hspace{.5in} t = R,\\ldots,T-h,\\]\n\\(R\\) estimation window size, \\(T\\) sample size, \\(h\\) forecast horizon length. null zero-mean forecast error equivalent testing \\(H_0: \\alpha = 0\\) OLS setting. \\(h\\)-step-ahead forecast errors, \\(h>1\\), autocorrelation consistent standard errors used.","code":""},{"path":"forecasting-methods-and-routines.html","id":"efficiency","chapter":"3 Forecasting Methods and Routines","heading":"3.3.2 Efficiency","text":"Testing \\(Cov(e_{t+h|t},y_{t+h|t})=0\\). Set regression: \\[e_{t+h|t} = \\alpha + \\beta y_{t+h|t} + \\upsilon_{t+h} \\hspace{.5in} t = R,\\ldots,T-h.\\] null forecast error independence information set equivalent testing \\(H_0: \\beta = 0\\) OLS setting. \\(h\\)-step-ahead forecast errors, \\(h>1\\), autocorrelation consistent standard errors used.","code":""},{"path":"forecasting-methods-and-routines.html","id":"no-autocorrelation","chapter":"3 Forecasting Methods and Routines","heading":"3.3.3 No Autocorrelation","text":"Testing \\(Cov(e_{t+1|t},e_{t|t-1})=0\\). Set regression: \\[e_{t+1|t} = \\alpha + \\gamma e_{t|t-1} + \\upsilon_{t+1} \\hspace{.5in} t = R+1,\\ldots,T-1.\\] null forecast error autocorrelation equivalent testing \\(H_0: \\gamma = 0\\) OLS setting.","code":""},{"path":"deterministic-time-series-models.html","id":"deterministic-time-series-models","chapter":"Deterministic Time Series Models","heading":"Deterministic Time Series Models","text":"","code":""},{"path":"trends.html","id":"trends","chapter":"4 Trends","heading":"4 Trends","text":"Economic time series usually characterized trending behavior, often present seasonal pattern well. Trend unidirectional change time series extended period time arises accumulation information time. Seasonality repeating pattern within calendar year arises links technologies, preferences, institutions calendar. Modeling forecasting time series features fairly straightforward task. get , let’s discuss may happen ignore presence trends /seasonality analyzing time series data.","code":""},{"path":"trends.html","id":"spurious-relationship","chapter":"4 Trends","heading":"4.1 Spurious Relationship","text":"Nothing trending time series necessarily violates classical linear regression model assumptions. issue may arise, however, unobserved trending variable simultaneously correlated dependent variable well one independent variables time series regression. case, may find (statistically significant) relationship two unrelated economic variables simply trending. relationship referred spurious relationship.illustrate, consider two trending variables: \\[y_t = \\gamma t + \\nu_t,\\;~~\\nu\\sim N(0,\\sigma_{\\nu}^2),\\] \\[x_t = \\delta t + \\upsilon_t,\\;~~\\upsilon\\sim N(0,\\sigma_{\\upsilon}^2),\\] \\(Cov(\\nu_t,\\upsilon_t) = 0\\). simplicity, can assume \\(\\sigma_{\\nu}^2=\\sigma_{\\upsilon}^2=1\\). Suppose, \\(\\gamma\\) \\(\\delta\\) positive scalars, say, \\(0.3\\) \\(0.5\\), respectively. , \\(y\\) \\(x\\) trending direction. example time series:\nestimate \\[y_t = \\alpha+\\beta x_t + \\varepsilon_t,\\] likely find relationship two – case \\(\\beta>0\\) – even though, know, two related. illustrate , generate 1000 samples size 120 \\(y\\) \\(x\\), case estimate parameter \\(\\beta\\). following graph illustrates empirical distribution parameter estimates:\nLuckily, can easily “fix” issue, incorporating trend regression: \\[y_t = \\alpha+\\beta x_t + \\eta t + \\varepsilon_t.\\] trend accounted , previously illustrated “bias” disappears. Using similar simulation exercise , following graph illustrates empirical distribution parameter estimates:\nfact, “fix” equivalent regressing de-trended \\(y\\) de-trended \\(x\\). de-trend variable, first run regression: \\(y_t = \\gamma_0 + \\gamma_1 t + \\nu_t\\), obtain fitted values fixed trend (typically zero), : \\(\\tilde{y}_t = \\hat{\\gamma}_0+\\hat{\\nu}_t\\), \\(\\hat{\\gamma}_0\\) \\(\\hat{\\nu}_t\\) parameter estimate residuals foregoing regression.","code":""},{"path":"trends.html","id":"modeling","chapter":"4 Trends","heading":"4.2 Modeling","text":"seen, accounting trends time series can help us resolve regression issues. trend can inherent feature times series. end, can apply deterministic trends forecast time series.simplest (perhaps frequently applied) model account trending time series linear trend model: \\[y_t = \\alpha + \\beta t\\]likely candidate trend specifications polynomial (e.g. quadratic, cubic, etc.), exponential, shifting (switching) trend models, respectively given :\n\\[\\begin{aligned}\n    y_t &= \\alpha + \\beta_1 t + \\beta_2 t^2 + \\ldots + \\beta_p t^p \\\\\n    y_t &= e^{\\alpha + \\beta t}\\;~~\\mbox{}\\;~~\\ln{y_t} = \\alpha + \\beta t \\\\\n    y_t &= \\alpha + \\beta_1 t + \\beta_2 (t-\\tau)(t>\\tau),\\;~~\\tau\\\\mathsf{T} \n    \\end{aligned}\\], primarily consider linear quadratic trends. exponential trend, standpoint modeling forecasting, equivalent linear trend fitted natural logarithm series. time series \\(\\{y_t: t=1,\\ldots,T\\}\\), natural logarithm : \\(z_t = \\ln{y_t}\\). benefits transformation :easier interpret (relative/percentage change).homogenizes variance time series.may result improved forecasting accuracy.Exponential trends suitable time series characterized stable relative change time (e.g., economic time series grow 2% every year).cover shifting/switching trend models another chapter.Trends (relatively) easy model forecast. Caution needed, however, (higher order) polynomial trends, may fit well -sample, cause major problems --sample.Consider linear trend model additive error term: \\[y_t = \\alpha + \\beta t + \\varepsilon_t\\] estimate model parameters, \\(\\mathbf{\\theta}=\\{\\alpha,\\beta\\}\\), fitting trend model time series using least-squares regression: \\[\\hat{\\theta} = \\operatorname*{argmin}_{\\mathbf{\\theta}} \\sum_{t=1}^{T}\\big(y_t - \\alpha - \\beta t\\big)^2.\\] Fitted values given : \\[\\hat{y}_t = \\hat{\\alpha} + \\hat{\\beta} t\\]","code":""},{"path":"trends.html","id":"forecasting","chapter":"4 Trends","heading":"4.3 Forecasting","text":"linear trend model fitted data, future realization stochastic process assumed follow linear trend model: \\[y_{t+h} = \\alpha + \\beta (t+h) + \\varepsilon_{t+h}.\\]optimal forecast \\(y_{t+h}\\), therefore, given : \\[y_{t+h|t} = E(y_{t+h}|\\Omega_t) = E[\\alpha + \\beta (t+h) + \\varepsilon_{t+h}] = \\alpha + \\beta (t+h).\\]forecast error : \\[e_{t+h|t} = y_{t+h} - y_{t+h|t} = \\varepsilon_{t+h}\\]forecast variance, , : \\[\\sigma_{t+h|t}^2 = E(e_{t+h|t}^2) =  E(\\varepsilon_{t+h}^2) = \\hat{\\sigma}^2,\\;~~\\forall\\;h\\], can obtain interval forecast horizon, : \\[y_{t+h|t} \\pm 1.96 \\hat{\\sigma}.\\]features trend forecasts note:tend understate uncertainty (long horizons forecast interval doesn widen horizon);short-term trend forecasts can perform poorly; long-term trend forecasts typically perform poorly;sometimes may beneficial, standpoint achieving better accuracy, forecast growth rates, reconstruct level forecasts.","code":""},{"path":"deterministic-time-series-models-1.html","id":"deterministic-time-series-models-1","chapter":"Deterministic Time Series Models","heading":"Deterministic Time Series Models","text":"","code":""},{"path":"seasonality.html","id":"seasonality","chapter":"5 Seasonality","heading":"5 Seasonality","text":"Seasonality typically modeled monthly quarterly pattern, can also modeled higher frequency pattern (e.g. weekly). examples time series apparent seasonal patterns :Agricultural production.Sales energy products.Airfare (non-pandemic times).One way deal seasonality data “remove” prior use series (.e., work seasonally adjusted time series). Indeed, economic time series /also available seasonally-adjusted form.Otherwise, perhaps interestingly, can directly model seasonality regression setting incorporating seasonal dummy variables, example.","code":""},{"path":"seasonality.html","id":"modeling-1","chapter":"5 Seasonality","heading":"5.1 Modeling","text":"seasonal model given : \\[y_t = \\sum_{=1}^{s}\\gamma_i d_{} + \\varepsilon_t,\\]\n\\(s\\) denotes frequency data, \\(d_{}\\) takes value 1 repeatedly every \\(s\\) periods, \\(\\sum_{} d_{} = 1\\), \\(\\forall t\\).Alternatively seasonal model can rewritten : \\[y_t = \\alpha + \\sum_{=1}^{s-1}\\delta_i d_{} + \\varepsilon_t,\\] case \\(\\alpha\\) intercept omitted season, \\(\\delta_i\\) represents deviation \\(^{th}\\) season. typical form seasonal model.variants seasonal model result identical fit forecasts.dealing weekly daily data, dummy variable approach modeling seasonality may practical, efficient instances, require estimating another 51 364 parameters. way model seasonality without giving many degrees freedom using -called harmonic seasonal variables, set Fourier terms.Fourier terms can applied model seasonality frequency, indeed. Suppose, example, working monthly time series. model Fourier terms following form: \\[y_t = \\alpha+\\sum_{k=1}^{K}\\left[\\beta_{1k}\\sin\\left(\\frac{2\\pi kt}{12}\\right)+\\beta_{2k}\\cos\\left(\\frac{2\\pi kt}{12}\\right)\\right]+\\varepsilon_t,\\] value \\(K\\) can determined using information criterion (e.g., AIC SIC).","code":""},{"path":"seasonality.html","id":"forecasting-1","chapter":"5 Seasonality","heading":"5.2 Forecasting","text":"predictors seasonal models pre-determined, means, fitting model, can directly obtain point interval forecasts horizon \\(h\\).using dummy variable approach model seasonality, example, future realization random variable : \\[y_{t+h} = \\alpha + \\sum_{=1}^{s-1}\\delta_i d_{,t+h} + \\varepsilon_{t+h}.\\]optimal forecast \\(y_{t+h}\\) : \\[y_{t+h|t} = E(y_{t+h}|\\Omega_t) = \\alpha + \\sum_{=1}^{s-1}\\delta_i d_{,t+h}\\]forecast error : \\[e_{t+h|t} = y_{t+h} - y_{t+h|t} = \\varepsilon_{t+h}\\]forecast variance : \\[\\sigma_{t+h|t}^2 = E(e_{t+h|t}^2) =  E(\\varepsilon_{t+h}^2) = \\hat{\\sigma}^2,\\;~~\\forall\\;h\\]interval forecast : \\[y_{t+h|t} \\pm 1.96 \\hat{\\sigma}.\\]","code":""},{"path":"dynamic-time-series-models.html","id":"dynamic-time-series-models","chapter":"Dynamic Time Series Models","heading":"Dynamic Time Series Models","text":"","code":""},{"path":"linear-autoregression.html","id":"linear-autoregression","chapter":"6 Linear Autoregression","heading":"6 Linear Autoregression","text":"Economic time series often characterized stochastic cycles. cycle pattern periodic fluctuations, contained within calendar year. stochastic cycle one generated random variables. general terms, process given :\n\\[Y_t = f(Y_{t-1},Y_{t-2},\\ldots;\\mathbf{\\theta})+\\varepsilon_t.\\;~~t=1,\\ldots,T\\]autoregressive process (, simply, autoregression) regression dependent variable regressors belong stochastic process.autoregression order \\(p\\), denoted \\(AR(p)\\), following functional form:\n\\[y_t = \\alpha + \\beta_1 y_{t-1}+\\beta_2 y_{t-2}+ \\cdots + \\beta_p y_{t-p}+\\varepsilon_t\\]sum autoregressive parameters, \\(\\beta_1,\\ldots,\\beta_p\\), depicts persistence series. larger persistence (.e., closer one), longer takes effect shock dissolve. effect , eventually, dissolve long series covariance-stationary.autocorrelation, \\(\\rho\\), partial autocorrelation, \\(\\pi\\), functions covariance-stationary \\(AR(p)\\) process following distinctive features:\\(\\rho_1 = \\pi_1\\), \\(\\pi_p = \\beta_p\\).values \\(\\beta_1,\\ldots,\\beta_p\\) determine shape autocorrelation function (ACF); case, smaller (absolute terms) persistence measure, faster ACF decays toward zero.partial autocorrelation function (PACF) characterized “statistically significant” first \\(p\\) spikes \\(\\pi_1 \\neq 0,\\ldots,\\pi_p \\neq 0\\), remaining \\(\\pi_k = 0\\), \\(\\forall k > p\\).","code":""},{"path":"linear-autoregression.html","id":"modeling-2","chapter":"6 Linear Autoregression","heading":"6.1 Modeling","text":"","code":""},{"path":"linear-autoregression.html","id":"ar1","chapter":"6 Linear Autoregression","heading":"6.1.1 AR(1)","text":"first-order autoregression given : \\[y_t = \\alpha + \\beta_1 y_{t-1} + \\varepsilon_t,\\] \\(\\alpha\\) constant term; \\(\\beta_1\\) persistence parameter; \\(\\varepsilon_t\\) white noise process.necessary sufficient condition \\(AR(1)\\) process covariance stationary \\(|\\beta_1| < 1\\). can see substituting recursively lagged equations lagged dependent variables:\n\\[\n\\begin{aligned}\ny_t &= \\alpha + \\beta_1 y_{t-1} + \\varepsilon_t \\notag \\\\\ny_t &= \\alpha + \\beta_1 (\\alpha + \\beta_1 y_{t-2} + \\varepsilon_{t-1}) + \\varepsilon_t \\notag \\\\\n&= \\alpha(1+\\beta_1) + \\beta_1^2 (\\alpha + \\beta_1 y_{t-3} + \\varepsilon_{t-2}) + \\beta_1\\varepsilon_{t-1} + \\varepsilon_t \\notag \\\\\n&\\vdots  \\notag \\\\\n&= \\alpha\\sum_{=0}^{k-1}\\beta_1^+ \\beta_1^k y_{t-k} + \\sum_{=0}^{k-1}\\beta_1^\\varepsilon_{t-}\n\\end{aligned}\n\\]\nend-result general linear process geometrically declining coefficients. , \\(|\\beta_1| < 1\\) required convergence.Assuming \\(|\\beta_1| < 1\\), \\(k \\\\infty\\) process converges : \\[y_t = \\frac{\\alpha}{1-\\beta_1} + \\sum_{=0}^{\\infty}\\beta_1^\\varepsilon_{t-}\\]unconditional mean process : \\[\\mu = E\\left(y_t\\right) = E\\left(\\frac{\\alpha}{1-\\beta_1} + \\sum_{=0}^{\\infty}\\beta_1^\\varepsilon_{t-}\\right) = \\frac{\\alpha}{1-\\beta_1}\\]unconditional variance process : \\[\\gamma_0 = Var\\left(y_t\\right) = Var\\left(\\frac{\\alpha}{1-\\beta_1} + \\sum_{=0}^{\\infty}\\beta_1^\\varepsilon_{t-}\\right) = \\frac{\\sigma_{\\varepsilon}^2}{1-\\beta_1^2}\\]Autocovariance simply covariance \\(y_t\\) \\(y_{t-k}\\), : \\[\\gamma_k = Cov(y_t,y_{t-k}) = E[(y_t - \\mu)(y_{t-k} - \\mu)] = E(y_t y_{t-k}) - \\mu^2\\]algebraic manipulation can help us show : \\[\\gamma_k = \\beta_1\\gamma_{k-1},\\] : \\[\\rho_{k} = \\beta_1\\rho_{k-1}\\] (recall, \\(\\rho_k = \\gamma_k/\\gamma_0\\) autocorrelation coefficient).fact, AR(1), autocorrelation coefficient lag can represented autoregression parameter (instance equivalent persistence measure) power. :\n\\[\n\\begin{aligned}\n\\rho_1 &= \\beta_1\\rho_0 = \\beta_1 \\notag \\\\\n\\rho_2 &= \\beta_1\\rho_1 = \\beta_1^2 \\notag \\\\\n&\\vdots \\notag \\\\\n\\rho_k &= \\beta_1\\rho_{k-1} = \\beta_1^k\n\\end{aligned}\n\\]follows autocorrelation function covariance stationary AR(1) geometric decay; smaller \\(|\\beta_1|\\) rapid decay.imposing certain restrictions, AR(1) reduce already known models:\\(\\beta_1 = 0\\), \\(y_t\\) equivalent white noise.\\(\\beta_1 = 1\\) \\(\\alpha = 0\\), \\(y_t\\) random walk.\\(\\beta_1 = 1\\) \\(\\alpha \\neq 0\\), \\(y_t\\) random walk drift.general, smaller persistence parameter results quicker adjustment unconditional mean process.autocorrelation partial autocorrelation functions AR(1) process three distinctive features:\\(\\rho_1 = \\pi_1 = \\beta_1\\). , persistence parameter also autocorrelation partial autocorrelation coefficient.autocorrelation function decreases exponentially toward zero, decay faster persistence parameter smaller.partial autocorrelation function characterized one spike \\(\\pi_1 \\neq 0\\), remaining \\(\\pi_k = 0\\), \\(\\forall k > 1\\).illustrate foregoing, let’s generate series 100 observations follow process: \\(y_t=0.8y_{t-1}+\\varepsilon_t\\), \\(y_0=0\\) \\(\\varepsilon\\sim N(0,1)\\), plot ACF PACF series.","code":""},{"path":"linear-autoregression.html","id":"ar2","chapter":"6 Linear Autoregression","heading":"6.1.2 AR(2)","text":"Now consider second-order autoregression: \\[y_t = \\alpha + \\beta_1 y_{t-1} + \\beta_2 y_{t-2} + \\varepsilon_t\\]\\(\\alpha\\) constant term; \\(\\beta_1+\\beta_2\\) persistence measure; \\(\\varepsilon_t\\) white noise process.follows, necessary (1 2) sufficient (3 4) conditions \\(AR(2)\\) process covariance stationary :\\(|\\beta_2| < 1\\)\\(|\\beta_1| < 2\\)\\(\\beta_1 + \\beta_2 < 1\\)\\(\\beta_2 - \\beta_1 < 1\\)autocorrelation functions AR(2) process following distinctive features:\\(\\rho_1 = \\pi_1\\) (true \\(AR(p)\\) process), \\(\\pi_2 = \\beta_2\\).autocorrelation function decreases toward zero. path, however, varies depending values \\(\\beta_1\\) \\(\\beta_2\\). Nonetheless, decay faster persistence measure smaller.partial autocorrelation function characterized two spikes \\(\\pi_1 \\neq 0\\) \\(\\pi_2 \\neq 0\\), remaining \\(\\pi_k = 0\\), \\(\\forall k > 2\\)., illustrate, let’s generate series 100 observations follow process: \\(y_t=1.1y_{t-1}-0.4y_{t-2}+\\varepsilon_t\\), \\(y_{-1}=y_0=0\\) \\(\\varepsilon\\sim N(0,1)\\), plot ACF PACF series.","code":""},{"path":"linear-autoregression.html","id":"forecasting-2","chapter":"6 Linear Autoregression","heading":"6.2 Forecasting","text":"Making forecasts future period, \\(t+h\\), AR(p) model fit data including period \\(t+h-1\\) can straightforward exercise, long access relevant information set. one-step-ahead forecasts, information set readily available. multi-step-ahead forecasts, need ‘come ’ value variable realized yet. example, making two-step-ahead forecast period \\(t+2\\), need data period \\(t+1\\), available time forecast made. Instead, need use forecast period \\(t+1\\). applies forecasts subsequent periods future. approach known iterative method forecasting, wherein make forecast period using available data, iterate forward one period use recent forecast make next period’s forecast, forth.","code":""},{"path":"linear-autoregression.html","id":"one-step-ahead-forecast-from-ar1","chapter":"6 Linear Autoregression","heading":"6.2.1 One-step-ahead forecast from AR(1)","text":"realization random variable period \\(t+1\\) : \\[y_{t+1} = \\alpha + \\beta_1 y_{t} + \\varepsilon_{t+1}\\]optimal one-step-ahead forecast : \\[y_{t+1|t} = E(y_{t+1}|\\Omega_t) = E(\\alpha + \\beta_1 y_{t} + \\varepsilon_{t+1}) = \\alpha + \\beta_1 y_{t}\\]one-step-ahead forecast error : \\[e_{t+1|t} = y_{t+1} - y_{t+1|t} = \\alpha + \\beta_1 y_t + \\varepsilon_{t+1} - (\\alpha + \\beta_1 y_t) = \\varepsilon_{t+1}\\]one-step-ahead forecast variance: \\[\\sigma_{t+1|t}^2 = Var(y_{t+1}|\\Omega_t) = E(e_{t+1|t}^2) = E(\\varepsilon_{t+1}^2) = \\sigma_{\\varepsilon}^2\\]one-step-ahead (95%) interval forecast: \\[y_{t+1|t} \\pm z_{.025}\\sigma_{t+1|t} = y_{t+1|t} \\pm 1.96\\sigma_{\\varepsilon}\\]","code":""},{"path":"linear-autoregression.html","id":"two-step-ahead-forecast-from-ar1","chapter":"6 Linear Autoregression","heading":"6.2.2 Two-step-ahead forecast from AR(1)","text":"realization random variable period \\(t+2\\) : \\[y_{t+2} = \\alpha + \\beta_1 y_{t+1} + \\varepsilon_{t+2}\\]optimal two-step-ahead forecast : \\[y_{t+2|t} = E(y_{t+2}|\\Omega_t) = E(\\alpha + \\beta_1 y_{t+1} + \\varepsilon_{t+2}) = \\alpha(1+\\beta_1) + \\beta_1^2 y_t\\]Note, substituted \\(y_{t+1}\\) \\(\\alpha + \\beta_1 y_t + \\varepsilon_{t+1}\\).two-step-ahead forecast error :\n\\[\\begin{aligned}\ne_{t+2|t} &= y_{t+2} - y_{t+2|t} \\\\\n&= \\alpha(1+\\beta_1) + \\beta_1^2 y_t + \\beta_1\\varepsilon_{t+1} + \\varepsilon_{t+2} - [\\alpha(1+\\beta_1) + \\beta_1^2 y_t] \\\\\n&= \\beta_1\\varepsilon_{t+1} + \\varepsilon_{t+2}\n\\end{aligned}\\]two-step-ahead forecast variance :\n\\[\\begin{aligned}\n\\sigma_{t+2|t}^2 &= Var(y_{t+2}|\\Omega_t) \\\\\n&= E(e_{t+2|t}^2) = E(\\beta_1\\varepsilon_{t+1} + \\varepsilon_{t+2})^2 = \\sigma_{\\varepsilon}^2(1+\\beta_1^2)\n\\end{aligned}\\]two-step-ahead (95%) interval forecast: \\[y_{t+2|t} \\pm z_{.025}\\sigma_{t+2|t} = y_{t+2|t} \\pm 1.96\\sigma_{\\varepsilon}\\sqrt{1+\\beta_1^2}\\]","code":""},{"path":"linear-autoregression.html","id":"h-step-ahead-forecast-from-ar1","chapter":"6 Linear Autoregression","heading":"6.2.3 h-step-ahead forecast from AR(1)","text":"realization random variable period \\(t+h\\) : \\[y_{t+h} = \\alpha + \\beta_1 y_{t+h-1} + \\varepsilon_{t+h}\\]optimal h-step-ahead forecast: \\[y_{t+h|t} = E(y_{t+h}|\\Omega_t) = E(\\alpha + \\beta_1 y_{t+h-1} + \\varepsilon_{t+1}) = \\alpha\\textstyle\\sum_{j=0}^{h-1}\\beta_1^j + \\beta_1^h y_t\\]h-step-ahead forecast error: \\[e_{t+h|t} = y_{t+h} - y_{t+h|t} = \\textstyle\\sum_{j=0}^{h-1}\\beta_1^j\\varepsilon_{t+h-j}\\]h-step-ahead forecast variance: \\[\\sigma_{t+h|t}^2 = Var(y_{t+h}|\\Omega_t) = E(e_{t+h|t}^2) = \\sigma_{\\varepsilon}^2\\textstyle\\sum_{j=0}^{h-1}\\beta_1^{2j}\\]h-step-ahead (95%) interval forecast: \\[y_{t+h|t} \\pm z_{.025}\\sigma_{t+h|t} = y_{t+1|t} \\pm 1.96\\sigma_{\\varepsilon}\\sqrt{\\textstyle\\sum_{j=0}^{h-1}\\beta_1^{2j}}\\]series represent covariance-stationary process, .e. \\(|\\beta_1| < 1\\), \\(h \\\\infty\\):optimal point forecast converges : \\[y_{t+h|t} = \\frac{\\alpha}{1-\\beta_1}\\]forecast variance converges : \\[\\sigma_{t+h|t}^2 = \\frac{\\sigma_{\\varepsilon}^2}{1-\\beta_1^2}\\](95%) interval forecast converges : \\[y_{t+h|t} \\pm z_{.025}\\sigma_{t+h|t} = \\frac{\\alpha}{1-\\beta_1} \\pm 1.96\\frac{\\sigma_{\\varepsilon}}{\\sqrt{1-\\beta_1^2}}\\]","code":""},{"path":"linear-autoregression.html","id":"one-step-ahead-forecast-from-ar2","chapter":"6 Linear Autoregression","heading":"6.2.4 One-step-ahead forecast from AR(2)","text":"realization random variable period \\(t+1\\) : \\[y_{t+1} = \\alpha + \\beta_1 y_{t} + \\beta_2 y_{t-1} + \\varepsilon_{t+1}\\]optimal one-step-ahead forecast:\n\\[\\begin{aligned}\ny_{t+1|t} &= E(y_{t+1}|\\Omega_t) \\\\\n&= E(\\alpha + \\beta_1 y_{t} + \\beta_2 y_{t-1} + \\varepsilon_{t+1}) = \\alpha + \\beta_1 y_{t} + \\beta_2 y_{t-1}\n\\end{aligned}\\]one-step-ahead forecast error:\n\\[\\begin{aligned}\ne_{t+1|t} &= y_{t+1} - y_{t+1|t} \\\\\n&= \\alpha + \\beta_1 y_t + \\beta_2 y_{t-1} + \\varepsilon_{t+1} - (\\alpha + \\beta_1 y_t + \\beta_2 y_{t-1}) = \\varepsilon_{t+1}\n\\end{aligned}\\]one-step-ahead forecast variance: \\[\\sigma_{t+1|t}^2 = Var(y_{t+1}|\\Omega_t) = E(e_{t+1|t}^2) = E(\\varepsilon_{t+1}^2) = \\sigma_{\\varepsilon}^2\\]one-step-ahead (95%) interval forecast: \\[y_{t+1|t} \\pm z_{.025}\\sigma_{t+1|t} = y_{t+1|t} \\pm 1.96\\sigma_{\\varepsilon}\\]","code":""},{"path":"linear-autoregression.html","id":"two-step-ahead-forecast-from-ar2","chapter":"6 Linear Autoregression","heading":"6.2.5 Two-step-ahead forecast from AR(2)","text":"realization random variable period \\(t+2\\) : \\[y_{t+2} = \\alpha + \\beta_1 y_{t+1} + \\beta_2 y_{t} + \\varepsilon_{t+2}\\]optimal two-step-ahead forecast:\n\\[\\begin{aligned}\ny_{t+2|t} = E(y_{t+2}|\\Omega_t) &= E(\\alpha + \\beta_1 y_{t+1} + \\beta_2 y_{t} + \\varepsilon_{t+1}) \\\\\n&= \\alpha(1+\\beta_1) + (\\beta_1^2+\\beta_2) y_{t} + \\beta_1\\beta_2 y_{t-1}\n\\end{aligned}\\]two-step-ahead forecast error:\n\\[\\begin{aligned}\ne_{t+2|t} = y_{t+2} - y_{t+2|t} =& \\alpha + \\beta_1 y_{t+1} + \\beta_2 y_{t} + \\varepsilon_{t+2} \\\\\n&- (\\alpha + \\beta_1 y_{t+1|t} + \\beta_2 y_{t}) = \\beta_1\\varepsilon_{t+1} + \\varepsilon_{t+2}\n\\end{aligned}\\]two-step-ahead forecast variance:\n\\[\\sigma_{t+2|t}^2 = Var(y_{t+2}|\\Omega_t) = E(e_{t+2|t}^2) = E(\\beta_1\\varepsilon_{t+1} + \\varepsilon_{t+2})^2 = \\sigma_{\\varepsilon}^2(1+\\beta_1^2)\\]two-step-ahead (95%) interval forecast: \\[y_{t+2|t} \\pm z_{.025}\\sigma_{t+2|t} = y_{t+2|t} \\pm 1.96\\sigma_{\\varepsilon}\\sqrt{1+\\beta_1^2}\\]","code":""},{"path":"linear-autoregression.html","id":"one-step-ahead-forecast-from-ar2-1","chapter":"6 Linear Autoregression","heading":"6.2.6 One-step-ahead forecast from AR(2)","text":"realization random variable period \\(t+h\\) : \\[y_{t+h} = \\alpha + \\beta_1 y_{t+h-1} + \\beta_2 y_{t+h-2} + \\varepsilon_{t+h}\\]optimal h-step-ahead forecast (iterated method):\n\\[\\begin{aligned}\ny_{t+1|t} &= \\alpha + \\beta_1 y_t + \\beta_2 y_{t-1} \\\\\ny_{t+2|t} &= \\alpha + \\beta_1 y_{t+1|t} + \\beta_2 y_{t} \\\\\ny_{t+3|t} &= \\alpha + \\beta_1 y_{t+2|t} + \\beta_2 y_{t+1|t} \\\\\n&\\vdots \\\\\ny_{t+h|t} &= \\alpha + \\beta_1 y_{t+h-1|t} + \\beta_2 y_{t+h-2|t}\n\\end{aligned}\\]h-step-ahead forecast error: \\[e_{t+h|t} = y_{t+h} - y_{t+h|t} = \\varepsilon_{t+h}+\\beta_1 e_{t+h-1|t}+\\beta_2 e_{t+h-2|t}\\]h-step-ahead forecast variance:\n\\[\\begin{aligned}\n\\sigma_{t+h|t}^2 &= Var(y_{t+h}|\\Omega_t) = E(e_{t+h|t}^2) \\\\\n&= \\sigma_{\\varepsilon}^2+\\beta_1^2 Var(e_{t+h-1|t})+\\beta_2^2 Var(e_{t+h-2|t}) \\\\\n&+2\\beta_1\\beta_2Cov(e_{t+h-1|t},e_{t+h-2|t})\n\\end{aligned}\\](Note: formulas \\(\\sigma_{t+1|t}^2,\\sigma_{t+2|t}^2,\\ldots,\\sigma_{t+h|t}^2\\) \\(AR(p)\\), \\(p \\geq h-1\\)).h-step-ahead (95%) interval forecast: \\[y_{t+h|t} \\pm z_{.025}\\sigma_{t+h|t} = y_{t+1|t} \\pm 1.96\\sigma_{t+h|t}\\]optimal h-step-ahead forecast: \\[y_{t+h|t} = E(y_{t+h}|\\Omega_t) = \\alpha + \\beta_1 y_{t+h-1|t} + \\beta_2 y_{t+h-2|t} + \\cdots + \\beta_p y_{t+h-p|t}\\]h-step-ahead forecast error: \\[e_{t+h|t} = \\varepsilon_{t+h} + \\beta_1 e_{t+h-1|t} + \\beta_2 e_{t+h-2|t} + \\cdots + \\beta_p e_{t+h-p|t}\\]h-step-ahead forecast variance:\n\\[\\begin{aligned}\n\\sigma_{t+h|t}^2 & = Var(y_{t+h}|\\Omega_t) = E(e_{t+2|t}^2) \\\\\n&= \\sigma_{\\varepsilon}^2 + \\sum_{=1}^{p}\\beta_i^2 Var(e_{t+h-|t}) + 2\\sum_{\\neq j}\\beta_i\\beta_j Cov(e_{t+h-|t},e_{t+h-j|t})\n\\end{aligned}\\]h-step-ahead (95%) interval forecast: \\[y_{t+h|t} \\pm z_{.025}\\sigma_{t+h|t} = y_{t+h|t} \\pm 1.96\\sigma_{t+h|t}\\]","code":""},{"path":"vector-autoregression.html","id":"vector-autoregression","chapter":"7 Vector Autoregression","heading":"7 Vector Autoregression","text":"Many economic variables interrelated. example, changes household income impact consumption levels; changes interest rates impact investments economy. Often (albeit always) relationship variables goes directions. example, higher wages (, therefore, income) result higher prices (inflation), turn puts upward pressure wages.foregoing implies shock variable may propagate dynamic response variable, also related variables. dynamic linkages two () economic variables can modeled system equations, represented vector autoregressive (VAR) process.","code":""},{"path":"vector-autoregression.html","id":"modeling-3","chapter":"7 Vector Autoregression","heading":"7.1 Modeling","text":"begin, consider bivariate (two-dimensional) VAR order one, VAR(1).Let \\(\\{X_{1,t}\\}\\) \\(\\{X_{2,t}\\}\\) stationary stochastic processes. bivariate VAR(1), given :\n\\[\\begin{aligned}\nx_{1,t} &= \\alpha_1 + \\pi_{11}x_{1,t-1} + \\pi_{12}x_{2,t-1} + \\varepsilon_{1,t} \\\\\nx_{2,t} &= \\alpha_2 + \\pi_{21}x_{1,t-1} + \\pi_{22}x_{2,t-1} + \\varepsilon_{2,t}\n\\end{aligned}\\]\\(\\varepsilon_{1,t} \\sim iid(0,\\sigma_1^2)\\) \\(\\varepsilon_{2,t} \\sim iid(0,\\sigma_2^2)\\), two can correlated, .e., \\(Cov(\\varepsilon_{1,t},\\varepsilon_{2,t}) \\neq 0\\).generalize, consider multivariate (\\(n\\)-dimensional) VAR order \\(p\\), VAR(p), presented matrix notation: \\[\\mathbf{x}_t = \\mathbf{\\alpha} + \\Pi_1 \\mathbf{x}_{t-1} + \\ldots + \\Pi_p \\mathbf{x}_{t-p} + \\mathbf{\\varepsilon}_t,\\] \\(\\mathbf{x}_t = (x_{1,t},\\ldots,x_{n,t})'\\) vector \\(n\\) (potentially) related variables; \\(\\mathbf{\\varepsilon}_t = (\\varepsilon_{1,t},\\ldots,\\varepsilon_{n,t})'\\) vector error terms, \\(E(\\mathbf{\\varepsilon}_t) = \\mathbf{0}\\), \\(E(\\mathbf{\\varepsilon}_t^{}\\mathbf{\\varepsilon}_t^{\\prime}) = \\Sigma\\), \\(E(\\mathbf{\\varepsilon}_{t}^{}\\mathbf{\\varepsilon}_{s \\neq t}^{\\prime}) = 0\\). \\(\\Pi_1,\\ldots,\\Pi_p\\) \\(n\\)-dimensional parameter matrices:\n\\[\\Pi_j = \n        \\left[ \n        \\begin{array}{cccc} \n        \\pi_{11j} & \\pi_{12j} & \\cdots &  \\pi_{1nj} \\\\ \n        \\pi_{21j} & \\pi_{22j} & \\cdots &  \\pi_{2nj} \\\\  \n        \\vdots & \\vdots & \\ddots &  \\vdots \\\\  \n        \\pi_{n1j} & \\pi_{n2j} & \\cdots &  \\pi_{nnj} \\\\  \n        \\end{array} \n        \\right],\\;~~j=1,\\ldots,p\\]two variables modeled way, implies assumption variables endogenous ; , variables affects affected variables.three forms vector autoregressions: structural, recursive, reduced-form. structural VAR uses economic theory impose ‘structure’ correlations error terms system, thus, facilitate ‘causal’ interpretation. recursive VAR also introduces structure sort, primarily involves ordering equations system specific way error terms equation uncorrelated preceding equations. extent ‘identifying assumptions’ satisfied, contemporaneous values (variables) appear equation given variable. reduced-form VAR makes claims causality, instead includes lagged values variables equation system. extent variables entering system , indeed, correlated among , error terms reduced-form VAR (typically) contemporaneously correlated. , unless otherwise specified, VAR refers reduced-form VAR.features VAR :lagged values dependent variables considered right-hand-side variables (although, trend seasonal variables might also included higher-frequency data analysis);equation set right-hand-side variables (however, possible impose different lag structure across equations, especially \\(p\\) relatively large, preserve degrees freedom, particularly sample size relatively small several variables system).autregressive order VAR, \\(p\\), determined maximum lag variable across equations.order VAR, \\(p\\), can determined using system-wide information criteria:\\[\\begin{aligned}\n& AIC = \\ln\\left|\\Sigma_{\\varepsilon}\\right| + \\frac{2}{T}(pn^2+n) \\\\\n& SIC = \\ln\\left|\\Sigma_{\\varepsilon}\\right| + \\frac{\\ln{T}}{T}(pn^2+n)\n\\end{aligned}\\]\\(\\left|\\Sigma_{\\varepsilon}\\right|\\) determinant residual covariance matrix; \\(n\\) number equations, \\(T\\) effective sample size.can estimate equation VAR individually using OLS.","code":""},{"path":"vector-autoregression.html","id":"in-sample-granger-causality","chapter":"7 Vector Autoregression","heading":"7.1.1 In-Sample Granger Causality","text":"test joint significance parameters associated lags variable entering equation another variable known ‘Granger causality’ test. use term ‘causality’ context criticized. one variable can help explain movement another variable necessarily mean former causes latter. end use term misleading, indeed. Rather, simply means former helps predict latter, , effect, meaning causality Granger sense.illustrate testing framework, consider bivariate VAR(p):\n\\[\\begin{aligned}\nx_{1,t} &= \\alpha_1 + \\pi_{111} x_{1,t-1} + \\cdots + \\pi_{11p} x_{1,t-p} \\\\\n&+ \\pi_{121} x_{2,t-1} + \\cdots + \\pi_{12p} x_{2,t-p} +\\varepsilon_{1,t}  \\\\\nx_{2,t} &= \\alpha_1 + \\pi_{211} x_{1,t-1} + \\cdots + \\pi_{21p} x_{1,t-p} \\\\\n&+ \\pi_{221} x_{2,t-1} + \\cdots + \\pi_{22p} x_{2,t-p} +\\varepsilon_{2,t} \n\\end{aligned}\\]said :\n- \\(\\{X_2\\}\\) Granger cause \\(\\{X_1\\}\\) \\(\\pi_{121}=\\cdots=\\pi_{12p}=0\\)\n- \\(\\{X_1\\}\\) Granger cause \\(\\{X_2\\}\\) \\(\\pi_{211}=\\cdots=\\pi_{21p}=0\\)long variables system covariance-stationarity, can apply F test hypotheses testing. \\(p=1\\), t test equivalently applicable hypotheses testing.","code":""},{"path":"vector-autoregression.html","id":"forecasting-3","chapter":"7 Vector Autoregression","heading":"7.2 Forecasting","text":"Generating forecasts variable comprising VAR(p) model can straightforward exercise, long access relevant information set. case autoregressive models, make one-step-ahead forecasts based readily available data; make multi-step-ahead forecasts iteratively, using forecast periods data present.","code":""},{"path":"vector-autoregression.html","id":"one-step-ahead-forecast-from-bivariate-var1","chapter":"7 Vector Autoregression","heading":"7.2.1 One-step-ahead forecast from bivariate VAR(1)","text":"realizations variables period \\(t+1\\) :\n\\[\\begin{aligned}\nx_{1,t+1} &= \\alpha_1 + \\pi_{11} x_{1,t} + \\pi_{12} x_{2,t} + \\varepsilon_{1,t+1} \\\\\nx_{2,t+1} &= \\alpha_2 + \\pi_{21} x_{1,t} + \\pi_{22} x_{2,t} + \\varepsilon_{2,t+1}\n\\end{aligned}\\]optimal one-step-ahead forecasts :\n\\[\\begin{aligned}\nx_{1,t+1|t} &= E(x_{1,t+1}|\\Omega_t) = \\alpha_1 + \\pi_{11} x_{1,t} + \\pi_{12} x_{2,t} \\\\\nx_{2,t+1|t} &= E(x_{2,t+1}|\\Omega_t) = \\alpha_2 + \\pi_{21} x_{1,t} + \\pi_{22} x_{2,t}\n\\end{aligned}\\]one-step-ahead forecast errors :\n\\[\\begin{aligned}\ne_{1,t+1|t} &= x_{1,t+1} - x_{1,t+1|t} = \\varepsilon_{1,t+1} \\\\\ne_{2,t+1|t} &= x_{2,t+1} - x_{2,t+1|t} = \\varepsilon_{2,t+1}\n\\end{aligned}\\]one-step-ahead forecast variances :\n\\[\\begin{aligned}\n\\sigma_{1,t+1|t}^2 &= E(x_{1,t+1} - x_{1,t+1|t}|\\Omega_t)^2 = E(\\varepsilon_{1,t+1}^2) = \\sigma_{1}^2 \\\\\n\\sigma_{2,t+1|t}^2 &= E(x_{2,t+1} - x_{2,t+1|t}|\\Omega_t)^2 = E(\\varepsilon_{2,t+1}^2) = \\sigma_{2}^2\n\\end{aligned}\\]one-step-ahead (95%) interval forecasts :\n\\[\\begin{aligned}\nx_{1,t+1|t} \\pm z_{.025}\\sigma_{1,t+1|t} = x_{1,t+1|t} \\pm 1.96\\sigma_{\\varepsilon_1} \\\\\nx_{2,t+1|t} \\pm z_{.025}\\sigma_{2,t+1|t} = x_{2,t+1|t} \\pm 1.96\\sigma_{\\varepsilon_2}\n\\end{aligned}\\]","code":""},{"path":"vector-autoregression.html","id":"two-step-ahead-forecast-from-bivariate-var1","chapter":"7 Vector Autoregression","heading":"7.2.2 Two-step-ahead forecast from bivariate VAR(1)","text":"realizations variables period \\(t+2\\) :\n\\[\\begin{aligned}\nx_{1,t+2} &= \\alpha_1 + \\pi_{11} x_{1,t+1} + \\pi_{12} x_{2,t+1} + \\varepsilon_{1,t+2} \\\\\nx_{2,t+2} &= \\alpha_2 + \\pi_{21} x_{1,t+1} + \\pi_{22} x_{2,t+1} + \\varepsilon_{2,t+2}\n\\end{aligned}\\]optimal two-step-ahead forecasts :\n\\[\\begin{aligned}\nx_{1,t+2|t} &= E(x_{1,t+2}|\\Omega_t) = \\alpha_1 + \\pi_{11} x_{1,t+1|t} + \\pi_{12} x_{2,t+1|t} \\\\\nx_{2,t+2|t} &= E(x_{2,t+2}|\\Omega_t) = \\alpha_2 + \\pi_{21} x_{1,t+1|t} + \\pi_{22} x_{2,t+1|t}\n\\end{aligned}\\]two-step-ahead forecast errors :\n\\[\\begin{aligned}\ne_{1,t+2|t} &= x_{1,t+2} - x_{1,t+2|t} = \\pi_{11} e_{1,t+1|t} + \\pi_{12} e_{2,t+1|t} + \\varepsilon_{1,t+2} \\\\\ne_{2,t+2|t} &= x_{2,t+2} - x_{2,t+2|t} = \\pi_{21} e_{1,t+1|t} + \\pi_{22} e_{2,t+1|t} + \\varepsilon_{2,t+2}\n\\end{aligned}\\]two-step-ahead forecast variances :\n\\[\\begin{aligned}\n\\sigma_{1,t+2|t}^2 &= E(x_{1,t+2} - x_{1,t+2|t}|\\Omega_t)^2 \\\\ \n&= \\sigma_{1}^2(1+\\pi_{11}^2) + \\sigma_{2}^2\\pi_{12}^2 + 2\\pi_{11}\\pi_{12} Cov(\\varepsilon_{1},\\varepsilon_{2})\\\\\n\\sigma_{2,t+2|t}^2 &= E(x_{2,t+2} - x_{2,t+2|t}|\\Omega_t)^2 \\\\ \n&= \\sigma_{2}^2(1+\\pi_{22}^2) + \\sigma_{1}^2\\pi_{21}^2 + 2\\pi_{21}\\pi_{22} Cov(\\varepsilon_{1},\\varepsilon_{2})\n\\end{aligned}\\]two-step-ahead (95%) interval forecasts :\n\\[\\begin{aligned}\nx_{1,t+2|t} \\pm z_{.025}\\sigma_{1,t+2|t} \\\\\nx_{2,t+2|t} \\pm z_{.025}\\sigma_{2,t+2|t}\n\\end{aligned}\\]","code":""},{"path":"vector-autoregression.html","id":"out-of-sample-granger-causality","chapter":"7 Vector Autoregression","heading":"7.2.3 Out-of-Sample Granger Causality","text":"previously discussed (sample) test causality Granger sense frequently performed practice. noted , term ‘causality’ may misleading somewhat. Indeed, ‘true spirit’ test assess ability variable help predict another variable --sample setting.Consider restricted unrestricted information sets, former contains information variable \\(X_1\\), latter contains information well information variable \\(X_2\\):\n\\[\\begin{aligned}\n&\\Omega_{t,r} \\equiv \\Omega_{t}(X_1) = \\{x_{1,t},x_{1,t-1},\\ldots\\} \\\\\n&\\Omega_{t,u} \\equiv \\Omega_{t}(X_1,X_2) = \\{x_{1,t},x_{1,t-1},\\ldots,x_{2,t},x_{2,t-1},\\ldots\\}\n\\end{aligned}\\]Following Granger’s definition causality: \\(\\{X_2\\}\\) said cause \\(\\{X_1\\}\\) \\(\\sigma_{x_1}^2\\left(\\Omega_{t,u}\\right) < \\sigma_{x_1}^2\\left(\\Omega_{t,r}\\right)\\), meaning can better predict \\(X_1\\) using available information \\(X_1\\) \\(X_2\\), rather \\(X_1\\) .Let forecasts based information sets :\n\\[\\begin{aligned}\n    &x_{1,t+h|t,r} = E\\left(x_{1,t+h}|\\Omega_{t,r}\\right) \\\\\n    &x_{1,t+h|t,u} = E\\left(x_{1,t+h}|\\Omega_{t,u}\\right)\n\\end{aligned}\\]forecasts, corresponding forecast errors :\n\\[\\begin{aligned}\n    & e_{1,t+h|t,r} = x_{1,t+h} - x_{1,t+h|t,r}\\\\\n    & e_{1,t+h|t,u} = x_{1,t+h} - x_{1,t+h|t,u}\n\\end{aligned}\\]--sample forecast errors evaluated comparing loss functions based forecasts errors. example, assuming quadratic loss, \\(P\\) --sample forecasts:\n\\[\\begin{aligned}\nRMSFE_{r} &= \\sqrt{\\frac{1}{P}\\sum_{s=1}^{P}\\left(e_{1,R+s|R-1+s,r}\\right)^2} \\\\\nRMSFE_{u} &= \\sqrt{\\frac{1}{P}\\sum_{s=1}^{P}\\left(e_{1,R+s|R-1+s,u}\\right)^2}\n\\end{aligned}\\]\n\\(R\\) size (first) estimation window.\\(\\{X_2\\}\\) said cause Granger sense \\(\\{X_1\\}\\) \\(RMSFE_{u} < RMSFE_{r}\\).","code":""},{"path":"threshold-autoregression.html","id":"threshold-autoregression","chapter":"8 Threshold Autoregression","heading":"8 Threshold Autoregression","text":"stochastic process can approximated linear model, possible nonlinear model offers better fit data. Nonlinear models come many flavours. consider one type nonlinear models, belongs family regime-dependent models.regime-dependent model can seen combination linear specifications linked (nonlinear) way. end, nonlinear models also referred piecewise linear models - piece linear, taken together nonlinear model hand.","code":""},{"path":"threshold-autoregression.html","id":"nonlinear-models","chapter":"8 Threshold Autoregression","heading":"8.1 Nonlinear Models","text":"follows, consider two representative regime-dependent models: time-varying threshold autoregression self-exciting threshold autoregression. instances assume switch regimes happens based threshold variable, instantaneously.Consider AR(p) process deterministic trend: \\[y_t = \\alpha_0 + \\alpha_1 t + \\sum_{=1}^{p}\\beta_i y_{t-} + \\varepsilon_t,\\] \\(\\alpha_0 + \\alpha_1 t\\) time-specific deterministic component.specification implies linear trend, doesn’t need case. can quadratic cubic trends, example, can trend component .simple augmentation foregoing model autoregressive model switching trend component: \\[y_t = \\delta_{0} + \\delta_{1} t + \\delta_{2}(t-\\tau)(t>\\tau) + \\beta y_{t-1} + \\varepsilon_t,\\] \\(\\tau\\) threshold parameter.switch can extended whole autoregressive process. example, two-regime AR(p) drift can given : \\[y_t = \\delta_0 + \\delta_1 t + \\sum_{=1}^{p}\\beta_{1i} y_{t-} + \\left[\\delta_2(t-\\tau) + \\sum_{=1}^{p}\\beta_{2i} y_{t-}\\right](t>\\tau) + \\varepsilon_t.\\] equation implies trend, also autoregressive process changes around threshold parameter \\(\\tau\\).foregoing nonlinear specifications assumed switch model occurs point time, .e. regime-switching variable function time. regime-switching variable can also function dependent variable, well (potentially) related variables: \\[y_t = \\alpha_0 + \\sum_{=1}^{p}\\beta_{0i} y_{t-} + \\left(\\alpha_1 + \\sum_{=1}^{p}\\beta_{1i} y_{t-}\\right)(s_t>c) + \\varepsilon_t,\\] \\(s_t\\) regime-switching variable, \\(c\\) threshold, \\(\\underline{s}_t < c < \\overline{s}_t\\), \\(\\underline{s}_t\\) \\(\\overline{s}_t\\) lower upper quantiles regime-switching variable.equation referred threshold autoregression, TAR(p). specifically, TAR(p), \\(s_t = y_{t-d}\\), \\(d = 1,\\ldots,m\\), self-exciting threshold autoregression, SETAR(p); alternatively, \\(s_t = \\Delta y_{t-d}\\), \\(d = 1,\\ldots,m\\), model referred momentum threshold autoregression, momentum-TAR(p). latter typically preferred \\(y_t\\) (1) process.TAR (version ) can take multiple-regime form: \\[y_t = \\alpha_1 + \\sum_{=1}^{p}\\beta_{1i} y_{t-} + \\sum_{j=2}^{K}{\\left(\\alpha_j + \\sum_{=1}^{p}\\beta_{ji} y_{t-}\\right)(s_t>c_j)} + \\varepsilon_t,\\] \\(K\\) depicts number regimes equation.","code":""},{"path":"threshold-autoregression.html","id":"modeling-4","chapter":"8 Threshold Autoregression","heading":"8.2 Modeling","text":"estimating TAR-type models, priori knowledge number regimes, autoregressive order regime, regime-switching (threshold) variable, threshold values.threshold values unknown (need estimated), standard statistical inference longer applicable. Otherwise, given process stationary, standard statistical inference applies.Joint estimation model parameters require nonlinear optimization routine. Alternatively, can approximate optimization using grid-search procedure. procedure relies fact threshold parameter known, model reduces linear model, least squares estimator can applied . , \\[\\hat{\\tau} = \\arg\\min_{\\tau}\\hat{\\sigma}^2(\\tau),\\] \\[\\hat{\\sigma}^2(\\tau) = \\frac{1}{T-k}\\sum_{t=1}^{T}\\hat{\\epsilon}_t^2(\\tau)\\] candidate values \\(\\tau\\). candidate values \\(\\tau\\) typically belong range 10th 90th percentiles transition variable, simply trend case time-varying TAR, lagged dependent variable case self-exciting TAR.","code":""},{"path":"threshold-autoregression.html","id":"forecasting-4","chapter":"8 Threshold Autoregression","heading":"8.3 Forecasting","text":"case time-varying shifting trend (mean) models, recent trend component used obtain forecasts. end, forecasting routine similar linear trend models.case regime-switching models (e.g., TAR), obtaining one-step-ahead forecasts can rather straightforward exercise:\n\\[\\begin{aligned}\ny_{t+1|t} &= \\alpha_0+\\beta_{01}y_{t}+\\beta_{02}y_{t-1}+\\ldots \\\\\n          &+ (\\alpha_1+\\beta_{11}y_{t}+\\beta_{12}y_{t-1}+\\ldots)(s_t>c)\n\\end{aligned}\\]Obtaining h-step-ahead forecasts (\\(h\\geq2\\)) less trivial, however. available options:iterated method (, -called skeleton extrapolation) easy inefficient option.analytical method can unbearably tedious.numerical method applicable , moreover, resolves caveats previous two options.","code":""},{"path":"threshold-autoregression.html","id":"skeleton-extrapolation","chapter":"8 Threshold Autoregression","heading":"8.3.1 Skeleton Extrapolation","text":"One-step-ahead forecast: \\[y_{t+1|t} = E(y_{t+1}|\\Omega_{t}) = g(y_{t},y_{t-1},\\ldots,y_{t+1-p};\\theta)\\]Two-step-ahead forecast: \\[y_{t+2|t} = E(y_{t+2}|\\Omega_{t}) = g(y_{t+1|t},y_{t},\\ldots,y_{t+2-p};\\theta)\\]h-step-ahead forecast: \\[y_{t+h|t} = E(y_{t+h}|\\Omega_{t}) = g(y_{t+h-1|t},y_{t+h-2|t},\\ldots,y_{t+h-p|t};\\theta)\\]fine linear models; okay nonlinear models.","code":""},{"path":"threshold-autoregression.html","id":"analytical-method","chapter":"8 Threshold Autoregression","heading":"8.3.2 Analytical Method","text":"One-step-ahead forecast (uncertainty observed data).Two-step-ahead forecast : \\[\\tilde{y}_{t+2|t} = \\int_{-\\infty}^{\\infty}g(y_{t+1|t}+\\varepsilon_{t+1},y_{t},\\ldots,y_{t+2-p};\\theta)f(\\varepsilon_{t+1})d\\varepsilon_{t+1}\\]Unless model linear, \\(\\tilde{y}_{t+2|t} \\ne y_{t+2|t}\\).Longer horizon forecasts require multiple integrals.","code":""},{"path":"threshold-autoregression.html","id":"numerical-method-bootstrap-resampling","chapter":"8 Threshold Autoregression","heading":"8.3.3 Numerical Method: Bootstrap Resampling","text":"Bootstrap resampling helps approximate optimal forecast nonlinear models circumvents complexity integration.additional benefit, procedure generates forecast distribution, empirical confidence intervals (along point forecast) can obtained.Algorithm:Estimate time series model store residuals.set residuals, sample (replacement) vector shocks bootstrap iteration, \\(\\varepsilon^b = (\\varepsilon_{t+1}^b,\\varepsilon_{t+2}^b,\\ldots,\\varepsilon_{t+h}^b)'\\).Use sample shocks, along estimated parameters historical observations, generate forecast path given bootstrap iteration.Repeat steps 2-3 many times generate empirical distribution bootstrap forecasts.One-step-ahead bootstrap iteration: \\[y_{t+1|t,\\varepsilon_{t+1}^b} \\equiv y_{t+1|t}^b = g(y_{t},y_{t-1},\\ldots,y_{t+1-p};\\theta)+\\varepsilon_{t+1}^b\\]Two-step-ahead bootstrap iteration: \\[y_{t+2|t,\\varepsilon_{t+1}^b,\\varepsilon_{t+2}^b} \\equiv y_{t+2|t}^b = g(y_{t+1|t,\\varepsilon_{t+1}^b},y_{t},\\ldots,y_{t+2-p};\\theta)+\\varepsilon_{t+2}^b\\]example, consider linear \\(\\text{AR}(p)\\) model.One-step-ahead bootstrap iteration: \\[y_{t+1|t}^b = \\alpha + \\beta_1 y_{t} + \\ldots + \\beta_p y_{t+1-p}+\\varepsilon_{t+1}^b\\]Two-step-ahead bootstrap iteration: \\[y_{t+2|t}^b = \\alpha + \\beta_1 \\hat{y}_{t+1}^b + \\ldots + \\beta_p y_{t+2-p}+\\varepsilon_{t+2}^b\\]Now consider nonlinear \\(\\text{SETAR}(p,y_{t-1})\\) model:One-step-ahead bootstrap iteration:\n\\[\\begin{aligned}\ny_{t+1|t}^b &= (\\alpha_1 + \\beta_{11} y_{t} + \\ldots + \\beta_{1p} y_{t+1-p})(y_{t} \\leq c) \\\\ \n                &+ (\\alpha_2 + \\beta_{21} y_{t} + \\ldots + \\beta_{2p} y_{t+1-p})(y_{t} > c)+\\varepsilon_{t+1}^b\n\\end{aligned}\\]Two-step-ahead bootstrap iteration:\n\\[\\begin{aligned}\ny_{t+2|t}^b &= (\\alpha_1 + \\beta_{11} y_{t+1|t}^b + \\ldots + \\beta_{1p} y_{t+2-p})(y_{t+1|t}^b \\leq c) \\\\\n                &+ (\\alpha_2 + \\beta_{21} y_{t+1|t}^b + \\ldots + \\beta_{2p} y_{t+2-p})(y_{t+1|t}^b > c)+\\varepsilon_{t+2}^b\n\\end{aligned}\\]One-step-ahead bootstrap forecast: \\[\\bar{y}_{t+1|t} = B^{-1}\\sum_{b=1}^{B}y_{t+1|t}^b\\]Two-step-ahead bootstrap forecast: \\[\\bar{y}_{t+2|t} = B^{-1}\\sum_{b=1}^{B}y_{t+2|t}^b\\], \\(B\\) total number bootstrap iterations (usually many thousand iterations).","code":""},{"path":"forecast-assessment.html","id":"forecast-assessment","chapter":"Forecast Assessment","heading":"Forecast Assessment","text":"","code":""},{"path":"forecast-evaluation.html","id":"forecast-evaluation","chapter":"9 Forecast Evaluation","heading":"9 Forecast Evaluation","text":"","code":""},{"path":"forecast-evaluation.html","id":"the-need-for-the-forecast-evaluation","chapter":"9 Forecast Evaluation","heading":"9.1 The Need for the Forecast Evaluation","text":"typically several candidate econometric models methods forecast economic variable interest. Among , tend select adequate using -sample goodness fit measures (e.g., AIC SIC).sensible approach, least standpoint forecaster, assess goodness fit --sample setting. Recall models offer superior -sample fit don’t necessarily produce accurate --sample forecasts. , better -sample fit can achieved incorporating additional parameters model, complex models extrapolate estimated parameter uncertainty forecasts, thus sabotaging accuracy.Thus far applied following algorithm identify ‘best’ among competing forecasts:Decide loss function (e.g., quadratic loss).Obtain forecasts, forecast errors, corresponding sample expected loss (e.g., root mean squared forecast error) model consideration.Rank models according sample expected loss values.Select model lowest sample expected loss.loss function function random variable, practice deal sample information, sampling variation needs taken account. Statistical methods evaluation , therefore, desirable (least).","code":""},{"path":"forecast-evaluation.html","id":"relative-forecast-accuracy-tests","chapter":"9 Forecast Evaluation","heading":"9.2 Relative Forecast Accuracy Tests","text":"cover two tests hypothesis two forecasts equivalent, sense associated loss differential statistically significantly different zero.Consider time series length \\(T\\). Suppose \\(h\\)-step-ahead forecasts periods \\(R+h\\) \\(T\\) generated two competing models \\(\\) \\(j\\): \\(y_{t+h|t,}\\) \\(y_{t+h|t,j}\\), corresponding forecast errors: \\(e_{t+h,}\\) \\(e_{t+h,j}\\). null hypothesis equal predictive ability can given terms unconditional expectation loss differential: \\[H_0: E\\left[d(e_{t+h,ij})\\right] = 0,\\] \\(d(e_{t+h,ij}) = L(e_{t+h,})-L(e_{t+h,j})\\).","code":""},{"path":"forecast-evaluation.html","id":"the-morgan-granger-newbold-test","chapter":"9 Forecast Evaluation","heading":"9.2.1 The Morgan-Granger-Newbold Test","text":"Morgan-Granger-Newbold (MGN) test based auxiliary variables: \\(u_{1,t+h} = e_{t+h,}-e_{t+h,j}\\) \\(u_{2,t+h} = e_{t+h,}+e_{t+h,j}\\). follows : \\[E(u_{1,t+h},u_{2,t+h}) = MSFE(,t+h)-MSFE(j,t+h).\\] Thus, hypothesis interest equivalent testing whether two auxiliary variables correlated.MGN test statistic : \\[\\frac{r}{\\sqrt{(P-1)^{-1}(1-r^2)}}\\sim t_{P-1},\\] \\(t_{P-1}\\) Student t distribution \\(P-1\\) degrees freedom, \\(P\\) number --sample forecasts, \\[r=\\frac{\\sum_{t=R}^{T-h}{u_{1,t+h}u_{2,t+h}}}{\\sqrt{\\sum_{t=R}^{T-h}{u_{1,t+h}^2}\\sum_{t=R}^{T-h}{u_{2,t+h}^2}}}\\]MGN test relies assumption forecast errors (forecasts compared) unbiased, normally distributed, uncorrelated (). rather strict assumptions , often, violated empirical applications.","code":""},{"path":"forecast-evaluation.html","id":"the-diebold-mariano-test","chapter":"9 Forecast Evaluation","heading":"9.2.2 The Diebold-Mariano Test","text":"Diebold-Mariano (DM) test relaxes aforementioned requirements forecast errors. DM test statistic : \\[\\frac{\\bar{d}_h}{\\sqrt{\\sigma_d^2/P}} \\sim N(0,1),\\] \\(\\bar{d}_h=P^{-1}\\sum_{t=R}^{T-h} d(e_{t+h,ij})\\), \\(P=T-R-h+1\\) total number forecasts generated using rolling recursive widnow scheme, example.modified version DM statistic, due Harvey, Leybourne, Newbold (1998), addresses finite sample properties test, : \\[\\sqrt{\\frac{P+1-2h+P^{-1}h(h-1)}{P}}DM\\sim t_{P-1},\\] \\(t_{P-1}\\) Student t distribution \\(P-1\\) degrees freedom.practice, test equal predictive ability can applied within framework regression model follows: \\[d_{t+h} = \\delta + \\upsilon_{t+h}\\;~~t = R,\\ldots,T-h,\\] \\(d_{t+h} \\equiv d(e_{t+h,ij})\\). null equal predictive ability equivalent testing \\(H_0: \\delta = 0\\) OLS setting. Moreover, \\(d_{t+h}\\) may serially correlated, autocorrelation consistent standard errors used inference.","code":""},{"path":"forecast-combination.html","id":"forecast-combination","chapter":"10 Forecast Combination","heading":"10 Forecast Combination","text":"","code":""},{"path":"introduction-to-r.html","id":"introduction-to-r","chapter":"Introduction to R","heading":"Introduction to R","text":"R programming language data analysis visualisation. introduce basic commands facilitate understanding R. can enhance skillset using numerous online resources, well trial––error. extent new features added R daily basis, virtually limits far can advance knowledge programming language.work RStudio—go-interface R (R overly user-friendly platform). Thus, need installed , R RStudio devise (latter ‘find’ connect former ). R available CRAN, RStudio available RStudio.","code":""},{"path":"introduction-to-r.html","id":"data-management","chapter":"Introduction to R","heading":"Data Management","text":"number ways can work data R. Let’s begin matrices.Consider string observations:string, unlike vector, dimensions. can transform \\(n \\times 1\\) vector using .matrix() function:result \\(6 \\times 1\\) vector, column matrix. obtain \\(1 \\times 6\\) vector, row matrix, transpose foregoing vector using t() function:can create \\(n \\times k\\) matrix, using matrix() function. example, consider \\(3 \\times 2\\) matrix:can add column names row names matrix:, point, like work , say, first column matrix, can call using column number, (1), column name, (“c1”), follows:Similarly, want refer matrix element, say \\(b_{3,2}\\), can follows:Matrix multiplication done using %*% command, granted two matrices compatible. example, obtain product matrix \\(B\\) new \\(2 \\times 1\\) vector, \\(d\\), follows:can add columns (rows) existing matrix using cbind() function:can invert (n invertible) matrix using solve() function:","code":"\na <- c(1,0,4,3,2,6)\na## [1] 1 0 4 3 2 6\nb <- as.matrix(a)\nb##      [,1]\n## [1,]    1\n## [2,]    0\n## [3,]    4\n## [4,]    3\n## [5,]    2\n## [6,]    6\nbt <- t(b)\nbt##      [,1] [,2] [,3] [,4] [,5] [,6]\n## [1,]    1    0    4    3    2    6\ndim(bt)## [1] 1 6\nB <- matrix(a,nrow=3,ncol=2)\nB##      [,1] [,2]\n## [1,]    1    3\n## [2,]    0    2\n## [3,]    4    6\ncolnames(B) <- c(\"c1\",\"c2\")\nrownames(B) <- c(\"r1\",\"r2\",\"r3\")\nB##    c1 c2\n## r1  1  3\n## r2  0  2\n## r3  4  6\nB[,\"c1\"]## r1 r2 r3 \n##  1  0  4\nB[3,2]## [1] 6\nd <- as.matrix(c(5,-2))\nBd <- B%*%d\nBd##    [,1]\n## r1   -1\n## r2   -4\n## r3    8\nc3 <- c(0,1,0)\nD <- cbind(B,c3)\nD##    c1 c2 c3\n## r1  1  3  0\n## r2  0  2  1\n## r3  4  6  0\nDi <- solve(D)\nDi##            r1 r2         r3\n## c1 -1.0000000  0  0.5000000\n## c2  0.6666667  0 -0.1666667\n## c3 -1.3333333  1  0.3333333"},{"path":"introduction-to-r.html","id":"data-visualisation","chapter":"Introduction to R","heading":"Data Visualisation","text":"One comparative advantages R graphing aesthetics. Currently, best graphs plotted via ggplot2 package. Notably, package requires data maintained data.frame data.table format (latter, need load data.table package). Let’s create data.table object observe lines:Now, let’s load ggplot2 generate simple scatter plot:can augment plot number different ways. change point color red, add fitted regression line plot, add labels figure, apply ‘classic’ background theme:another example, let’s generate histogram (dependent variable):typically apply line plot illustrate time series (ordered date). follows, add date column data frame plot dependent variable chronological order:","code":"\nset.seed(1)\nx <- runif(120,0,2)\ny <- 0.2+0.7*x+rnorm(120)\n\nlibrary(data.table)\n\ndt <- data.table(y=y,x=x)\ndt##               y          x\n##   1:  2.9733299 0.53101733\n##   2:  0.6817335 0.74424780\n##   3:  1.6917341 1.14570673\n##   4:  1.4994931 1.81641558\n##   5: -0.2609185 0.40336386\n##  ---                      \n## 116:  0.1835826 0.02615515\n## 117:  1.9894321 1.43113213\n## 118:  2.4197029 0.20636847\n## 119:  1.8521905 0.89256870\n## 120:  2.3040499 1.28020209\nlibrary(ggplot2)\n\nggplot(dt,aes(x=x,y=y))+\n  geom_point()\nggplot(dt,aes(x=x,y=y))+\n  geom_point(color=\"goldenrod\")+\n  geom_smooth(method=\"lm\",formula=y~x,se=F,color=\"darkgray\")+\n  labs(title=\"A scatterplot with fitted regression line\", \n       x=\"Treatment Variable\", \n       y=\"Outcome Variable\", \n       caption=\"Caption: in case if needed.\")+\n  theme_classic()\nggplot(dt,aes(x=y))+\n  geom_histogram(color=\"white\",fill=\"goldenrod\",binwidth=.5)+\n  labs(title=\"A basic histogram\")+\n  theme_classic()\ndt$date <- seq(from=as.Date(\"2000-01-01\"),by=\"month\",along.with=y)\n\nggplot(dt,aes(x=date,y=y))+\n  geom_line(color=\"goldenrod\")+\n  labs(title=\"A basic time series plot\",  \n       x=\"Year\", \n       y=\"Outcome Variable\")+\n  theme_classic()"},{"path":"introduction-to-r.html","id":"regression-analysis","chapter":"Introduction to R","heading":"Regression Analysis","text":"illustrate OLS regression R, apply previously generated \\(x\\) \\(y\\) independent dependent variables. begin, obtain least squares estimator “hand” follows:can easily done using lm() function:can apply summary() function see complete set regression results:","code":"\nX <- cbind(1,x)\nb <- solve(t(X)%*%X)%*%t(X)%*%y\nb##        [,1]\n##   0.3577680\n## x 0.5781188\nols <- lm(y~x)\nols## \n## Call:\n## lm(formula = y ~ x)\n## \n## Coefficients:\n## (Intercept)            x  \n##      0.3578       0.5781\nsummary(ols)## \n## Call:\n## lm(formula = y ~ x)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.9662 -0.5983 -0.1127  0.5639  2.3882 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   0.3578     0.1904   1.879 0.062717 .  \n## x             0.5781     0.1641   3.522 0.000609 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.9716 on 118 degrees of freedom\n## Multiple R-squared:  0.09514,    Adjusted R-squared:  0.08748 \n## F-statistic: 12.41 on 1 and 118 DF,  p-value: 0.0006091"},{"path":"tutorial-1.html","id":"tutorial-1","chapter":"Tutorial 1","heading":"Tutorial 1","text":"tutorial, introduce several simple R functions, perform basic forecasting exercise.Let’s generate sequence 200 iid random variables mean zero variance 4, call e.Notice prior sampling set seed value (one instance). ensure can exactly replicate sample future.Nett, generate sequence 200 binary variables, call x.Construct dependent variable, y, using following formula: \\(y=2+0.5x+e\\).Regress y x, using lm() function, obtain estimates intercept slope parameters.Generate “future” realizations (100 observations) y.Note represent actual realizations variable; forecasts.Suppose think considered forecast period, x takes 1 (refer Model 1). Based , using parameter estimates , let’s generate forecasts period.point, actual realisations y forecasts. Thus, can obtain forecast errors, mean absolute forecast errors, root mean square forecast errors.Suppose, instead, think considered forecast period x takes 0 (refer Model 2). Based , using parameter estimates , let’s generate forecasts period.Using forecasts, obtain forecast errors, mean absolute forecast errors, root mean square forecast errors.comparing two sets forecasts, can observe somewhat rare yet unlikely scenario: MAFE points Model 1 accurate two models, RMSFE suggests Model 2 accurate one. often , however, two accuracy measures tend agree.","code":"\nset.seed(1)\ne <- rnorm(200,0,2)\nset.seed(2)\nx <- sample(c(0,1),200,replace=T)\ny <- 2+0.5*x+e\nols <- lm(y~x)\nols## \n## Call:\n## lm(formula = y ~ x)\n## \n## Coefficients:\n## (Intercept)            x  \n##      2.1244       0.3854\nset.seed(3)\ne <- rnorm(100,0,2)\n\nset.seed(4)\nx <- sample(c(0,1),100,replace=T)\n\ny <- 2+0.5*x+e\ny_f1 <- ols$coefficients[1]+ols$coefficients[2]*rep(1,100)\ne_f1 <- y-y_f1\n\nmafe1 <- mean(abs(e_f1))\nrmsfe1 <- sqrt(mean(e_f1^2))\n\nmafe1## [1] 1.43523\nrmsfe1## [1] 1.739508\ny_f0 <- ols$coefficients[1]+ols$coefficients[2]*rep(0,100)\ne_f0 <- y-y_f0\n\nmafe0 <- mean(abs(e_f0))\nrmsfe0 <- sqrt(mean(e_f0^2))\n\nmafe0## [1] 1.455768\nrmsfe0## [1] 1.736182"},{"path":"tutorial-2.html","id":"tutorial-2","chapter":"Tutorial 2","heading":"Tutorial 2","text":"tutorial, introduce ‘loop’, use generate time series well obtain one-step-ahead forecasts using rolling window procedure; also perform forecast error diagnostics.Let’s generate random walk process, \\(y_{t} = y_{t-1}+e_{t}\\), \\(e_{t} ~ N(0,1)\\), \\(y_{0}=0\\), \\(t=1,\\ldots,120\\).Store \\(y\\) \\(e\\) data.table, call ‘dt’. Add arbitrary dates data (e.g., suppose deal monthly series beginning January 2011).Plot realized time series using ggplot function.Generate sequence one-step-ahead forecasts naive average methods, using rolling window scheme, first rolling window ranges period 1 period 80.Calculate RMSFE measures two forecasting methods.Perform forecast error diagnostics two considered methods.Zero mean forecast errors: \\(E(e_{t+1|t})=0\\).\nperform test regressing forecast error constant, checking whether coefficient statistically significantly different zero.correlation forecast errors forecasts: \\(Cov(e_{t+1|t},y_{t+1|t})=0\\). perform test regressing forecast error forecast, checking whether slope coefficient statistically significantly different zero.serial correlation one-step-ahead forecast errors: \\(Cov(e_{t+1|t},y_{t|t-1})=0\\). perform test regressing forecast error lag, checking whether slope coefficient statistically significantly different zero.\n(Note: first need generate lagged forecast errors)","code":"\nn <- 120\n\nset.seed(1)\ne <- rnorm(n)\n\ny <- rep(NA,n)\n\ny[1] <- e[1]\n\nfor(i in 2:n){\n  y[i] <- y[i-1] + e[i]\n}\ndt <- data.table(y,e)\n\ndt$date <- seq(as.Date(\"2011-01-01\"),as.Date(\"2020-12-01\"),by=\"month\")\nggplot(dt,aes(x=date,y=y))+\n  geom_line(size=1)+\n  labs(x=\"Year\",y=\"Random Walk\")+\n  theme_classic()\ndt$average <- NA\ndt$naive <- NA\n\nR <- 80\nP <- n-R\nfor(i in 1:P){\n  w <- y[i:(R-1+i)]\n  dt$average[R+i] <- mean(w)\n  dt$naive[R+i] <- w[length(w)]\n}\ndt[,`:=`(e_average=y-average,e_naive=y-naive)]\n\nrmsfe_average <- sqrt(mean(dt$e_average^2,na.rm=T))\nrmsfe_naive <- sqrt(mean(dt$e_naive^2,na.rm=T))\n\nrmsfe_average## [1] 4.672947\nrmsfe_naive## [1] 0.850081\nsummary(lm(e_average~1,data=dt))$coefficients##             Estimate Std. Error  t value     Pr(>|t|)\n## (Intercept) 4.434858  0.2358002 18.80769 3.682273e-21\nsummary(lm(e_naive~1,data=dt))$coefficients##              Estimate Std. Error t value Pr(>|t|)\n## (Intercept) 0.1168396    0.13483 0.86657 0.391478\nsummary(lm(e_average~average,data=dt))$coefficients##              Estimate Std. Error  t value   Pr(>|t|)\n## (Intercept) 2.4180785  1.2929708 1.870172 0.06917788\n## average     0.2942557  0.1856048 1.585389 0.12116580\nsummary(lm(e_naive~naive,data=dt))$coefficients##                Estimate Std. Error   t value  Pr(>|t|)\n## (Intercept)  1.06489905 0.69740557  1.526944 0.1350565\n## naive       -0.08486143 0.06127484 -1.384931 0.1741512\ndt[,`:=`(e_average.l1=shift(e_average),e_naive.l1=shift(e_naive))]\n\nsummary(lm(e_average~e_average.l1,data=dt))$coefficients##               Estimate Std. Error  t value     Pr(>|t|)\n## (Intercept)  0.7898068 0.42027705 1.879253 6.810403e-02\n## e_average.l1 0.8275396 0.08966026 9.229726 3.892504e-11\nsummary(lm(e_naive~e_naive.l1,data=dt))$coefficients##               Estimate Std. Error   t value  Pr(>|t|)\n## (Intercept) 0.12971406  0.1403668 0.9241081 0.3614178\n## e_naive.l1  0.03780853  0.1631333 0.2317647 0.8179979"},{"path":"tutorial-3.html","id":"tutorial-3","chapter":"Tutorial 3","heading":"Tutorial 3","text":"tutorial, generate trending series, apply information criterion select suitable trend model, obtain compare one-step-ahead forecasts using rolling window procedure.Let’s generate time series follows quadratic trend: \\(y_{t} = 10+0.01t+0.002t^2+e_{t}\\), \\(e_{t} \\sim N(0,16)\\), \\(t=1,\\ldots,180\\).Store \\(y\\) \\(trend\\) data.table, call ‘dt’. Add arbitrary dates data (e.g., suppose deal monthly series beginning January 2006).Plot realized time series using ggplot function.Calculate Akaike Information Criteria linear, quadratic, cubic, exponential trend models, using observations series.Generate sequence one-step-ahead forecasts linear, quadratic, cubic, exponential trend models, using rolling window scheme, first rolling window ranges period 1 period 120.Plot original series overlay one-step-ahead forecasts four considered trend models. Note, convenience first ‘melt’ data.table ‘long’ format.Calculate RMSFE measures two forecasting methods.","code":"\nn <- 180\n\nset.seed(7)\ne <- rnorm(n,0,4)\n\ntrend <- c(1:n)\n\ny <- 10+0.01*trend+0.002*trend^2+e\ndt <- data.table(y,trend)\n\ndt$date <- seq(as.Date(\"2006-01-01\"),by=\"month\",along.with=y)\nggplot(dt,aes(x=date,y=y))+\n  geom_line(size=1)+\n  labs(x=\"Year\",y=\"Trending Series\")+\n  theme_classic()\nAIC_vec <- matrix(ncol=4,nrow=1)\nfor(i in 1:4){\n  if(i < 4){\n    reg <- lm(y~poly(trend,degree=i,raw=T),data=dt)\n    AIC_vec[i] <- log(crossprod(reg$residuals))+2*length(reg$coefficients)/n\n  }else{\n    reg <- lm(log(y)~trend,data=dt)\n    yhat <- reg$fitted.values\n    sig <- sd(reg$residuals)\n    ystar <- exp(yhat+sig^2/2)\n    res <- dt$y-ystar\n    AIC_vec[i] <- log(crossprod(res))+2*length(reg$coefficients)/n\n  }\n}\n\nAIC_vec##          [,1]     [,2]     [,3]     [,4]\n## [1,] 8.879646 7.841645 7.849521 7.975999\ndt$t1 <- NA\ndt$t2 <- NA\ndt$t3 <- NA\ndt$te <- NA\n\nR <- 120\nP <- n-R\nfor(i in 1:P){\n  reg1 <- lm(y~trend,data=dt[i:(R-1+i)])\n  dt$t1[R+i] <- reg1$coef[1]+reg1$coef[2]*(R+i)\n  \n  reg2 <- lm(y~poly(trend,degree=2,raw=T),data=dt[i:(R-1+i)])\n  dt$t2[R+i] <- reg2$coef[1]+reg2$coef[2]*(R+i)+reg2$coef[3]*((R+i)^2)\n  \n  reg3 <- lm(y~poly(trend,degree=3,raw=T),data=dt[i:(R-1+i)])\n  dt$t3[R+i] <- reg3$coef[1]+reg3$coef[2]*(R+i)+reg3$coef[3]*((R+i)^2)+reg3$coef[4]*((R+i)^3)\n  \n  rege <- lm(log(y)~trend,data=dt[i:(R-1+i)])\n  sig <- sd(rege$residuals)\n  dt$te[R+i] <- exp(rege$coef[1]+rege$coef[2]*(R+i)+sig^2/2)\n}\ndt_long <- melt(dt[,.(date,y,linear=t1,quadratic=t2,cubic=t3,exponential=te)],id.vars=\"date\")\n\nggplot(dt_long,aes(x=date,y=value,color=variable))+\n  geom_line(size=1,na.rm=T)+\n  scale_color_manual(values=c(\"darkgray\",\"black\",\"goldenrod\",\"steelblue\",\"indianred\"))+\n  labs(x=\"Year\",y=\"Trending Series and Forecasts\")+\n  theme_classic()+\n  theme(legend.title=element_blank(),legend.position=c(.15,.85))\ndt[,`:=`(e_t1=y-t1,e_t2=y-t2,e_t3=y-t3,e_te=y-te)]\n\nrmsfe_t1 <- sqrt(mean(dt$e_t1^2,na.rm=T))\nrmsfe_t2 <- sqrt(mean(dt$e_t2^2,na.rm=T))\nrmsfe_t3 <- sqrt(mean(dt$e_t3^2,na.rm=T))\nrmsfe_te <- sqrt(mean(dt$e_te^2,na.rm=T))\n\nrmsfe_t1## [1] 6.151904\nrmsfe_t2## [1] 3.796861\nrmsfe_t3## [1] 3.906023\nrmsfe_te## [1] 5.132312"},{"path":"tutorial-4.html","id":"tutorial-4","chapter":"Tutorial 4","heading":"Tutorial 4","text":"tutorial, generate autocorrelated series, apply information criterion select suitable autoregressive model, obtain compare one-step-ahead forecasts competing models using rolling window procedure, generate one set multi-step forecasts illustrate convergence unconditional mean series.Let’s generate time series follows AR(2) process: \\(y_{t} = 1.2y_{t-1}-0.3y_{t-2}+e_{t}\\), \\(e_{t} \\sim N(0,1)\\), \\(t=1,\\ldots,180\\).Generate vector arbitrary dates (e.g., suppose deal monthly series beginning January 2006), store along \\(y\\) data.table, call ‘dt’.Plot realized time series using ggplot function.Generate plot autocorrelation function partial autocorrelation function lags 1 18.Calculate Akaike Information Criteria (AIC) Schwarz Information Criteria (SIC) AR(1), AR(2), AR(3), AR(4) models, using observations series, decide optimal lag length.Generate sequence one-step-ahead forecasts random walk, well AR(1), AR(2), AR(3), using rolling window scheme, first rolling window ranges period 1 period 120.Calculate RMSFE measures considered models.Using first rolling window information set, generate multi-step-ahead forecast hold-period.","code":"\nn <- 180\n\nset.seed(7)\ne <- rnorm(n,0,1)\n\ny <- rep(NA,n)\ny[1] <- e[1]\ny[2] <- 1.2*y[1]+e[2]\nfor(i in 3:n){\n  y[i] <- 1.2*y[i-1]-0.3*y[i-2]+e[i]\n}\ndate <- seq(as.Date(\"2006-01-01\"),by=\"month\",along.with=y)\n\ndt <- data.table(date,y)\nggplot(dt,aes(x=date,y=y))+\n  geom_line(size=1)+\n  labs(x=\"Year\",y=\"Trending Series\")+\n  theme_classic()\nacf_vec <- c(acf(dt$y,lag.max=18,plot=F)$acf[-1])\npacf_vec <- c(pacf(dt$y,lag.max=18,plot=F)$acf)\n\nsd_rho <- sqrt(1/n)\n\nacf_dt <- data.table(lags=1:18,acf=acf_vec,pacf=pacf_vec)\n\nggplot(acf_dt,aes(x=lags,y=acf)) +\n  geom_hline(yintercept=0,color=\"darkgray\",linetype=3,size=.5) +\n  geom_hline(yintercept=c(-1.96*sd_rho,1.96*sd_rho),color=\"goldenrod\",linetype=2,size=.6) +\n  geom_segment(aes(xend=lags,yend=0),color=\"steelblue\",size=.8)+\n  labs(x=\"Lag\",y=\"ACF\")+\n  coord_cartesian(ylim=c(-1,1))+\n  theme_classic()\nggplot(acf_dt,aes(x=lags,y=pacf)) +\n  geom_hline(yintercept=0,color=\"darkgray\",linetype=3,size=.5) +\n  geom_hline(yintercept=c(-1.96*sd_rho,1.96*sd_rho),color=\"goldenrod\",linetype=2,size=.6) +\n  geom_segment(aes(xend=lags,yend=0),color=\"steelblue\",size=.8)+\n  labs(x=\"Lag\",y=\"PACF\")+\n  coord_cartesian(ylim=c(-1,1))+\n  theme_classic()\ndt[,`:=`(y_l1=shift(y),y_l2=shift(y,2),y_l3=shift(y,3),y_l4=shift(y,4))]\n\n# get rid of the rows with NAs\ndt <- dt[complete.cases(dt)]\n\nIC_dt <- data.table(lag=c(1:4),AIC=NA,SIC=NA)\n\nfor(i in 1:nrow(IC_dt)){\n  \n  fmla <- as.formula(paste(\"y\",paste0(\"y_l\",c(1:i),collapse=\"+\"),sep=\"~\"))\n  reg.ar <- lm(fmla,data=dt)\n  \n  IC_dt$AIC[i] <- log(crossprod(reg.ar$residuals))+2*(i+1)/nrow(dt)\n  IC_dt$SIC[i] <- log(crossprod(reg.ar$residuals))+log(nrow(dt))*(i+1)/nrow(dt)\n\n}\n\nIC_dt##    lag      AIC      SIC\n## 1:   1 5.192156 5.228184\n## 2:   2 5.011351 5.065393\n## 3:   3 5.018687 5.090743\n## 4:   4 5.024127 5.114198\nR <- 120\nP <- nrow(dt)-R\n\ndt$rw <- NA\ndt$ar1 <- NA\ndt$ar2 <- NA\n\nfor(i in 1:P){\n  dt$rw[R+i] <- dt$y[R-1+i]\n  \n  ar1 <- lm(y~y_l1,data=dt[i:(R-1+i)])\n  ar2 <- lm(y~y_l1+y_l2,data=dt[i:(R-1+i)])\n  \n  dt$ar1[R+i] <- ar1$coefficients[1]+ar1$coefficients[2]*dt$y[R-1+i]\n  dt$ar2[R+i] <- ar2$coefficients[1]+ar2$coefficients[2]*dt$y[R-1+i]+ar2$coefficients[3]*dt$y[R-2+i]\n}\ndt[,`:=`(rw_e=y-rw,ar1_e=y-ar1,ar2_e=y-ar2)]\n\nrmsfe_rw <- sqrt(mean(dt$rw_e^2,na.rm=T))\nrmsfe_ar1 <- sqrt(mean(dt$ar1_e^2,na.rm=T))\nrmsfe_ar2 <- sqrt(mean(dt$ar2_e^2,na.rm=T))\n\nrmsfe_rw## [1] 0.9569683\nrmsfe_ar1## [1] 0.9298374\nrmsfe_ar2## [1] 0.8901728\ndt[,`:=`(ar2_multi=y)]\n\nar2 <- lm(y~y_l1+y_l2,data=dt[1:R])\n\nfor(i in 1:P){\n\n  dt$ar2_multi[R+i] <- ar2$coefficients[1]+ar2$coefficients[2]*dt$ar2_multi[R-1+i]+ar2$coefficients[3]*dt$ar2_multi[R-2+i]\n  \n}\n\ndt[1:R]$ar2_multi <- NA\n\nggplot(dt,aes(x=date))+\n  geom_line(aes(y=y),color=\"darkgray\",size=1)+\n  geom_line(aes(y=ar2_multi),color=\"steelblue\",na.rm=T,size=1,linetype=5)+\n  theme_classic()"},{"path":"tutorial-5.html","id":"tutorial-5","chapter":"Tutorial 5","heading":"Tutorial 5","text":"tutorial, generate bivariate series, apply system-wide information criterion select suitable vector autoregressive model, perform -sample test Granger causality, obtain compare one-step-ahead forecasts competing models using rolling window procedure, investigate evidence Granger causality --sample setting. run code, data.table MASS packages need installed loaded.Let’s generate two-dimensional vector time series follow VAR(1) process following form: \\[\\begin{aligned}\nx_{1,t} &= 0.3 + 0.7x_{1,t-1} + 0.1x_{2,t-1} + \\varepsilon_{1,t} \\\\\nx_{2,t} &= -0.2 + 0.9x_{1,t-1} + \\varepsilon_{2,t}\n\\end{aligned}\\] \\(\\mathbf{e}_{t} \\sim N(\\mathbf{0},\\Sigma)\\), \\(\\Sigma\\) covariance matrix residuals \\(Cov(\\varepsilon_{1,t},\\varepsilon_{2,t}) = 0.3\\) \\(t=1,\\ldots,180\\). (Note: code, \\(x_1\\) denoted \\(y\\) \\(x_2\\) denoted \\(x\\)).Generate vector arbitrary dates (e.g., suppose deal monthly series beginning January 2006), store along \\(y\\) data.table, call ‘dt’.Plot realized time series using ggplot function.Estimate VAR(1) VAR(2) running regressions equation separately. Collect residuals obtain system-wide AIC two models.Perfrom tests (-sample) Granger causality two models. Note, case VAR(1), t tests F tests applicable provide identical inference. case VAR(p), \\(p>1\\), appropriate test F test joint significance parameters associated lags potentially causal variable.Generate sequence one-step-ahead forecasts VAR(1) using rolling window scheme, first rolling window ranges period 1 period 120.Calculate RMSFE measures restricted unrestricted models, compare make suggestion --sample Granger causality.","code":"\nn <- 180\n\nR <- matrix(c(1,0.3,0.3,1),nrow=2,ncol=2)\nset.seed(1)\ne <- mvrnorm(n,mu=c(0,0),Sigma=R)\n\ne_y <- e[,1]\ne_x <- e[,2]\n\ny <- rep(NA,n)\nx <- rep(NA,n)\n\ny[1] <- e_y[1]\nx[1] <- e_x[1]\n\nfor(i in 2:n){\n  y[i] <- 0.3+0.7*y[i-1]+0.1*x[i-1]+e_y[i]\n  x[i] <- -0.2+0.9*x[i-1]+e_x[i]\n}\ndate <- seq(as.Date(\"2006-01-01\"),by=\"month\",along.with=y)\n\ndt <- data.table(date,y,x)\ndt_long <- melt(dt,id.vars=\"date\")\n\nggplot(dt_long,aes(x=date,y=value,color=variable,linetype=variable))+\n  geom_line(size=1)+\n  scale_color_manual(values=c(\"darkgray\",\"steelblue\"))+\n  labs(x=\"Year\",y=\"Series\")+\n  theme_classic()\ndt[,`:=`(y_l1=shift(y,1),y_l2=shift(y,2),x_l1=shift(x,1),x_l2=shift(x,2))]\n\n# VAR(1)\np <- 1\nk <- 2\n\nvar1y <- lm(y~y_l1+x_l1,data=dt)\nvar1x <- lm(x~y_l1+x_l1,data=dt)\n\nvar1r <- cbind(var1y$residuals,var1x$residuals)\ncov1r <- crossprod(var1r)/(nrow(dt)-(p*k^2+k))\n\nAIC1 <- log(det(cov1r))+2*(p*k^2+k)/nrow(dt)\n\n# VAR(2)\np <- 2\nk <- 2\n\nvar2y <- lm(y~y_l1+y_l2+x_l1+x_l2,data=dt)\nvar2x <- lm(x~y_l1+y_l2+x_l1+x_l2,data=dt)\n\nvar2r <- cbind(var2y$residuals,var2x$residuals)\ncov2r <- crossprod(var2r)/(nrow(dt)-(p*k^2+k))\n\nAIC2 <- log(det(cov2r))+2*(p*k^2+k)/nrow(dt)\n\nAIC1## [1] -0.1270596\nAIC2## [1] -0.047212\n# VAR(1)\n\n## t test\nsummary(var1y)## \n## Call:\n## lm(formula = y ~ y_l1 + x_l1, data = dt)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -2.32279 -0.63680 -0.00953  0.68826  2.63468 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  0.34926    0.11146   3.134  0.00202 ** \n## y_l1         0.68903    0.05877  11.725  < 2e-16 ***\n## x_l1         0.11304    0.04509   2.507  0.01308 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.976 on 176 degrees of freedom\n##   (1 observation deleted due to missingness)\n## Multiple R-squared:  0.5689, Adjusted R-squared:  0.564 \n## F-statistic: 116.1 on 2 and 176 DF,  p-value: < 2.2e-16\nsummary(var1x)## \n## Call:\n## lm(formula = x ~ y_l1 + x_l1, data = dt)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -2.38491 -0.68087  0.03216  0.66109  2.74762 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -0.22353    0.10805  -2.069    0.040 *  \n## y_l1         0.05814    0.05697   1.021    0.309    \n## x_l1         0.85285    0.04371  19.511   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.9461 on 176 degrees of freedom\n##   (1 observation deleted due to missingness)\n## Multiple R-squared:  0.7536, Adjusted R-squared:  0.7508 \n## F-statistic: 269.2 on 2 and 176 DF,  p-value: < 2.2e-16\n## F test\nar1y <- lm(y~y_l1,data=dt)\nar1x <- lm(x~x_l1,data=dt)\n\nanova(var1y,ar1y)## Analysis of Variance Table\n## \n## Model 1: y ~ y_l1 + x_l1\n## Model 2: y ~ y_l1\n##   Res.Df    RSS Df Sum of Sq      F  Pr(>F)  \n## 1    176 167.64                              \n## 2    177 173.62 -1   -5.9866 6.2852 0.01308 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nanova(var1x,ar1x)## Analysis of Variance Table\n## \n## Model 1: x ~ y_l1 + x_l1\n## Model 2: x ~ x_l1\n##   Res.Df    RSS Df Sum of Sq      F Pr(>F)\n## 1    176 157.54                           \n## 2    177 158.47 -1  -0.93231 1.0416 0.3089\n## VAR(2)\n\n### t test (no longer applicable to test GC)\nsummary(var2y)## \n## Call:\n## lm(formula = y ~ y_l1 + y_l2 + x_l1 + x_l2, data = dt)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -2.38088 -0.71387 -0.01504  0.72538  2.70511 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  0.32203    0.11775   2.735  0.00689 ** \n## y_l1         0.65954    0.07822   8.431  1.3e-14 ***\n## y_l2         0.04964    0.07965   0.623  0.53397    \n## x_l1         0.15919    0.08052   1.977  0.04962 *  \n## x_l2        -0.05991    0.08104  -0.739  0.46080    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.9816 on 173 degrees of freedom\n##   (2 observations deleted due to missingness)\n## Multiple R-squared:  0.5709, Adjusted R-squared:  0.561 \n## F-statistic: 57.55 on 4 and 173 DF,  p-value: < 2.2e-16\nsummary(var2x)## \n## Call:\n## lm(formula = x ~ y_l1 + y_l2 + x_l1 + x_l2, data = dt)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -2.37790 -0.64364  0.05401  0.67542  2.71827 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -0.24577    0.11419  -2.152   0.0328 *  \n## y_l1         0.03070    0.07586   0.405   0.6862    \n## y_l2         0.04486    0.07724   0.581   0.5621    \n## x_l1         0.86196    0.07809  11.039   <2e-16 ***\n## x_l2        -0.01627    0.07859  -0.207   0.8362    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.9519 on 173 degrees of freedom\n##   (2 observations deleted due to missingness)\n## Multiple R-squared:  0.7546, Adjusted R-squared:  0.7489 \n## F-statistic:   133 on 4 and 173 DF,  p-value: < 2.2e-16\n### F test\nar2y <- lm(y~y_l1+y_l2,data=dt)\nar2x <- lm(x~x_l1+x_l2,data=dt)\n\nanova(var2y,ar2y)## Analysis of Variance Table\n## \n## Model 1: y ~ y_l1 + y_l2 + x_l1 + x_l2\n## Model 2: y ~ y_l1 + y_l2\n##   Res.Df    RSS Df Sum of Sq      F  Pr(>F)  \n## 1    173 166.68                              \n## 2    175 172.78 -2   -6.0971 3.1641 0.04471 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nanova(var2x,ar2x)## Analysis of Variance Table\n## \n## Model 1: x ~ y_l1 + y_l2 + x_l1 + x_l2\n## Model 2: x ~ x_l1 + x_l2\n##   Res.Df    RSS Df Sum of Sq      F Pr(>F)\n## 1    173 156.76                           \n## 2    175 158.03 -2   -1.2729 0.7024 0.4968\nR <- 120\nP <- nrow(dt)-R\n\ndt$ar1y <- NA\ndt$ar1x <- NA\ndt$var1y <- NA\ndt$var1x <- NA\n\nfor(i in 1:P){\n  \n  ar1y <- lm(y~y_l1,data=dt[i:(R-1+i)])\n  ar1x <- lm(x~x_l1,data=dt[i:(R-1+i)])\n  \n  var1y <- lm(y~y_l1+x_l1,data=dt[i:(R-1+i)])\n  var1x <- lm(x~y_l1+x_l1,data=dt[i:(R-1+i)])\n  \n  dt$ar1y[R+i] <- ar1y$coefficients[1]+ar1y$coefficients[2]*dt$y[R-1+i]\n  dt$ar1x[R+i] <- ar1x$coefficients[1]+ar1x$coefficients[2]*dt$x[R-1+i]\n  \n  dt$var1y[R+i] <- var1y$coefficients[1]+var1y$coefficients[2]*dt$y[R-1+i]+var1y$coefficients[3]*dt$x[R-1+i]\n  dt$var1x[R+i] <- var1x$coefficients[1]+var1x$coefficients[2]*dt$y[R-1+i]+var1x$coefficients[3]*dt$x[R-1+i]\n  \n}\ndt[,`:=`(ar1y_e=y-ar1y,ar1x_e=x-ar1x,var1y_e=y-var1y,var1x_e=x-var1x)]\n\n# calculate RMSFE for restructed and unrestricted models\nrmsfe_yr <- sqrt(mean(dt$ar1y_e^2,na.rm=T))\nrmsfe_yu <- sqrt(mean(dt$var1y_e^2,na.rm=T))\n\nrmsfe_xr <- sqrt(mean(dt$ar1x_e^2,na.rm=T))\nrmsfe_xu <- sqrt(mean(dt$var1x_e^2,na.rm=T))\n\nrmsfe_yr## [1] 1.134867\nrmsfe_yu## [1] 1.104268\nrmsfe_xr## [1] 1.00908\nrmsfe_xu## [1] 1.011653"},{"path":"tutorial-6.html","id":"tutorial-6","chapter":"Tutorial 6","heading":"Tutorial 6","text":"tutorial, generate regime-dependent series, apply grid-search method obtain threshold parameter, obtain compare one-step-ahead forecasts competing models using rolling window procedure, apply bootstrap resampling method generate multi-step-ahead forecasts threshold regression. run code, data.table ggplot2 packages need installed loaded.Let’s generate time series follow TAR(2) process following form:\n\\[y_t = \\left\\{\\begin{array}\n{ll}\ny_{t-1} + \\varepsilon_t & \\text{}~~y_{t-1}\\ge0 \\\\\n1.2y_{t-1}-0.3y_{t-2} + \\varepsilon_t & \\text{}~~y_{t-1} < 0\n\\end{array}\\right.\n\\]\n\\(e_{t} \\sim N(0,\\sigma^2)\\). suggests time series follow unit root process lagged dependent variable non-negative, otherwise time series mean-reverting AR(2) process.Generate vector arbitrary dates (e.g., suppose deal monthly series beginning January 1991), store along \\(y\\) data.table, call ‘dt’.Plot realized time series using ggplot function.Decide optimal lag length based AIC.now need get estimate threshold parameter (.e., value switch regimes happens). , perform grid-search routine. consider range candidate thresholds within 10th 90th percentile lagged dependent variable. candidate threshold, run OLS calculate residual sums squares. threshold yields lowest residual sum squares estimate.Estimate threshold autoregression compare parameter estimates true parameters model.Generate sequence one-step-ahead forecasts TAR(2) using rolling window scheme, first rolling window ranges period 1 period 240. comparison, also generate one-step-ahead forecasts AR(2) random walk models. Calculate RMSFE measures three models.Obtain multi-step-ahead forecasts period 241 onward using -called ‘skeleton extrapolation’ method (yields biased forecasts) bootstrap resampling method (numerical method yields valid multi-step-ahead forecasts nonlinear models). Plot two forecasts along time series.","code":"\nn <- 360\n\nset.seed(6)\ne <- rnorm(n,0,1)\n\ny <- rep(NA,n)\ny[1] <- e[1]\nif(y[1]>=0){\n  y[2] <-  1.0*y[1]+e[2]\n}else{\n  y[2] <- 1.2*y[1]+e[2]\n}\n\nfor(i in 3:n){\n  if(y[i-1]>=0){\n    y[i] <-  1.0*y[i-1]+e[i]\n  }else{\n    y[i] <- 1.2*y[i-1]-0.3*y[i-2]+e[i]\n  }\n}\ndate <- seq(as.Date(\"1991-01-01\"),by=\"month\",along.with=y)\n\ndt <- data.table(date,y)\nggplot(dt,aes(x=date,y=y))+\n  geom_line(size=1)+\n  labs(x=\"Year\",y=\"Series\")+\n  theme_classic()\ndt[,`:=`(y_l1=shift(y),y_l2=shift(y,2),y_l3=shift(y,3),y_l4=shift(y,4))]\ndt <- dt[complete.cases(dt)]\n\nIC_dt <- data.table(lag=c(1:4),AIC=as.numeric(NA),SIC=as.numeric(NA))\n\nfor(i in 1:nrow(IC_dt)){\n  \n  fmla <- as.formula(paste(\"y\",paste0(\"y_l\",c(1:i),collapse=\"+\"),sep=\"~\"))\n  reg.ar <- lm(fmla,data=dt)\n  \n  IC_dt$AIC[i] <- log(crossprod(reg.ar$residuals))+2*(i+1)/nrow(dt)\n  IC_dt$SIC[i] <- log(crossprod(reg.ar$residuals))+log(nrow(dt))*(i+1)/nrow(dt)\n\n}\n\nIC_dt##    lag      AIC      SIC\n## 1:   1 5.822589 5.844358\n## 2:   2 5.821342 5.853996\n## 3:   3 5.826916 5.870454\n## 4:   4 5.832294 5.886717\nqy <- round(quantile(dt$y,c(.1,.9)),1)\n\ntr <- seq(qy[1],qy[2],by=.1)\n\ngrid_dt <- data.table(tr,ssr=NA)\ngrid_dt[,`:=`(ssr=as.numeric(ssr))]\n\nfor(i in tr){\n  \n  dt[,`:=`(d=ifelse(y_l1>=i,1,0))]\n  \n  tar <- lm(y~(y_l1+y_l2):I(d)+(y_l1+y_l2):I(1-d),data=dt)\n  \n  grid_dt[tr==i]$ssr <- crossprod(tar$residuals)\n  \n}\n\ntr_hat <- grid_dt[ssr==min(ssr)]$tr\ntr_hat## [1] -1\ndt[,`:=`(d=ifelse(y_l1>=tr_hat,1,0))]\n\ntar <- lm(y~(y_l1+y_l2):I(d)+(y_l1+y_l2):I(1-d),data=dt)\nsummary(tar)## \n## Call:\n## lm(formula = y ~ (y_l1 + y_l2):I(d) + (y_l1 + y_l2):I(1 - d), \n##     data = dt)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -2.97704 -0.60528 -0.02506  0.61754  2.66733 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   -0.01380    0.07519  -0.183 0.854535    \n## y_l1:I(d)      0.97060    0.06283  15.449  < 2e-16 ***\n## y_l2:I(d)      0.00998    0.06160   0.162 0.871403    \n## y_l1:I(1 - d)  1.24182    0.10229  12.140  < 2e-16 ***\n## y_l2:I(1 - d) -0.34188    0.10158  -3.366 0.000848 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.9586 on 351 degrees of freedom\n## Multiple R-squared:  0.916,  Adjusted R-squared:  0.9151 \n## F-statistic: 957.5 on 4 and 351 DF,  p-value: < 2.2e-16\nR <- 240\nP <- nrow(dt)-R\n\ndt[,`:=`(rw=as.numeric(NA),ar=as.numeric(NA),tar=as.numeric(NA))]\n\nfor(i in 1:P){\n  \n  ar <- lm(y~y_l1+y_l2,data=dt[i:(R-1+i)])\n  \n  tar <- lm(y~(y_l1+y_l2):I(d)+(y_l1+y_l2):I(1-d),data=dt[i:(R-1+i)])\n  \n  dt$rw[R+i] <- dt$y[R-1+i]\n  \n  dt$ar[R+i] <- ar$coefficients[1]+ar$coefficients[2]*dt$y[R-1+i]+ar$coefficients[3]*dt$y[R-2+i]\n  \n  if(dt$y[R-1+i]>=0){\n    dt$tar[R+i] <- tar$coefficients[1]+tar$coefficients[2]*dt$y[R-1+i]+tar$coefficients[3]*dt$y[R-2+i]\n  }else{\n    dt$tar[R+i] <- tar$coefficients[1]+tar$coefficients[4]*dt$y[R-1+i]+tar$coefficients[5]*dt$y[R-2+i]\n  }\n  \n}\n\ndt[,`:=`(rw_e=y-rw,ar_e=y-ar,tar_e=y-tar)]\n\nrmsfe_rw <- sqrt(mean(dt$rw_e^2,na.rm=T))\nrmsfe_ar <- sqrt(mean(dt$ar_e^2,na.rm=T))\nrmsfe_tar <- sqrt(mean(dt$tar_e^2,na.rm=T))\n\nrmsfe_rw## [1] 0.9258102\nrmsfe_ar## [1] 0.9811523\nrmsfe_tar## [1] 0.9996084\ndt[,`:=`(tar_skeleton=y)]\n\ntar <- lm(y~(y_l1+y_l2):I(d)+(y_l1+y_l2):I(1-d),data=dt[1:R])\n\nfor(i in 1:P){\n  \n  if(dt$tar_skeleton[R-1+i]>=0){\n    dt$tar_skeleton[R+i] <- tar$coefficients[1]+tar$coefficients[2]*dt$tar_skeleton[R-1+i]+tar$coefficients[3]*dt$tar_skeleton[R-2+i]\n  }else{\n    dt$tar_skeleton[R+i] <- tar$coefficients[1]+tar$coefficients[4]*dt$tar_skeleton[R-1+i]+tar$coefficients[5]*dt$tar_skeleton[R-2+i]\n  }\n  \n}\n\ndt[1:R]$tar_skeleton <- NA\n\n\nB <- 5000 # the number of bootstrap simulations\n\nboot_mat <- replicate(B,dt$y)\n\nfor(b in 1:B){\n  eps <- sample(tar$residuals,P,replace=T)\n  \n  for(i in 1:P){\n    \n    if(boot_mat[R-1+i,b]>=0){\n      boot_mat[R+i,b] <- tar$coefficients[1]+tar$coefficients[2]*boot_mat[R-1+i,b]+tar$coefficients[3]*boot_mat[R-2+i,b]+eps[i]\n    }else{\n      boot_mat[R+i,b] <- tar$coefficients[1]+tar$coefficients[4]*boot_mat[R-1+i,b]+tar$coefficients[5]*boot_mat[R-2+i,b]+eps[i]\n    }\n    \n  }\n  \n}\n\ndt$tar_boot <- rowMeans(boot_mat)\ndt[1:R]$tar_boot <- NA\n\nsub_lg <- melt(dt[,.(date,y,tar_skeleton,tar_boot)],id.vars=\"date\")\n\nggplot(sub_lg,aes(x=date,y=value,color=variable,linetype=variable))+\n  geom_line(size=1,na.rm=T)+\n  scale_color_manual(values=c(\"darkgray\",\"indianred\",\"steelblue\"))+\n  scale_linetype_manual(values=c(1,2,5))+\n  labs(x=\"Year\",y=\"Series\")+\n  theme_classic()+\n  theme(legend.position=\"top\",legend.title=element_blank())"},{"path":"tutorial-7.html","id":"tutorial-7","chapter":"Tutorial 7","heading":"Tutorial 7","text":"tutorial, generate time series, obtain one-step-ahead forecasts competing models using rolling window procedure, perform Diebold-Mariano type regression-based test equal predictive ability competing models. run code, data.table, ggplot2, lmtest, sandwich packages need installed loaded.Let’s generate time series follow AR(2) process following form:\n\\[y_t = 0.2+1.1y_{t-1}-0.3y_{t-2}+\\varepsilon_t\\]\n\\(e_{t} \\sim N(0,\\sigma^2)\\).Generate vector arbitrary dates (e.g., suppose deal monthly series beginning January 2000), store along \\(y\\) data.table, call ‘dt’, plot realized time series using ggplot function.Suppose candidate models AR(1), AR(2), AR(3), want compare forecasts obtained models random walk process. Generate sequence one-step-ahead forecasts using rolling window scheme, first rolling window ranges period 1 period 180. Calculate RMSFE measures canddidate models.autoregressive models generate ‘statistically significantly’ accurate forecasts random walk model? answer question performing regression-based Diebold-Mariano tests. First generate loss differentials; run three separate regressions assess predictive accuracy AR(1), AR(2), AR(3) relative random walk; finally base decision heteroskedasticity autocorrelation consistent standard errors.","code":"\nn <- 240\n\nset.seed(6)\ne <- rnorm(n,0,1)\n\ny <- rep(NA,n)\ny[1] <- 0.2+e[1]\ny[2] <- 0.2+1.1*y[1]+e[2]\nfor(i in 3:n){\n  y[i] <- 0.2+1.1*y[i-1]-0.3*y[i-2]+e[i]\n}\ndate <- seq(as.Date(\"2000-01-01\"),by=\"month\",along.with=y)\n\ndt <- data.table(date,y)\n\nggplot(dt,aes(x=date,y=y))+\n  geom_line(size=1)+\n  labs(x=\"Year\",y=\"Series\")+\n  theme_classic()\ndt[,`:=`(y1=shift(y,1),y2=shift(y,2),y3=shift(y,3))]\n\nR <- 180\nP <- nrow(dt)-R\n\ndt[,`:=`(rw=as.numeric(NA),a1=as.numeric(NA),a2=as.numeric(NA),a3=as.numeric(NA))]\n\nfor(i in 1:P){\n  \n  dt$rw[R+i] <- dt$y[R+i-1]\n  \n  ar1 <- lm(y~y1,data=dt[i:(R+i-1)])\n  ar2 <- lm(y~y1+y2,data=dt[i:(R+i-1)])\n  ar3 <- lm(y~y1+y2+y3,data=dt[i:(R+i-1)])\n  \n  dt$a1[R+i] <- ar1$coefficients%*%as.numeric(c(1,dt[R+i,c(\"y1\")]))\n  dt$a2[R+i] <- ar2$coefficients%*%as.numeric(c(1,dt[R+i,c(\"y1\",\"y2\")]))\n  dt$a3[R+i] <- ar3$coefficients%*%as.numeric(c(1,dt[R+i,c(\"y1\",\"y2\",\"y3\")]))\n  \n}\n\ndt$rw_e <- dt$y-dt$rw\ndt$a1_e <- dt$y-dt$a1\ndt$a2_e <- dt$y-dt$a2\ndt$a3_e <- dt$y-dt$a3\n\n# RMSFEs\nsqrt(mean(dt$rw_e^2,na.rm=T))## [1] 0.9653331\nsqrt(mean(dt$a1_e^2,na.rm=T))## [1] 0.9053279\nsqrt(mean(dt$a2_e^2,na.rm=T))## [1] 0.8842908\nsqrt(mean(dt$a3_e^2,na.rm=T))## [1] 0.8877883\ndt$ld1 <- dt$rw_e^2-dt$a1_e^2\ndt$ld2 <- dt$rw_e^2-dt$a2_e^2\ndt$ld3 <- dt$rw_e^2-dt$a3_e^2\n\nreg.ld1 <- lm(ld1~1,data=dt)\nreg.ld2 <- lm(ld2~1,data=dt)\nreg.ld3 <- lm(ld3~1,data=dt)\n\ncoeftest(reg.ld1,vcov.=vcovHAC(reg.ld1))## \n## t test of coefficients:\n## \n##             Estimate Std. Error t value Pr(>|t|)  \n## (Intercept) 0.112249   0.047525  2.3619   0.0215 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\ncoeftest(reg.ld2,vcov.=vcovHAC(reg.ld2))## \n## t test of coefficients:\n## \n##             Estimate Std. Error t value Pr(>|t|)  \n## (Intercept) 0.149898   0.083086  1.8041  0.07632 .\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\ncoeftest(reg.ld3,vcov.=vcovHAC(reg.ld3))## \n## t test of coefficients:\n## \n##             Estimate Std. Error t value Pr(>|t|)  \n## (Intercept) 0.143700   0.081309  1.7673  0.08235 .\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"tutorial-8.html","id":"tutorial-8","chapter":"Tutorial 8","heading":"Tutorial 8","text":"tutorial, generate time series, obtain one-step-ahead forecasts set models using rolling window procedure, combine forecasts assess accuracy combined forecast. run code, data.table, ggplot2, lmtest, sandwich packages need installed loaded.Let’s generate time series follow AR(2) process quadratic trend component follows:\n\\[y_t = 0.03t-0.0001t^2+0.6y_{t-1}+0.2y_{t-2}+\\varepsilon_t\\]\n\\(e_{t} \\sim N(0,\\sigma^2)\\).Generate vector arbitrary dates (e.g., suppose deal monthly series beginning January 2000), store along \\(y\\) data.table, call ‘dt’, plot realized time series using ggplot function.Suppose candidate models AR(1), AR(2), linear trend model, want compare forecasts obtained models random walk process. Generate sequence one-step-ahead forecasts using rolling window scheme, first rolling window ranges period 1 period 180.either considered models ‘sttaistically significantly’ outperform random walk?models , average, generate accurate forecasts random walk. AR(2) generates statistically significantly accurate forecasts, based Diebold-Mariano test applied quadratic loss function.Might model contain useful information improving forecast accuracy? Let’s combine forecasts AR(1) linear trend model using equal weights scheme assess combined forecast.","code":"\nn <- 240\n\nset.seed(4)\ne <- rnorm(n,0,1)\n\ny <- rep(NA,n)\ny[1] <- 0.03*1-0.0001*(1^2)+e[1]\ny[2] <- 0.03*2-0.0001*(2^2)+0.6*y[1]+e[2]\nfor(i in 3:n){\n  y[i] <- 0.03*i-0.0001*(i^2)+0.6*y[i-1]+0.2*y[i-2]+e[i]\n}\ndate <- seq(as.Date(\"2000-01-01\"),by=\"month\",along.with=y)\n\ndt <- data.table(date,y)\n\nggplot(dt,aes(x=date,y=y))+\n  geom_line(size=1)+\n  labs(x=\"Year\",y=\"Series\")+\n  theme_classic()\ndt[,`:=`(y1=shift(y,1),y2=shift(y,2),y3=shift(y,3),trend=c(1:nrow(dt)))]\n\nR <- 180\nP <- nrow(dt)-R\n\ndt[,`:=`(rw=as.numeric(NA),a1=as.numeric(NA),a2=as.numeric(NA),tr=as.numeric(NA))]\n\nfor(i in 1:P){\n  \n  dt$rw[R+i] <- dt$y[R+i-1]\n  \n  a1 <- lm(y~y1,data=dt[i:(R+i-1)])\n  a2 <- lm(y~y1+y2,data=dt[i:(R+i-1)])\n  tr <- lm(y~y1+y2+trend,data=dt[i:(R+i-1)])\n  \n  dt$a1[R+i] <- a1$coefficients%*%as.numeric(c(1,dt[R+i,c(\"y1\")]))\n  dt$a2[R+i] <- a2$coefficients%*%as.numeric(c(1,dt[R+i,c(\"y1\",\"y2\")]))\n  dt$tr[R+i] <- tr$coefficients%*%as.numeric(c(1,dt[R+i,c(\"y1\",\"y2\",\"trend\")]))\n  \n}\ndt$rw_e <- dt$y-dt$rw\ndt$a1_e <- dt$y-dt$a1\ndt$a2_e <- dt$y-dt$a2\ndt$tr_e <- dt$y-dt$tr\n\ndt$ld1 <- dt$rw_e^2-dt$a1_e^2\ndt$ld2 <- dt$rw_e^2-dt$a2_e^2\ndt$ldt <- dt$rw_e^2-dt$tr_e^2\n\nreg.ld1 <- lm(ld1~1,data=dt)\nreg.ld2 <- lm(ld2~1,data=dt)\nreg.ldt <- lm(ldt~1,data=dt)\n\ncoeftest(reg.ld1,vcov.=vcovHAC(reg.ld1))## \n## t test of coefficients:\n## \n##             Estimate Std. Error t value Pr(>|t|)  \n## (Intercept) 0.081204   0.048040  1.6903  0.09624 .\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\ncoeftest(reg.ld2,vcov.=vcovHAC(reg.ld2))## \n## t test of coefficients:\n## \n##             Estimate Std. Error t value Pr(>|t|)  \n## (Intercept)  0.32634    0.14912  2.1883  0.03262 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\ncoeftest(reg.ldt,vcov.=vcovHAC(reg.ldt))## \n## t test of coefficients:\n## \n##             Estimate Std. Error t value Pr(>|t|)\n## (Intercept)  0.24461    0.14943  1.6369    0.107\ndt$t1 <- dt$a1*.5+dt$tr*.5\ndt$t1_e <- dt$y-dt$t1\n\ndt$ldt1 <- dt$rw_e^2-dt$t1_e^2\n\nreg.ldt1 <- lm(ldt1~1,data=dt)\n\ncoeftest(reg.ldt1,vcov.=vcovHAC(reg.ldt1))## \n## t test of coefficients:\n## \n##             Estimate Std. Error t value Pr(>|t|)  \n## (Intercept) 0.232926   0.098544  2.3637  0.02141 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"tutorial-9.html","id":"tutorial-9","chapter":"Tutorial 9","heading":"Tutorial 9","text":"tutorial, generate time series, obtain multi-step-ahead forecasts using direct iterated methods autoregressive models using rolling window procedure, assess accuracy forecast. turn interval forecasts, assess coverage accuracy. run code, data.table, ggplot2, lmtest, sandwich packages need installed loaded.Let’s generate time series follow AR(2) process follows:\n\\[y_t = 0.01t+0.6y_{t-1}+0.2y_{t-2}+\\varepsilon_t\\]\n\\(e_{t} \\sim N(0,1)\\).Generate vector arbitrary dates (e.g., monthly series beginning January 2000), store along \\(y\\) data.table, call ‘dt’, plot realized time series using ggplot function.Suppose believe series follow either AR(1) AR(2) process, want compare iterated multi-step forecasts obtained models direct multi-step forecasts. follows, generate twelve-step-ahead point forecasts two methods considered two models using rolling window scheme, first rolling window ranges period 1 period 180. obtaining iterated multi-step forecasts, rolling window, generate sequence one--twelve-step-ahead forecasts, retain twelve-step-ahead forecasts.can now test whether forecasts iterated method ‘statistically significantly’ outperform direct method?Obtain interval forecasts horizon 12 using direct method AR(1) model linear trend model; plot interval forecasts along observed series.Test unconditional coverage interval forecasts considered two models.","code":"\nn <- 240\n\nset.seed(9)\ne <- rnorm(n,0,1)\n\ny <- rep(NA,n)\ny[1] <- 0.01+e[1]\ny[2] <- 0.02+0.6*y[1]+e[2]\nfor(i in 3:n){\n  y[i] <- 0.01*i+0.6*y[i-1]+0.2*y[i-2]+e[i]\n}\ndate <- seq(as.Date(\"2000-01-01\"),by=\"month\",along.with=y)\n\ndt <- data.table(date,y)\n\nggplot(dt,aes(x=date,y=y))+\n  geom_line(size=1)+\n  labs(x=\"Year\",y=\"Series\")+\n  theme_classic()\nh <- 12\n\ndt <- dt[,.(date,y,y1=shift(y,1),y2=shift(y,2),y12=shift(y,h),y13=shift(y,h+1))]\ndt <- dt[complete.cases(dt)]\n\ndt[,`:=`(a1i=as.numeric(NA),a1d=as.numeric(NA),a2i=as.numeric(NA),a2d=as.numeric(NA))]\n\nR <- 180\nP <- nrow(dt)-R-h+1\n\nfor(i in 1:P){\n  \n  ### iterated multi-step method\n  a1i <- lm(y~y1,data=dt[i:(R+i-1)])\n  a2i <- lm(y~y1+y2,data=dt[i:(R+i-1)])\n  \n  iter_dt <- data.table(hor=1:h,a1=as.numeric(NA),a2=as.numeric(NA))\n  \n  ## AR(1)\n  iter_dt$a1[1] <- a1i$coefficients[1]+a1i$coefficients[2]*dt[R+i-1]$y\n  for(j in 2:h){\n    iter_dt$a1[j] <- a1i$coefficients[1]+a1i$coefficients[2]*iter_dt$a1[j-1]\n  }\n  \n  ## AR(2)\n  iter_dt$a2[1] <- a2i$coefficients[1]+a2i$coefficients[2]*dt[R+i-1]$y+a2i$coefficients[3]*dt[R+i-1]$y1\n  iter_dt$a2[2] <- a2i$coefficients[1]+a2i$coefficients[2]*iter_dt$a2[1]+a2i$coefficients[3]*dt[R+i-1]$y\n  for(j in 3:h){\n    iter_dt$a2[j] <- a2i$coefficients[1]+a2i$coefficients[2]*iter_dt$a2[j-1]+a2i$coefficients[3]*iter_dt$a2[j-2]\n  }\n\n  dt$a1i[R+i+h-1] <- iter_dt$a1[h]\n  dt$a2i[R+i+h-1] <- iter_dt$a2[h]\n  \n  ### direct multi-step method\n  a1d <- lm(y~y12,data=dt[i:(R+i-1)])\n  a2d <- lm(y~y12+y13,data=dt[i:(R+i-1)])\n  \n  dt$a1d[R+i+h-1] <- a1d$coefficients[1]+a1d$coefficients[2]*dt[R+i-1]$y\n  dt$a2d[R+i+h-1] <- a2d$coefficients[1]+a2d$coefficients[2]*dt[R+i-1]$y+a2d$coefficients[3]*dt[R+i-1]$y1\n  \n}\ndt$a1i_e <- dt$y-dt$a1i\ndt$a2i_e <- dt$y-dt$a2i\ndt$a1d_e <- dt$y-dt$a1d\ndt$a2d_e <- dt$y-dt$a2d\n\ndt$ld1 <- dt$a1d_e^2-dt$a1i_e^2\ndt$ld2 <- dt$a2d_e^2-dt$a2i_e^2\n\nreg_ld1 <- lm(ld1~1,data=dt)\nreg_ld2 <- lm(ld2~1,data=dt)\n\ncoeftest(reg_ld1,vcov.=vcovHAC(reg_ld1))## \n## t test of coefficients:\n## \n##             Estimate Std. Error t value  Pr(>|t|)    \n## (Intercept)  -6.9266     1.7170 -4.0342 0.0002828 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\ncoeftest(reg_ld2,vcov.=vcovHAC(reg_ld2))## \n## t test of coefficients:\n## \n##             Estimate Std. Error t value Pr(>|t|)  \n## (Intercept)  -3.1094     1.3271  -2.343  0.02494 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\ndt$trend <- c(1:nrow(dt))\ndt[,`:=`(a1dl=as.numeric(NA),a1du=as.numeric(NA),trdl=as.numeric(NA),trdu=as.numeric(NA))]\n\nfor(i in 1:P){\n  \n  a1d <- lm(y~y12,data=dt[i:(R+i-1)])\n  trd <- lm(y~trend,data=dt[i:(R+i-1)])\n  \n  a1d_pf <- a1d$coefficients[1]+a1d$coefficients[2]*dt[R+i-1]$y\n  trd_pf <- trd$coefficients[1]+trd$coefficients[2]*dt[R+i-1+h]$trend\n  \n  a1d_sd <- summary(a1d)$sigma\n  trd_sd <- summary(trd)$sigma\n  \n  dt$a1dl[R+i+h-1] <- a1d_pf-1.96*a1d_sd\n  dt$a1du[R+i+h-1] <- a1d_pf+1.96*a1d_sd\n  \n  dt$trdl[R+i+h-1] <- trd_pf-1.96*trd_sd\n  dt$trdu[R+i+h-1] <- trd_pf+1.96*trd_sd\n}\n\nggplot(dt,aes(x=date,y=y))+\n  geom_line(size=1,color=\"black\")+\n  geom_line(aes(y=a1dl),size=1,color=\"steelblue\",linetype=5,na.rm=T)+\n  geom_line(aes(y=a1du),size=1,color=\"steelblue\",linetype=5,na.rm=T)+\n  geom_line(aes(y=trdl),size=1,color=\"indianred\",linetype=5,na.rm=T)+\n  geom_line(aes(y=trdu),size=1,color=\"indianred\",linetype=5,na.rm=T)+\n  theme_classic()\ndt[,`:=`(i1=ifelse(y>=a1dl & y<=a1du,1,0),it=ifelse(y>=trdl & y<=trdu,1,0))]\n\n\n# AR(1)\np1 <- mean(dt$i1,na.rm=T)\n\nL10 <- (1-.95)^(P*(1-p1))*(.95)^(P*p1)\nL11 <- (1-p1)^(P*(1-p1))*(p1)^(P*p1)\n\nLR1 <- -2*log(L10/L11)\n\npchisq(LR1,df=1,lower.tail=F)## [1] 0.01027851\n# AR(2)\np2 <- mean(dt$it,na.rm=T)\n\nL20 <- (1-.95)^(P*(1-p2))*(.95)^(P*p2)\nL21 <- (1-p2)^(P*(1-p2))*(p2)^(P*p2)\n\nLR2 <- -2*log(L20/L21)\n\npchisq(LR2,df=1,lower.tail=F)## [1] 0.1441849"}]
