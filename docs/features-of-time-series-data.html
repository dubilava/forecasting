<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Chapter 2 – Features of Time Series Data | Educated Guess" />
<meta property="og:type" content="book" />
<meta property="og:image" content="/forecasting-logo.png" />
<meta property="og:description" content="Forecasting With Time Series Models Using R" />


<meta name="author" content="David Ubilava" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Forecasting With Time Series Models Using R">

<title>Chapter 2 – Features of Time Series Data | Educated Guess</title>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<link href="libs/msmb-css-0/msmb.css" rel="stylesheet" />
<script>
function toggle_visibility(id1, id2) {
var e = document.getElementById(id1);
var f = document.getElementById(id2);

e.style.display = ((e.style.display!='none') ? 'none' : 'block');

if(f.classList.contains('fa-plus-square')) {
    f.classList.add('fa-minus-square')
    f.classList.remove('fa-plus-square')
} else {
    f.classList.add('fa-plus-square')
    f.classList.remove('fa-minus-square')
}

}
</script>
<script>
function copy_link(id) {
  var dummy = document.createElement('input'),
  text = window.location.href.split(/[?#]/)[0] + '#' + id;
  document.body.appendChild(dummy);
  dummy.value = text;
  dummy.select();
  document.execCommand('copy');
  document.body.removeChild(dummy);
  
  var tooltip = document.getElementById(id + '-tooltip');
  tooltip.innerHTML = 'Copied!';
}

function reset_tooltip(id) {
  var tooltip = document.getElementById(id);
  tooltip.innerHTML = 'Copy link';
}
</script>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
<script>
  $(document).ready(function () {
    var element_label= $("label[for *= 'tufte-sn-']");
    var count = $(element_label).length;
    $(element_label).each(function( index ) {
      //console.log( ++index + ": " + $( this ).text() );
      $(this).attr('for','tufte-sn-'+ ++index);
      $(this).text(index);
    });
  });
  $(document).ready(function () {
    var element_input= $("input[id *= 'tufte-sn-']");
    var count = $(element_input).length;
    $(element_input).each(function( index ) {
      //console.log( ++index + ": " + $( this ).text() );
      $(this).attr('id','tufte-sn-'+ ++index);
    }); 
  });
  $(document).ready(function () {
    var element_span= $("span[class *= 'sidenote-number']");
    var count = $(element_span).length;
    $(element_span).each(function( index ) {
      //console.log( ++index + ": " + $( this ).text() );
      $(this).text(++index);
    }); 
  });
</script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }

code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul class="navbar">
<li class="msmb"><p class="title">Educated Guess<p><p class="author">David Ubilava</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="index.html" id="toc-an-intuitive-guide-to-forecasting-with-time-series-models-using-r">An Intuitive Guide to Forecasting With Time Series Models Using R</a>
<a href="forecasting-with-time-series-models.html" id="toc-forecasting-with-time-series-models">Forecasting With Time Series Models</a>
<a href="introduction-to-forecasting.html" id="toc-introduction-to-forecasting"><span class="toc-section-number">1</span> – Introduction to Forecasting</a>
<a id="active-page" href="features-of-time-series-data.html" id="toc-features-of-time-series-data"><span class="toc-section-number">2</span> – Features of Time Series Data</a><ul class="toc-sections">
<li class="toc"><a href="#stochastic-process-and-time-series"> Stochastic Process and Time Series</a></li>
<li class="toc"><a href="#stationarity"> Stationarity</a></li>
<li class="toc"><a href="#serial-dependence"> Serial Dependence</a></li>
<li class="toc"><a href="#transformations"> Transformations</a></li>
</ul>
<a href="forecasting-methods-and-routines.html" id="toc-forecasting-methods-and-routines"><span class="toc-section-number">3</span> – Forecasting Methods and Routines</a>
<a href="trends.html" id="toc-trends"><span class="toc-section-number">4</span> – Trends</a>
<a href="seasonality.html" id="toc-seasonality"><span class="toc-section-number">5</span> – Seasonality</a>
<a href="autoregression.html" id="toc-autoregression"><span class="toc-section-number">6</span> – Autoregression</a>
<a href="vector-autoregression.html" id="toc-vector-autoregression"><span class="toc-section-number">7</span> – Vector Autoregression</a>
<a href="threshold-autoregression.html" id="toc-threshold-autoregression"><span class="toc-section-number">8</span> – Threshold Autoregression</a>
<a href="forecast-evaluation.html" id="toc-forecast-evaluation"><span class="toc-section-number">9</span> – Forecast Evaluation</a>
<a href="forecast-combination.html" id="toc-forecast-combination"><span class="toc-section-number">10</span> – Forecast Combination</a>
<a href="forecasting-using-r.html" id="toc-forecasting-using-r">Forecasting Using R</a>
<a href="tutorial-1-introduction-to-r.html" id="toc-tutorial-1-introduction-to-r">Tutorial 1: Introduction to R</a>
<a href="tutorial-2-data-management-and-visualisation.html" id="toc-tutorial-2-data-management-and-visualisation">Tutorial 2: Data Management and Visualisation</a>
<a href="tutorial-3-forecasting-methods-and-routines.html" id="toc-tutorial-3-forecasting-methods-and-routines">Tutorial 3: Forecasting Methods and Routines</a>
<a href="tutorial-4-trends.html" id="toc-tutorial-4-trends">Tutorial 4: Trends</a>
<a href="tutorial-5-seasonality.html" id="toc-tutorial-5-seasonality">Tutorial 5: Seasonality</a>
<a href="tutorial-6-autoregression.html" id="toc-tutorial-6-autoregression">Tutorial 6: Autoregression</a>
<a href="tutorial-8-vector-autoregression.html" id="toc-tutorial-8-vector-autoregression">Tutorial 8: Vector Autoregression</a>
<a href="tutorial-9-threshold-autoregression.html" id="toc-tutorial-9-threshold-autoregression">Tutorial 9: Threshold Autoregression</a>
<a href="tutorial-10-forecast-evaluation.html" id="toc-tutorial-10-forecast-evaluation">Tutorial 10: Forecast Evaluation</a>
<a href="tutorial-11-forecast-combination.html" id="toc-tutorial-11-forecast-combination">Tutorial 11: Forecast Combination</a>
<a href="references.html" id="toc-references">References</a>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="features-of-time-series-data" class="section level1" number="2">
<h1>
<span class="header-section-number">Chapter 2</span> – Features of Time Series Data</h1>
<p><img src="art/timeseries.png"></p>
<div id="stochastic-process-and-time-series" class="section level2" number="2.1">
<h2>
<span class="header-section-number">2.1</span> Stochastic Process and Time Series<div class="tooltip"><button class="internal-link-btn" onclick="copy_link('stochastic-process-and-time-series')" onmouseout="reset_tooltip('stochastic-process-and-time-series-tooltip')"><span class="tooltiptext" id="stochastic-process-and-time-series-tooltip">Copy link</span><i class="fa fa-link"></i></button></div>
</h2>
<p><span class="newthought">A time series</span> is an observed sequence of realizations of chronologically stored random variables. The sequence of random variables indexed by time, <span class="math inline">\(\{\ldots,Y_{-1},Y_0,Y_1,\ldots,Y_T,Y_{T+1}\ldots\}\)</span>, is referred to as the <em>stochastic process</em>. Thus, a time series is a realization of a stochastic process.</p>
<p>We can think of a time series as a finite sample from an underlying doubly–infinite sequence: <span class="math inline">\(\{\ldots,y_{-1},y_{0},y_1,y_2,\ldots,y_T,y_{T+1},y_{T+2},\ldots\}\)</span>. This is to say that the history extends beyond the starting and ending time periods of the sample at hand.</p>
<p>But we don’t observe, or choose not to observe, those time periods. Instead, we observe and work with the sample of time series <span class="math inline">\(\{y_1,y_2,\ldots,y_T\}\)</span>, which we can also denote as <span class="math inline">\(\{y_t\}_{t=1}^{T}\)</span> or simply <span class="math inline">\(\{y_t\}\)</span>. Similarly, we denote the stochastic process over the observed time periods as <span class="math inline">\(\{Y_t\}_{t=1}^{T}\)</span> or <span class="math inline">\(\{Y_t\}\)</span>.</p>
</div>
<div id="stationarity" class="section level2" number="2.2">
<h2>
<span class="header-section-number">2.2</span> Stationarity<div class="tooltip"><button class="internal-link-btn" onclick="copy_link('stationarity')" onmouseout="reset_tooltip('stationarity-tooltip')"><span class="tooltiptext" id="stationarity-tooltip">Copy link</span><i class="fa fa-link"></i></button></div>
</h2>
<p>If all random variables, from where the time series are drawn, have the same distribution, then we refer to such data as <em>stationary</em> time series. Stationarity is an important feature, and the assumption on which time series econometrics heavily relies.</p>
<p>Consider a simplest kind of a time series comprised of realizations from independent and identically distributed (iid) normal random variable with zero mean and constant variance: <span class="math inline">\(Y_t \sim iid~\text{N}\left(0,\sigma^2\right)\)</span>. The following graph plots the realized time series from this process:</p>
<div class="figure">
<span style="display:block;" id="fig:white-noise"></span>
<p class="caption marginnote shownote">
Figure 2.1: White noise process
</p>
<img src="forecasting_files/figure-html/white-noise-1.png" alt="White noise process" width="600">
</div>
<p>Such time series is a realization of what is referred to as a <em>white noise</em> process.<label for="tufte-sn-4" class="margin-toggle sidenote-number">4</label><input type="checkbox" id="tufte-sn-4" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">4</span> Technically, this is a <em>Gaussian</em> white noise process as the random variables are iid normally distributed. Without the normality assumption we would have an <em>Independent</em> white noise process. Without the indepence assumption we would have simply a white noise process.</span> That is, <span class="math inline">\(\{Y_t\}\)</span>, is a white noise process if:
<span class="math display">\[\begin{align*}
&amp; E(Y_t) = 0,\;~\forall~t\\
&amp; Var(Y_t) = \sigma^2,\;~\forall~t\\
&amp; Cov(Y_t,Y_{t-k}) = 0,\;~\forall~k \ne 0
\end{align*}\]</span></p>
<p>Because each observation is drawn from the same distribution, white noise is a stationary process. Indeed, it is a special type of the stationary process insofar as its mean, variance, and covariance are time-invariant. Note, for stationarity, neither the mean nor the covariances are required to be equal to zero. Thus, <span class="math inline">\(\{Y_t\}\)</span> is a stationary process<label for="tufte-sn-5" class="margin-toggle sidenote-number">5</label><input type="checkbox" id="tufte-sn-5" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">5</span> To be precise, this is a definition of covariance-stationarity or weak form of stationarity. Strict stationarity is defined by time invariant joint distribution of random variables.</span> if its mean and variance are independent of <span class="math inline">\(t\)</span>, and the autocovariances are independent of <span class="math inline">\(t\)</span> for all <span class="math inline">\(k\)</span>.</p>
<p>Why should we care about stationarity? The gist of the matter is that in each time period we only observe one realization of the respective random variable. We don’t have the sample of observations for that period—just a single observation is all we have. Of course, over time, we have many such observations—one for each period. The question then is, could we use this sample of time series and conclude something about the moments of the stochastic process? Stationarity in conjunction with <em>ergodicity</em> enables us to do just this.</p>
<p>Ergodicity implies independence of two random variables that are far apart from each other in the stochastic process. That is, if <span class="math inline">\(\{Y_t\}\)</span> is stationary and ergodic with <span class="math inline">\(E(Y_t)=\mu\)</span>, then <span class="math inline">\(Cov(Y_t,Y_{t-k}) = 0\)</span> for some large integer <span class="math inline">\(k\)</span>. More importantly, when the process is stationary and ergodic, the mean of the sample of time series converges to the mean of the stochastc process as the sample size increases.</p>
</div>
<div id="serial-dependence" class="section level2" number="2.3">
<h2>
<span class="header-section-number">2.3</span> Serial Dependence<div class="tooltip"><button class="internal-link-btn" onclick="copy_link('serial-dependence')" onmouseout="reset_tooltip('serial-dependence-tooltip')"><span class="tooltiptext" id="serial-dependence-tooltip">Copy link</span><i class="fa fa-link"></i></button></div>
</h2>
<p>It is more of the norm rather than the exception for a time series to be correlated over time. Indeed, because of the sequential nature of the stochastic process, we commonly observe dependence among the temporally adjacent random variables. That is, we expect <span class="math inline">\(Y_t\)</span> and <span class="math inline">\(Y_{t-k}\)</span> to be correlated for a reasonably small integer <span class="math inline">\(k\)</span>. Such correlation, referred to as the <em>autocorrelation</em>, or more specifically, the <span class="math inline">\(k^{th}\)</span> order autocorrelation is given by: <span class="math display">\[\rho_k=Cor(Y_{t},Y_{t-k}) = \frac{Cov(Y_{t},Y_{t-k})}{\sqrt{Var(Y_{t})}\sqrt{Var(Y_{t-k})}},\;~~k=1,2,\ldots\]</span></p>
<p>Under the assumption of stationarity, <span class="math inline">\(Var(Y_{t})=Var(Y_{t-k})\)</span>, so the autocorrelation can be simplified to: <span class="math inline">\(\rho_k= \frac{Cov(Y_{t},Y_{t-k})}{Var(Y_{t})}.\)</span></p>
<p>Autocorrelations are commonly illustrated via the so-called <em>autocorrelogram</em>, which plots the sequence of autocorrelation coefficients against the lags at which these coefficients are obtained. The following figure illustrates an autocorrelogram for a sample of time series with 240 observations:</p>
<div class="figure">
<span style="display:block;" id="fig:acf"></span>
<p class="caption marginnote shownote">
Figure 2.2: Autocorrelation
</p>
<img src="forecasting_files/figure-html/acf-1.png" alt="Autocorrelation" width="600">
</div>
<p>For each lag, <span class="math inline">\(k\)</span>, the vertical line extending from zero represents an estimate of the autocorrelation coefficient at that lag. The dashed lines denote the 95% confidence interval, given by <span class="math inline">\(\pm 1.96/\sqrt{T}\)</span>, where <span class="math inline">\(T\)</span> is the size of the time series sample.</p>
<p>Another relevant measure of the time series dependence is <em>partial</em> autocorrelation, which is correlation between <span class="math inline">\(Y_t\)</span> and <span class="math inline">\(Y_{t-k}\)</span> net of any correlations between <span class="math inline">\(Y_t\)</span> and <span class="math inline">\(Y_{t-k+j}\)</span>, for all <span class="math inline">\(j=1,\ldots,k-1\)</span>. Similar to autocorrelations, partial autocorrelations, here denoted by <span class="math inline">\(\pi\)</span>, can also be illustrated using autocorrelograms:</p>
<div class="figure">
<span style="display:block;" id="fig:pacf"></span>
<p class="caption marginnote shownote">
Figure 2.3: Partial Autocorrelation
</p>
<img src="forecasting_files/figure-html/pacf-1.png" alt="Partial Autocorrelation" width="600">
</div>
<p>Autocorrelogram is a useful tool for detecting dynamic properties of a time series. For example, autocorrelations of the stationary and ergodic stochastic process dissipate to zero as <span class="math inline">\(k\)</span> increases. Thus, by observing autocorrelograms, in conjunction with a visual inspection of the time series, we can get an idea whether the process is stationary.<label for="tufte-sn-6" class="margin-toggle sidenote-number">6</label><input type="checkbox" id="tufte-sn-6" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">6</span> There are, of course, formal tests of stationarity, which we will discuss in one of the subsequent chapters.</span> If a time series appear nonstationary, we can make changes to it, so that the transformed series becomes stationary.</p>
</div>
<div id="transformations" class="section level2" number="2.4">
<h2>
<span class="header-section-number">2.4</span> Transformations<div class="tooltip"><button class="internal-link-btn" onclick="copy_link('transformations')" onmouseout="reset_tooltip('transformations-tooltip')"><span class="tooltiptext" id="transformations-tooltip">Copy link</span><i class="fa fa-link"></i></button></div>
</h2>
<p>It is common to transform time series by taking logarithms (if applicable), by first-differencing the series, or by first-differencing the logarithms of the series. Such transformations are done to work with a suitable variable for the desired econometric analysis, or to address some underlying issues of the series (e.g., nonstationarity).</p>
<p>Consider the Dow Jones Industrial Average (commonly referred to as the Dow Jones Index), for example. In levels, the series are clearly nonstationary. This is because the series are trending upward (on average), and also because the series become more volatile over time.</p>
<div class="figure">
<span style="display:block;" id="fig:dji"></span>
<p class="caption marginnote shownote">
Figure 2.4: Dow Jones Industrial Average
</p>
<img src="forecasting_files/figure-html/dji-1.png" alt="Dow Jones Industrial Average" width="600">
</div>
<p>Could first-differencing resolve the issue of nonstationarity? Turns out, not exactly. The first-differenced Dow Jones Index, while no longer trending, reveals increasing volatility over time.</p>
<div class="figure">
<span style="display:block;" id="fig:ddji"></span>
<p class="caption marginnote shownote">
Figure 2.5: Dow Jones Industrial Average: Differences
</p>
<img src="forecasting_files/figure-html/ddji-1.png" alt="Dow Jones Industrial Average: Differences" width="600">
</div>
<p>As it turns out, we can largely resolve the issue if we first-difference the log-transformed series. When we log-transform the series, the difference in observations from two different period is no longer measured in absolute terms, rather it is measured in relative terms. Then, by first-differencing the log-transformed series we, in effect, obtain a measure of a growth rate of the original series. The growth rate of the Dow Jones Index appears to be stationary.</p>
<div class="figure">
<span style="display:block;" id="fig:dlndji"></span>
<p class="caption marginnote shownote">
Figure 2.6: Dow Jones Industrial Average: Log-Differences
</p>
<img src="forecasting_files/figure-html/dlndji-1.png" alt="Dow Jones Industrial Average: Log-Differences" width="600">
</div>

</div>
</div>
<p style="text-align: center;">
<a href="introduction-to-forecasting.html"><button class="btn btn-default">Previous</button></a>
<a href="forecasting-methods-and-routines.html"><button class="btn btn-default">Next</button></a>
</p>
<p class="build-date">Page built: 
2022-11-13
 using 
R version 4.2.1 (2022-06-23 ucrt)
</p>
</div>
</div>



</body>
</html>
