[["index.html", "Educated Guess An Intuitive Guide to Forecasting With Time Series Models Using R ", " Educated Guess An Intuitive Guide to Forecasting With Time Series Models Using R David Ubilava June 2023 ‘The future will be the child of the past and the present, even if a rebellious child.’ This quote belongs to George Crumb—an American composer of classical music. I don’t listen to classical music, and I had not heard of George Crumb until when I searched for a quote by Ilia Chavchavadze—a Georgian public figure of the late 19th and early 20th centuries—who said: ‘The present born from the past is a parent of the future.’ Neither George Crumb nor Ilia Chavchavadze was an expert in forecasting or in time series analysis. But in their quotes, they both described rather eloquently the very essence of forecasting: We can make an educated guess about the future because we observe and take lessons from the past. A diverse set of forecasting methods typically rely on insights from econometric analysis of time series. In time series analysis, the implied assumption is that the past tends to repeat itself, at least to some extent. So, if we well study the past, we may be able to forecast an event with some degree of accuracy. This book is about econometric models that help us learn the past to predict the future. It aims to serve as a primer for econometric time series analysis and forecasting. The book consists of two parts. The first part introduces econometric models and methods for forecasting. This part helps develop intuition behind time series modeling and forecasting. The second part presents tutorials that complement the material from the first part. The tutorials help with hands-on experience in coding in R to forecast time series data. "],["forecasting-with-time-series-models.html", "Forecasting With Time Series Models", " Forecasting With Time Series Models "],["introduction-to-forecasting.html", "Chapter 1 – Introduction to Forecasting 1.1 What Forecast Is and Is Not 1.2 A Brief History of the Study of Forecasting 1.3 Through the Lens of Causal Inference 1.4 Self Fulfilling Prophecy 1.5 Knowing the Unknown", " Chapter 1 – Introduction to Forecasting 1.1 What Forecast Is and Is Not What is a forecast if not a guess? An educated guess, nonetheless. We guess because we have no precise knowledge of how things will turn up in the future—near or distant. We may guess today whether it will rain tomorrow, for example. It may or may not rain tomorrow.1 With each of these two outcomes having some chance of occurring, our claim about whether it will rain is a guess. We guess events that are uncertain. These are events for which several outcomes—each with some chance of appearing—are possible. In other words, we guess random variables. Making a claim about whether it will rain tomorrow is a guess because rain is a random variable. In its simplest form, we can think of a rain as a discrete random variable—a binary random variable, to be precise—it either rains or it does not rain. Or, more accurately, we can think of a rain as a continuous random variable, albeit truncated from below at zero. So, it either does not rain. But if it rains, it may rain a little, or it may rain a lot. By contrast, making a claim about whether the sun will rise tomorrow is hardly a guess. We know with absolute precision when will the sun rise. There is no prophecy about predicting the sunrise. A forecast is a guess, but a guess is not necessarily a forecast. A guess that relies on some knowledge or understanding of the underlying causes of an event is a forecast. When commercial banks, or professional forecasters, guess whether the central bank will increase or decrease the nominal interest rates by some fraction of percentage points, they forecast because they base their guess on their understanding of the markets as well as on their understanding of how the central bank reacts to changes in the markets. Otherwise, an uninformed guess is merely a gamble.2 Thus, making a guess that it will rain tomorrow may be a forecast or a gamble. It is a forecast if we make a claim it will rain tomorrow because we looked out of the window and saw clouds today. Such a guess relies on our understanding of meteorological phenomena. It would be a gamble had we made the very same claim based on a hunch, without looking out of the window. 1.2 A Brief History of the Study of Forecasting The roots of forecasting extend very much to the beginning of human history. In their desire to predict the future, people have attempted to make forecasts of their own or have used the services of others. This desire to guess what was to come, has been necessitated by the potential benefits such information could offer. For many centuries, much of forecasting revolved around weather forecasting. Primarily because weather was the single most important factor that impacted the livelihood of people, and indeed the fate of civilizations,3 through its intrinsic links with agriculture. Early attempts at weather forecasting were rather simplistic. The Babylonians based their weather forecasts on the appearance of clouds. The ancient Egyptians measured the levels of the Nile River’s waters to predict an approaching famine due to droughts or destructive floods. Over time, advancements in physics and related fields, and the invention of measuring instruments such as the barometer and the thermometer, contributed to the development of the study of meteorology, the way we know it. The birth of the modern weather forecast is attributed to the invention of the telegraph, however. The telegraph made it possible for the weather forecast to travel sooner than the weather itself and thus made the weather forecast relevant. Much like a better understanding of the laws of physics enabled meteorological research, the development of the study of econometrics helped develop the methods and practices of economic forecasting. Irving Fisher, one of the most influential economists of all times and the first generation econometrician—indeed, a founding member and the first president of the Econometric Society—was one of the first academic economists who contributed to the study of economic forecasting through his “Equation of Exchange,” which he used as the foundation to forecast prices, albeit with varying success. Other notorious economic forecasters of that age, albeit not as known or celebrated academics as Fisher, were Charles Bullock and Warren Persons, who ran a Harvard-affiliated quasi-academic center for business cycle research with main purpose to generate economic forecasts based on historical precedents. Unlike Fisher’s forecasts that were model-based, Bullock and Persons’ forecasts were more data-driven.4 Much of the success of the early economic forecasters came during the early years of the 20th century. The failure to predict the Great Depression adversely impacted their reputation and, indeed, fortunes. As with the telegraph back in the 19th century, the evolution of the computing power in the second half of the 20th century facilitated the resurrection of economic forecasting, which became increasingly based on the effective use of econometric methods. Toward the end of the 20th century, and particularly from the beginning of the 21st century, the evolution of the Internet and the ease and affordability of computing power allowed the storage and distribution of high-frequency granular data that has further aided the advancement of the study of forecasting. 1.3 Through the Lens of Causal Inference Forecasting relies on some understanding of cause and effect between two variables over two time periods, even if one is not intrinsically aware of such a relationship. Such an understanding was apparent in the Babylonians’ and Egyptians’ methods of weather forecasting. Birds’ behavior, historically, has been the single most reliable predictor of upcoming changes to the weather.5 The most likely reason for birds flying low is the sudden change in atmospheric pressure leading to the storm. Birds are “nature’s barometers” of the sort. We know it now, but our predecessors did not know it back when the barometer was yet to be invented. And yet, it turns out that their basing of the weather forecast on birds’ behavior was not unfounded. Birds do not cause storms; changes to air pressure do. But both the birds’ behavior and the storm are the outcomes of the same cause—atmospheric pressure. As it happens, one outcome (birds’ behavior) precedes the other outcome (the storm). And such temporal ordering of the events—each an outcome of an exogenous shock—helps build a simple forecasting model, which is based on a mere correlation, and yet it works. Consider the following diagram. Figure 1.1: Causality, correlation, and forecasting On this diagram, Pressure denotes air pressure, Birds denotes birds’ behavior, and Storm denotes a storm. The subscripts clarify temporal ordering. The solid lines indicate a causal relationship, and the dashed line indicates a correlation without causality. The gray lines indicate an unobserved effect, and the black line indicates the observed effect. While we do not observe the true causal effects,6 the observed correlation and the temporal ordering of the two outcomes of the same cause helps us build a useful predictive model. This illustration alludes to a notable feature of forecasting. To make a forecast, one does not need a “clean” identification of the causal mechanism. A mere correlation might as well suffice. In some instances, this might even be a preferred setting. Correlation captures information on unobserved variables, which aids in forecasting, whereas causal inference specifically ignores this information. That’s not to say that forecasting does not rely on causal relations. It does, very much so. And everything else held constant, the causal relationship would be preferred to the correlational. In our example, once the scientifically substantiated causal linkage between changes in atmospheric pressure and the occurrence of storms was established, there was little need and hardly any value in the correlational relationship between birds’ behavior and the occurrence of storms. 1.4 Self Fulfilling Prophecy The foregoing illustration paints an uncharacteristically simplistic picture of forecasting. It describes a rather simplified model as it omits several factors, other than air pressure, that could influence birds’ behavior or the formation of a storm. It also illustrates a relatively straightforward forecasting exercise because it involves relations between climate and nature. Such relationship is closer to a hard science than to a soft science. Things become increasingly more complex and much less predictable when people, with their behavioral peculiarities, enter the equation. Economists, who obviously deal with systems that involve or are comprised by people, are notorious for making bad forecasts.7 Often, we fail to forecast because we cannot know what will happen in the future. But sometimes, we fail to forecast because we exactly know what is to come. The issue lies in our ability to influence the future because we could forecast it reasonably well. An extreme case that helps illustrate the point is the concept of efficient markets. Consider a scenario where, through news, we learned that stock prices of a publicly traded company would increase tomorrow. This creates a temporal arbitrage opportunity: we can buy the stock today, when its price is still low, and sell it tomorrow, when its price has increased. If we were the only people who became aware of such an opportunity, the scheme would just work. But the information is available to everyone and at once. Thus, many people will learn about this opportunity and act accordingly: They will all try to buy the shares of the company immediately to sell them for the higher price later. This will create demand, and the share prices will increase today instead of tomorrow. And the temporal arbitrage opportunity will be no more. What we just described is known as the efficient market hypothesis. The efficient market hypothesis suggests that markets adjust immediately and correctly to the relevant new information. As a result, traders cannot take advantage of this new information. The moment one realizes there can be an arbitrage opportunity, it is already too late. So, some things, such as stock prices, are unpredictable but not because we don’t know what will happen to them. Au contraire, it is because we know this all too well. 1.5 Knowing the Unknown Methods of forecasting may differ, but they have one thing in common: they all rely—or at least pretend to rely—upon information. We record and observe information in the form of data. When the data are organized and stored in a certain way—chronologically and at regular intervals—we have a time series. By construction, a time series is historical data. Forecasting involves examining historical data to make an informed guess about what is to come in the near or the distant future. The left-hand side of the graph features a time series of 20 observations, given by \\(x_t\\), where \\(t=1,\\ldots,20\\). The observation in the first period, \\(x_1\\), takes the value of two; the observation in the second period, \\(x_2\\), takes the value of four; and so forth until the observation in the last period, \\(x_{20}\\), which takes the value of five. The question is, given the observed data, what would be our best guess about \\(x_{21}\\), that is, the value of the observation in the subsequent period? In other words, the question asks us to forecast the next observation, given that we have observed the chronological sequence of the previous 20 observations. To answer this question, let’s summarize what we know. The density8 of the observed time series, presented in the form of stacked dots on the right-hand side of the figure, does that. We can see that the observed values range from one to six, and some of them appear more frequently than others. Specifically, three appears most frequently in our observed sample. So, our best guess can be three. But we can also give another answer. We can see that the most recently observed value is five. And if we suspect some temporal dependence in the time series—that is, if we believe there is some positive correlation between back–to–back observations—then our guess may be tilted toward five. So, in this instance, our best guess could be four. The foregoing illustration alludes to an important clarifier of a forecast. A forecast is a bunch of outcomes or values, each with some probability of occurring. If a variable of interest is continuous, then a forecast is a density. More specifically, a forecast is a conditional density, which depends on the available information at the time when the forecast is made, and it combines everything we know or do not know about the unknown.9 We rarely use a density to communicate our forecast, however. When commercial banks or professional forecasters make a guess about the upcoming interest rate hike by the central bank, they usually report a single value. That is a point forecast—the simplest and the most frequently used summary of a forecast. A point forecast is one value that qualifies or quantifies our guess about the future realization of an event, for example, a guess that it will rain tomorrow or a that the central bank will increase interest rates by 0.25 percentage points. In our illustration, the guess that in period 21 we could observe three is a point forecast. We denote point forecast by \\(\\hat{y}_{t+h|t}\\), where \\(h\\) is the forecast horizon.10 In other words, \\(\\hat{y}_{t+h|t}\\) is an \\(h\\)-step-ahead point forecast made in period \\(t\\). In the foregoing example, where we were attempting to guess the realization of the variable in period 21 based on information up to and including period 20, our forecast horizon was one. References "],["features-of-time-series-data.html", "Chapter 2 – Features of Time Series Data 2.1 Stochastic Process and Time Series 2.2 Stationarity 2.3 Serial Dependence 2.4 Transformations", " Chapter 2 – Features of Time Series Data 2.1 Stochastic Process and Time Series A time series is an observed sequence of realizations of chronologically stored random variables. The sequence of random variables indexed by time, \\(\\{\\ldots,Y_{-1},Y_0,Y_1,\\ldots,Y_T,Y_{T+1}\\ldots\\}\\), is referred to as the stochastic process. Thus, a time series is a realization of a stochastic process. We can think of a time series as a finite sample from an underlying doubly–infinite sequence: \\(\\{\\ldots,y_{-1},y_{0},y_1,y_2,\\ldots,y_T,y_{T+1},y_{T+2},\\ldots\\}\\). This is to say that the history extends beyond the starting and ending time periods of the sample at hand. But we don’t observe, or choose not to observe, those time periods. Instead, we observe and work with the sample of time series \\(\\{y_1,y_2,\\ldots,y_T\\}\\), which we can also denote as \\(\\{y_t\\}_{t=1}^{T}\\) or simply \\(\\{y_t\\}\\). Similarly, we denote the stochastic process over the observed time periods as \\(\\{Y_t\\}_{t=1}^{T}\\) or \\(\\{Y_t\\}\\). 2.2 Stationarity If all random variables, from where the time series are drawn, have the same distribution, then we refer to such data as stationary time series. Stationarity is an important feature, and the assumption on which time series econometrics heavily relies. Consider a simplest kind of a time series comprised of realizations from independent and identically distributed (iid) normal random variable with zero mean and constant variance: \\(Y_t \\sim iid~\\text{N}\\left(0,\\sigma^2\\right)\\). The following graph plots the realized time series from this process: Figure 2.1: White noise process Such time series is a realization of what is referred to as a white noise process.11 That is, \\(\\{Y_t\\}\\), is a white noise process if: \\[\\begin{align*} &amp; E(Y_t) = 0,\\;~\\forall~t\\\\ &amp; Var(Y_t) = \\sigma^2,\\;~\\forall~t\\\\ &amp; Cov(Y_t,Y_{t-k}) = 0,\\;~\\forall~k \\ne 0 \\end{align*}\\] Because each observation is drawn from the same distribution, white noise is a stationary process. Indeed, it is a special type of the stationary process insofar as its mean, variance, and covariance are time-invariant. Note, for stationarity, neither the mean nor the covariances are required to be equal to zero. Thus, \\(\\{Y_t\\}\\) is a stationary process12 if its mean and variance are independent of \\(t\\), and the autocovariances are independent of \\(t\\) for all \\(k\\). Why should we care about stationarity? The gist of the matter is that in each time period we only observe one realization of the respective random variable. We don’t have the sample of observations for that period—just a single observation is all we have. Of course, over time, we have many such observations—one for each period. The question then is, could we use this sample of time series and conclude something about the moments of the stochastic process? Stationarity in conjunction with ergodicity enables us to do just this. Ergodicity implies independence of two random variables that are far apart from each other in the stochastic process. That is, if \\(\\{Y_t\\}\\) is stationary and ergodic with \\(E(Y_t)=\\mu\\), then \\(Cov(Y_t,Y_{t-k}) = 0\\) for some large integer \\(k\\). More importantly, when the process is stationary and ergodic, the mean of the sample of time series converges to the mean of the stochastc process as the sample size increases. 2.3 Serial Dependence It is more of the norm rather than the exception for a time series to be correlated over time. Indeed, because of the sequential nature of the stochastic process, we commonly observe dependence among the temporally adjacent random variables. That is, we expect \\(Y_t\\) and \\(Y_{t-k}\\) to be correlated for a reasonably small integer \\(k\\). Such correlation, referred to as the autocorrelation, or more specifically, the \\(k^{th}\\) order autocorrelation is given by: \\[\\rho_k=Cor(Y_{t},Y_{t-k}) = \\frac{Cov(Y_{t},Y_{t-k})}{\\sqrt{Var(Y_{t})}\\sqrt{Var(Y_{t-k})}},\\;~~k=1,2,\\ldots\\] Under the assumption of stationarity, \\(Var(Y_{t})=Var(Y_{t-k})\\), so the autocorrelation can be simplified to: \\(\\rho_k= \\frac{Cov(Y_{t},Y_{t-k})}{Var(Y_{t})}.\\) Autocorrelations are commonly illustrated via the so-called autocorrelogram, which plots the sequence of autocorrelation coefficients against the lags at which these coefficients are obtained. The following figure illustrates an autocorrelogram for a sample of time series with 240 observations: Figure 2.2: Autocorrelation For each lag, \\(k\\), the vertical line extending from zero represents an estimate of the autocorrelation coefficient at that lag. The dashed lines denote the 95% confidence interval, given by \\(\\pm 1.96/\\sqrt{T}\\), where \\(T\\) is the size of the time series sample. Another relevant measure of the time series dependence is partial autocorrelation, which is correlation between \\(Y_t\\) and \\(Y_{t-k}\\) net of any correlations between \\(Y_t\\) and \\(Y_{t-k+j}\\), for all \\(j=1,\\ldots,k-1\\). Similar to autocorrelations, partial autocorrelations, here denoted by \\(\\pi\\), can also be illustrated using autocorrelograms: Figure 2.3: Partial Autocorrelation Autocorrelogram is a useful tool for detecting dynamic properties of a time series. For example, autocorrelations of the stationary and ergodic stochastic process dissipate to zero as \\(k\\) increases. Thus, by observing autocorrelograms, in conjunction with a visual inspection of the time series, we can get an idea whether the process is stationary.13 If a time series appear nonstationary, we can make changes to it, so that the transformed series becomes stationary. 2.4 Transformations It is common to transform time series by taking logarithms (if applicable), by first-differencing the series, or by first-differencing the logarithms of the series. Such transformations are done to work with a suitable variable for the desired econometric analysis, or to address some underlying issues of the series (e.g., nonstationarity). Consider the Dow Jones Industrial Average (commonly referred to as the Dow Jones Index), for example. In levels, the series are clearly nonstationary. This is because the series are trending upward (on average), and also because the series become more volatile over time. Figure 2.4: Dow Jones Industrial Average Could first-differencing resolve the issue of nonstationarity? Turns out, not exactly. The first-differenced Dow Jones Index, while no longer trending, reveals increasing volatility over time. Figure 2.5: Dow Jones Industrial Average: Differences As it turns out, we can largely resolve the issue if we first-difference the log-transformed series. When we log-transform the series, the difference in observations from two different period is no longer measured in absolute terms, rather it is measured in relative terms. Then, by first-differencing the log-transformed series we, in effect, obtain a measure of a growth rate of the original series. The growth rate of the Dow Jones Index appears to be stationary. Figure 2.6: Dow Jones Industrial Average: Log-Differences Technically, this is a Gaussian white noise process as the random variables are iid normally distributed. Without the normality assumption we would have an Independent white noise process. Without the indepence assumption we would have simply a white noise process.↩︎ To be precise, this is a definition of covariance-stationarity or weak form of stationarity. Strict stationarity is defined by time invariant joint distribution of random variables.↩︎ There are, of course, formal tests of stationarity, which we will discuss in one of the subsequent chapters.↩︎ "],["generating-and-evaluating-forecasts.html", "Chapter 3 – Generating and Evaluating Forecasts 3.1 Pseudo-Forecasting Routine 3.2 Forecast Assessment", " Chapter 3 – Generating and Evaluating Forecasts 3.1 Pseudo-Forecasting Routine Forecast accuracy should only be determined by considering how well a model performs on data not used in estimation. But to assess forecast accuracy we need access to the data, typically from future time periods, that was not used in estimation. This leads to the so-called ‘pseudo forecasting’ routine. This routine involves splitting the available data into two segments referred to as ‘in-sample’ and ‘out-of-sample’. The in-sample segment of a series is also known as the ‘estimation set’ or the ‘training set’. The out-of-sample segment of a series is also known as the ‘hold-out set’ or the ‘test set’. Thus, we make the so-called ‘genuine’ forecasts using only the information from the estimation set, and assess the accuracy of these forecasts in an out-of-sample setting. Because forecasting is often performed in a time series context, the estimation set typically predates the hold-out set. In non-dynamic settings such chronological ordering may not be necessary, however. There are different forecasting schemes for updating the information set in the pseudo-forecasting routine. These are: recursive, rolling, and fixed. The recursive forecasting environment uses a sequence of expanding windows to update model estimates and the information set. The rolling forecasting environment uses a sequence of rolling windows of the same size to update model estimates and the information set. The fixed forecasting environment uses one fixed window for model estimates, and only updates the information set. The forecasts are made in period 13 for periods 14 and onward. The first estimation window of the pseudo-forecasting routine covers periods one–through–ten. In this illustration, shaded segments illustrate the range of the data used for estimation of model parameters. 3.2 Forecast Assessment To assess the accuracy of a forecast method, we compare forecast errors from two (or more) methods to each other. Consider a time series, \\(\\{y_t\\}\\), with a total of \\(T\\) observations. To generate genuine forecasts, we decide on the size of the in-sample set consisting of \\(R\\) observations so that \\(R &lt; T\\) (typically, \\(R \\approx 0.75T\\)), and the out-of-sample set consisting of \\(P\\) observations, where \\(P=T-R-h+1\\) and where \\(h\\) is the forecast horizon. For example, if we are interested in one-step-ahead forecast assessment, this way we will produce a sequence of forecasts: \\(\\{y_{R+1|R},y_{R+2|{R+1}},\\ldots,y_{T|{T-1}}\\}\\) for \\(\\{Y_{R+1},Y_{R+2},\\ldots,Y_{T}\\}\\). Forecast errors, \\(e_{R+j} = y_{R+j} - y_{R+j|{R+j-1}}\\), then can be computed for \\(j = 1,\\ldots,T-R\\). Two most commonly applied measures of forecast accuracy are the mean absolute forecast error (MAFE) and the root mean squared forecast error (RMSFE): \\[\\begin{aligned} \\text{MAFE} = &amp; \\frac{1}{P}\\sum_{i=1}^{P}|e_i|\\\\ \\text{RMSFE} = &amp; \\sqrt{\\frac{1}{P}\\sum_{i=1}^{P}e_i^2} \\end{aligned}\\] The lower is the measure of accuracy associated with a given method, the better this method performs in generating accurate forecasts. As noted earlier, ‘better’ does not mean ‘without errors’. Forecast errors of a ‘good’ forecasting method will have the following properties: zero mean; otherwise, the forecasts are biased. no correlation with the forecasts; otherwise, there is information left that should be used in computing forecasts. no serial correlation among one-step-ahead forecast errors. Note that \\(k\\)-step-ahead forecasts, for \\(k&gt;1\\), can be, and usually are, serially correlated. Any forecasting method that does not satisfy these properties has a potential to be improved. The aforementioned three desired properties of forecast errors are, in effect, hypotheses that can be tested. This can be done in a basic regression setting. 3.2.1 Unbiasedness Testing \\(E(e_{t+h|t})=0\\). Set up a regression: \\[e_{t+h|t} = \\alpha+\\upsilon_{t+h},\\;~~t = R,\\ldots,T-h,\\] where \\(R\\) is the estimation window size, \\(T\\) is the sample size, and \\(h\\) is the forecast horizon length. The null of zero-mean forecast error is equivalent of testing \\(H_0: \\alpha = 0\\) in the OLS setting. For \\(h\\)-step-ahead forecast errors, when \\(h&gt;1\\), autocorrelation consistent standard errors should be used. 3.2.2 Efficiency Testing \\(Cov(e_{t+h|t},\\hat{y}_{t+h|t})=0\\). Set up a regression: \\[e_{t+h|t} = \\alpha + \\beta \\hat{y}_{t+h|t} + \\upsilon_{t+h},\\;~~t = R,\\ldots,T-h.\\] The null of forecast error independence of the information set is equivalent of testing \\(H_0: \\beta = 0\\) in the OLS setting. For \\(h\\)-step-ahead forecast errors, when \\(h&gt;1\\), autocorrelation consistent standard errors should be used. 3.2.3 No Autocorrelation Testing \\(Cov(e_{t+1|t},e_{t|t-1})=0\\). Set up a regression: \\[e_{t+1|t} = \\alpha + \\gamma e_{t|t-1} + \\upsilon_{t+1},\\;~~t = R+1,\\ldots,T-1.\\] The null of no forecast error autocorrelation is equivalent of testing \\(H_0: \\gamma = 0\\) in the OLS setting. "],["trends.html", "Chapter 4 – Trends 4.1 Trends in the Data 4.2 Spurious Relationships 4.3 Modeling 4.4 Forecasting", " Chapter 4 – Trends 4.1 Trends in the Data Trends are common in economic time series (and not only). Observed over an extended period of time, gross domestic product, population, agricultural yields, and most recently global temperatures, all increase, on average. A trend is a pattern of change (increase or decrease) of a time series. It is possible for the direction as well as the rate of the change to alter over time. Interest rates, for example, were trending upward prior to 1980s, after which they started trending downward. Agricultural yields of cereal crops, for example, have been trending upward, but the slope of the trend has flattened over time. Modeling and forecasting trending time series is a fairly straightforward task. But before we get to it, let’s examine the issues that may arise if we ignore the presence of trends when analyzing the time series data. 4.2 Spurious Relationships Nothing about trending time series necessarily violates the classical linear regression model assumptions. The issue may arise, however, if an unobserved trending variable is simultaneously correlated with the dependent variable as well as one of the independent variables in a time series regression. In such case, we may find a (statistically significant) relationship between two or more unrelated economic variables simply because they are all trending. Such relationship is referred to as a spurious relationship. 4.2.1 Deterministic Trends To begin, we will illustrate the issue using two random variables, each containing a deterministic trend. To keep things simple, we will consider linear trends, but the “findings” of this exercise can be generalized to any form of a deterministic trend (some of the key forms we discuss below). So, consider two trending variables: \\[y_t = \\gamma t + \\nu_t,\\;~~\\nu\\sim N(0,\\sigma_{\\nu}^2),\\] and \\[x_t = \\delta t + \\upsilon_t,\\;~~\\upsilon\\sim N(0,\\sigma_{\\upsilon}^2),\\] where \\(Cov(\\nu_t,\\upsilon_t) = 0\\). For simplicity, we can assume \\(\\sigma_{\\nu}^2=\\sigma_{\\upsilon}^2=1\\). Suppose, \\(\\gamma\\) and \\(\\delta\\) are some positive scalars, say, \\(0.3\\) and \\(0.5\\), respectively. That is, \\(y\\) and \\(x\\) are trending in the same direction but at different rates. Below is an example of such time series: If we were to estimate \\[y_t = \\alpha+\\beta x_t + \\varepsilon_t,\\] we are likely to find the relationship between the two, in this case we will find \\(\\beta&gt;0\\), even though, we know, these variables are not related. To illustrate this, we will generate 1000 samples of size 120 for \\(y\\) and \\(x\\), and in each case we will estimate the parameter \\(\\beta\\). The following graph illustrates the empirical distribution of these parameter estimates: Luckily, we can easily “fix” the issue, by incorporating a trend in the regression: \\[y_t = \\alpha+\\beta x_t + \\eta t + \\varepsilon_t.\\] Once the trend is accounted for, the previously illustrated bias disappears. Using a similar simulation exercise as before, the following graph illustrates the empirical distribution of these parameter estimates: In fact, this “fix” is equivalent to regressing a de-trended \\(y\\) on a de-trended \\(x\\). To de-trend a variable, we first run a regression: \\(y_t = \\gamma_0 + \\gamma_1 t + \\nu_t\\), and then obtain the fitted values for some fixed time period (typically zero), that is: \\(\\tilde{y}_t = \\hat{\\gamma}_0+\\hat{\\nu}_t\\), where \\(\\hat{\\gamma}_0\\) and \\(\\hat{\\nu}_t\\) are the parameter estimate and the residuals from the foregoing regression. This equiavlence is the special case of the Frisch–Waugh–Lovell (FWL) theorem, due to Frisch and Waugh (1933) and Lovell (1963). 4.2.2 Stochastic Trends A time series may also follow a stochastic trend. A random walk process, \\(y_t=y_{t-1}+\\zeta_t\\), represents a stochastic trend. As it turns out, a spurious relationship can be detected between two stochastic trend processes. Suppose, in addition to the aforementioned random walk process, we have another random walk, \\(x_t=x_{t-1}+\\xi_t\\). As before, to keep things simple, suppose \\(\\zeta\\sim N(0,1)\\) and \\(\\xi\\sim N(0,1)\\), and \\(Cov(\\zeta_t,\\xi_t)=0\\). The two variables, obviously, are not related. And yet, if we regress one on another, we are likely to reject the null more frequently than we should. The following graph illustrates the empirical distribution of t statistics: The previous ‘fix’, which involved adding a trend in the regression, doesn’t quite work here, as observed in the following histogram. But there is a fix to the issue, and that involves first-differencing both series and regressing \\(\\Delta y_t\\) on \\(\\Delta x_t\\), which resolves the issue: 4.3 Modeling As seen, accounting for trends in a time series can help us resolve some regression issues. But a trend in and of itself can be an inherent feature of a times series. To that end, we can, and indeed should, account for trends, when they are present, to forecast time series. In what follows, the main focus is deterministic trends. We will address stochastic trends in another chapter. The simplest (and perhaps most frequently applied) model to account for the trending time series is a linear trend model: \\[y_t = \\alpha + \\beta t\\] Other likely candidate trend specifications are polynomial (e.g. quadratic, cubic, etc.), exponential, and shifting (or switching) trend models, respectively given by: \\[\\begin{aligned} y_t &amp;= \\alpha + \\beta_1 t + \\beta_2 t^2 + \\ldots + \\beta_p t^p \\\\ y_t &amp;= e^{\\alpha + \\beta t}\\;~~\\mbox{or}\\;~~\\ln{y_t} = \\alpha + \\beta t \\\\ y_t &amp;= \\alpha + \\beta_1 t + \\beta_2 (t-\\tau)I(t&gt;\\tau),\\;~~\\tau\\in\\mathsf{T} \\end{aligned}\\] In what follows, we will model and forecast linearly trending time series. Any polynomial trend model is an extension of a linear trend model, and shares the general characteristics of the latter. An exponential trend model, from the standpoint of modeling and forecasting, is largely equivalent to a linear trend model fitted to the natural logarithm of a series. For a time series \\(\\{y_t: t=1,\\ldots,T\\}\\), the natural logarithm is: \\(w_t = \\ln{y_t}\\). Some of the benefits of such a transformation are: easier to interpretation and comparison across different series (e.g., GDP growth across different countries); homogenized (across time) variability of a time series; possibly improved forecasting accuracy. We will cover the shifting/switching trend models in another chapter. Trends are (relatively) easy to model and forecast. Caution is needed, however, with (higher order) polynomial trends, as they may fit well in-sample, but cause major problems out-of-sample. 4.3.1 Trends in mortgage rates Consider monthly series of the U.S. mortgage rates (sourced from the online data portal of the Federal Reserve Bank of St. Louis) spanning the January 1991 – December 2020 period. Figure 4.1: 30-Year Fixed Rate Mortgage Average in the U.S. The series feature apparent downward trend, which we can approximate by a linear trend model. We estimate the model parameters by regressing interest rates, which we denote by \\(y\\) on time index, which we denote by \\(t\\). Fitted values are then given by: \\(\\hat{y}_t = \\hat{\\alpha} + \\hat{\\beta} t\\), which we overlay onto the time series: Figure 4.2: Fitted linear trend It appears that the linear trend well approximates the general long-term pattern of the time series. One potential issue becomes immediately apparent, however. If we were to present a multi-decade prediction, at some point, forecasts will enter the negative range, which makes no economic sense. To avoid the issue, we may favor an exponential trend model instead. We estimate the model parameters by regressing interest rates, which we denote by \\(y\\) on time index, which we denote by \\(t\\). Fitted values are then given by: \\(\\hat{y}_t = \\hat{\\alpha} + \\hat{\\beta} t\\), which we overlay onto the time series: Figure 4.3: Fitted exponential trend As it turns out, the exponential trend also well approximates the general long-term pattern of the time series. And, moreover, there is not risl for forecasts to ever take on a negative value, thus remaining economically sensible. Whether linear or exponential trend model yields a more accurate short-term forecasts, is a matter of forecast accuracy testing. 4.4 Forecasting The predictors of the deterministic trend models are pre-determined, which means, after fitting the model, we can readily obtain point and interval forecasts for any horizon \\(h\\). If a linear trend model is assumed, then a realization of the stochastic process at forecast horizon \\(h\\) is: \\[y_{t+h} = \\alpha + \\beta (t+h) + \\varepsilon_{t+h},\\] where \\(\\varepsilon_{t+h}\\sim iid~\\text{N}(0,\\sigma^2_{\\varepsilon})\\) is the error term. Point forecast of \\(y_{t+h}\\) is given by: \\[\\hat{y}_{t+h|t} = E(y_{t+h}|\\Omega_t;\\hat{\\theta}) = \\hat{\\alpha} + \\hat{\\beta} (t+h)\\] Let’s ignore parameter uncertainty at the moment, so that \\(\\hat{y}_{t+h|t}=y_{t+h|t}\\). The forecast error, then, is: \\[e_{t+h|t} = y_{t+h} - y_{t+h|t} = \\varepsilon_{t+h}\\] The forecast variance, then, is: \\[\\sigma_{t+h|t}^2 = E(e_{t+h|t}^2) = E(\\varepsilon_{t+h}^2) = \\hat{\\sigma}^2_{\\varepsilon},\\;~~\\forall\\;h\\] From this, we can obtain interval forecast at any horizon, which is: \\[y_{t+h|t} \\pm 1.96 \\hat{\\sigma}_{\\varepsilon}.\\] To illustrate the foregoing, let’s revisit the U.S. mortgage rates data, and obtain point and interval forecasts for periods from January 2011 onward based on parameter estimates using data up to and including December 2010. Figure 4.4: Linear trend forecast Several characteristics of the deterministic trend forecasts: they tend to understate uncertainty (at long horizons as the forecast interval does not widen with the horizon); short-term trend forecasts can perform poorly; long-term trend forecasts typically perform poorly; sometimes it may be beneficial, from the standpoint of achieving better accuracy, to forecast growth rates, and then reconstruct level forecasts. References "],["seasonality.html", "Chapter 5 – Seasonality 5.1 Seasonal Fluctuations in the Data 5.2 Modeling 5.3 Forecasting", " Chapter 5 – Seasonality 5.1 Seasonal Fluctuations in the Data Seasonality characterizes some economic time series. Prices of agricultural commodities, for example, tend to have a seasonal pattern, owing to relative abundance of the commodity after harvest, and their gradual depletion as the year progresses. Energy sales also tend to have seasonal pattern, due to relatively high demand for energy during the winter (the need for heating) and summer (the need for cooling) months. Seasonality is a self-repeating pattern within a calendar year that arises from the links of economic outcomes to the calendar. Modeling and forecasting seasonal time series, as it was the case with trending time series, is usually a straightforward exercise. 5.2 Modeling We typically model seasonality as a monthly or quarterly pattern, but it can also be modeled as a higher frequency pattern (e.g. weekly or daily). One way to account for seasonality in the data is to “remove” it prior to the use of the series (i.e., work with a seasonally adjusted time series). Indeed, some economic time series are only/also available in a seasonally-adjusted form. Otherwise, and perhaps more interestingly, we can incorporate the seasonal component into a regression setting. 5.2.1 Seasonal dummy variables A seasonal model, typically, is given by: \\[y_t = \\sum_{i=1}^{s}\\gamma_i d_{it} + \\varepsilon_t,\\] where \\(s\\) denotes the frequency of the data, and \\(d_{it}\\) takes the value of one repeatedly after every \\(s\\) periods, and is zero in all other periods, so that \\(\\sum_{i} d_{it} = 1\\), \\(\\forall t\\). Each \\(\\gamma_i\\) is, in effect, an intercept of a given season. Alternatively the seasonal model can be rewritten as: \\[y_t = \\alpha + \\sum_{i=1}^{s-1}\\delta_i d_{it} + \\varepsilon_t,\\] in which case \\(\\alpha\\) is an intercept of an omitted season, and \\(\\delta_i\\) represents a deviation from it during the \\(i^{th}\\) season. This is a more typical form of a seasonal model. Both variants of a seasonal model result in an identical fit and forecasts. Indeed, the two models are equivalent. Assuming the dummy variable associated with a season \\(s\\), \\(d_{s}\\), was dropped, \\(\\alpha=\\gamma_s\\), and \\(\\alpha+\\delta_i=\\gamma_i\\;~\\forall~i\\ne s\\). Consider a monthly series of vehicle miles traveled in the United States (sourced from the online data portal of the Federal Reserve Bank of St. Louis) spanning the January 2003 – December 2015 period. Figure 5.1: Vehicle miles traveled in the U.S. Seasonality is apparent in the series. We can approximate the seasonality using monthly dummy variables. Specifically, we can estimate the model parameters by regressing vehicle miles traveled, which we denote by \\(y\\), on intercept and 11 monthly dummy variables, omitting the dummy variable associated with the month of December. Fitted values are then given by: \\(\\hat{y}_t = \\hat{\\alpha}+\\sum_{i=1}^{11}\\hat{\\delta}_{i}d_{it}\\), which we overlay onto the time series: Figure 5.2: Fitted monthly dummy variables As observed, the fitted series mimic the seasonal pattern of the U.S. vehicle miles traveled exceedingly well. 5.2.2 Seasonal harmonic variables When dealing with weekly or daily data, the dummy variable approach of modeling seasonality may not be practical, nor efficient in most instances, as that will require estimating another 51 or 364 parameters. A way to model seasonality without giving up too many degrees of freedom is by using the so-called seasonal harmonic variables, which are terms from a Fourier series. Fourier terms can be applied to model seasonality at any frequency. Suppose, for example, we are working with monthly time series. A model with Fourier terms will have the following form: \\[y_t = \\alpha+\\sum_{k=1}^{K}\\left[\\beta_{1k}\\sin\\left(\\frac{2\\pi kt}{12}\\right)+\\beta_{2k}\\cos\\left(\\frac{2\\pi kt}{12}\\right)\\right]+\\varepsilon_t,\\] where the value of \\(K\\) can be determined using an information criterion (e.g., AIC or SIC). Consider a weekly series of initial unemployment claims filed by individuals after a separation from an employer in the United States (sourced from the online data portal of the Federal Reserve Bank of St. Louis) spanning the January 1993 – December 2008 period. Figure 5.3: Initial unemployment claims in the U.S. An apparent seasonal pattern is present in the series. It seems sensible to approximate the seasonality using Fourier series. Specifically, we can regress initial claims, which we denote by \\(y\\), on sine and cosine terms of up to order two, from a Fourier series. Fitted values are then given by: \\(\\hat{y}_t = \\hat{\\alpha}+\\sum_{k=1}^{2}\\left[\\hat{\\beta}_{1k}\\sin\\left(\\frac{2\\pi kt}{52.143}\\right)+\\hat{\\beta}_{2k}\\cos\\left(\\frac{2\\pi kt}{52.143}\\right)\\right]\\), which we overlay onto the time series: Figure 5.4: Fitted Fourier terms up to order two The fitted series mimic the general seasonal pattern reasonably well. There are some major departures at times, however. Especially so in the first week of the year, for example. While we could add higher order terms from a Fourier series to try and better approximate the seasonal pattern, perhaps a more efficient approach would be to simply add a dummy variable for the first week of the year, which should account for the beginning-of-calendar-year spike in the series. 5.3 Forecasting As it was the case with trend models, the predictors of the seasonal models are pre-determined. This means, after fitting the model, we can readily obtain point and interval forecasts for any horizon \\(h\\). To illustrate, consider the seasonal dummy variable model. A future realization of the random variable is: \\[y_{t+h} = \\alpha + \\sum_{i=1}^{s-1}\\delta_i d_{i,t+h} + \\varepsilon_{t+h}.\\] Point forecast of \\(y_{t+h}\\) is: \\[\\hat{y}_{t+h|t} = E(y_{t+h}|\\Omega_t;\\hat{\\theta}) = \\hat{\\alpha} + \\sum_{i=1}^{s-1}\\hat{\\delta}_i d_{i,t+h}\\] As before, we will assume that parameter estimates are equal to the true parameters of the model, so that the forecast error is: \\[e_{t+h|t} = y_{t+h} - y_{t+h|t} = \\varepsilon_{t+h}\\] The forecast variance is: \\[\\sigma_{t+h|t}^2 = E(e_{t+h|t}^2) = E(\\varepsilon_{t+h}^2) = \\hat{\\sigma}^2_{\\varepsilon},\\;~~\\forall\\;h\\] The interval forecast is: \\[y_{t+h|t} \\pm 1.96 \\hat{\\sigma}_{\\varepsilon}.\\] To illustrate the foregoing, let’s revisit the U.S. vehicle miles traveled data, and obtain point and interval forecasts for periods from January 2011 onward based on parameter estimates using data up to and including December 2010. Figure 5.5: Seasonal forecast "],["autoregression.html", "Chapter 6 – Autoregression 6.1 Stochastic Cycles 6.2 Modeling 6.3 Forecasting", " Chapter 6 – Autoregression 6.1 Stochastic Cycles Often there is a cyclical pattern in economic time series. Cycles are characterized by a sequence of expansions and contractions, almost like in the case of seasonality. Unlike seasonality, however, a cycle is not contained within a calendar year and, moreover, the amplitude and length of cycles may vary from one another. Such cycles, which are (assumed to be) generated by random variables, are referred to as stochastic cycles. Autoregressive stochastic cycle is a special and widely applied case of a stochastic cycle, where a random variable in a given period of a stochastic process is expressed as a function of random variables, of the same stochastic process, from preceding periods. That is: \\[Y_t = f(Y_{t-1},Y_{t-2},\\ldots),\\;~~t=1,\\ldots,T.\\] Autoregressive models, or simply autoregressions are deployed to approximate dynamics of such stochastic cycles. 6.2 Modeling An autoregression of order \\(p\\), denoted as \\(AR(p)\\), has the following functional form: \\[y_t = \\alpha + \\beta_1 y_{t-1}+\\beta_2 y_{t-2}+ \\cdots + \\beta_p y_{t-p}+\\varepsilon_t,\\] where \\(\\varepsilon\\sim iid~\\text{N}\\left(0,\\sigma^2_{\\varepsilon}\\right)\\) The sum of the autoregressive parameters, \\(\\beta_1,\\ldots,\\beta_p\\), depicts the persistence of the series. The larger is the persistence (i.e., closer it is to one), the longer it takes for the effect of a shock to dissolve. The effect will, eventually, dissolve so long as the series are covariance-stationary. The autocorrelation, \\(\\rho\\), and partial autocorrelation, \\(\\pi\\), functions of the covariance-stationary \\(AR(p)\\) process have the following distinctive features: \\(\\rho_1 = \\pi_1\\), and \\(\\pi_p = \\beta_p\\). The values of \\(\\beta_1,\\ldots,\\beta_p\\) determine the shape of the autocorrelation function (ACF); in any case, the smaller (in absolute terms) is the persistence measure, the faster the ACF decays toward zero. The partial autocorrelation function (PACF) is characterized by “statistically significant” first \\(p\\) spikes \\(\\pi_1 \\neq 0,\\ldots,\\pi_p \\neq 0\\), and the remaining \\(\\pi_k = 0\\), \\(\\forall k &gt; p\\). In what follows, we consider the cases of the first-order autoregression as an illustration. 6.2.1 First-order autoregression A first-order autoregression is given by: \\[y_t = \\alpha + \\beta_1 y_{t-1} + \\varepsilon_t,\\] where \\(\\alpha\\) is a constant term; \\(\\beta_1\\) is the persistence parameter; and \\(\\varepsilon_t\\) is a white noise process. A necessary and sufficient condition for an \\(AR(1)\\) process to be covariance stationary is that \\(|\\beta_1| &lt; 1\\). We can see this by substituting recursively the lagged equations into the lagged dependent variables: \\[ \\begin{aligned} y_t &amp;= \\alpha + \\beta_1 y_{t-1} + \\varepsilon_t \\notag \\\\ y_t &amp;= \\alpha + \\beta_1 (\\alpha + \\beta_1 y_{t-2} + \\varepsilon_{t-1}) + \\varepsilon_t \\notag \\\\ &amp;= \\alpha(1+\\beta_1) + \\beta_1^2 (\\alpha + \\beta_1 y_{t-3} + \\varepsilon_{t-2}) + \\beta_1\\varepsilon_{t-1} + \\varepsilon_t \\notag \\\\ &amp;\\vdots \\notag \\\\ &amp;= \\alpha\\sum_{i=0}^{k-1}\\beta_1^i + \\beta_1^k y_{t-k} + \\sum_{i=0}^{k-1}\\beta_1^i\\varepsilon_{t-i} \\end{aligned} \\] The end-result is a general linear process with geometrically declining coefficients. Here, \\(|\\beta_1| &lt; 1\\) is required for convergence. Assuming \\(|\\beta_1| &lt; 1\\), as \\(k \\to \\infty\\) the process converges to: \\[y_t = \\frac{\\alpha}{1-\\beta_1} + \\sum_{i=0}^{\\infty}\\beta_1^i\\varepsilon_{t-i}\\] The unconditional mean of this process is: \\[\\mu = E\\left(y_t\\right) = E\\left(\\frac{\\alpha}{1-\\beta_1} + \\sum_{i=0}^{\\infty}\\beta_1^i\\varepsilon_{t-i}\\right) = \\frac{\\alpha}{1-\\beta_1}\\] The unconditional variance of this process is: \\[\\gamma_0 = Var\\left(y_t\\right) = Var\\left(\\frac{\\alpha}{1-\\beta_1} + \\sum_{i=0}^{\\infty}\\beta_1^i\\varepsilon_{t-i}\\right) = \\frac{\\sigma_{\\varepsilon}^2}{1-\\beta_1^2}\\] The Autocovariance is simply the covariance between \\(y_t\\) and \\(y_{t-k}\\), that is: \\[\\gamma_k = Cov(y_t,y_{t-k}) = E[(y_t - \\mu)(y_{t-k} - \\mu)] = E(y_t y_{t-k}) - \\mu^2\\] Some algebraic manipulation can help us show that: \\[\\gamma_k = \\beta_1\\gamma_{k-1},\\] and that: \\[\\rho_{k} = \\beta_1\\rho_{k-1}\\] (recall, \\(\\rho_k = \\gamma_k/\\gamma_0\\) is the autocorrelation coefficient). In fact, for AR(1), an autocorrelation coefficient of some lag can be represented as the autoregression parameter (which in this instance is equivalent to the persistence measure) to that power. That is: \\[ \\begin{aligned} \\rho_1 &amp;= \\beta_1\\rho_0 = \\beta_1 \\notag \\\\ \\rho_2 &amp;= \\beta_1\\rho_1 = \\beta_1^2 \\notag \\\\ &amp;\\vdots \\notag \\\\ \\rho_k &amp;= \\beta_1\\rho_{k-1} = \\beta_1^k \\end{aligned} \\] It follows that the autocorrelation function of a covariance stationary AR(1) is a geometric decay; the smaller is \\(|\\beta_1|\\) the more rapid is the decay. Moreover, a smaller persistence parameter results in a quicker adjustment to the unconditional mean of the process. 6.2.2 Unit Roots and Non-stationarity A special case of an AR(1) process is a random walk with drift. The latter is obtained by setting \\(\\beta_1=1\\). Note that the unconditional mean and variance are undefined under this restriction. We cannot directly test the null of non-stationarity in an autoregression. That is, we cannot just estimate: \\[y_t = \\alpha+\\beta_1 y_{t-1}+\\varepsilon_t\\] and test whether \\(\\hat{\\beta}_1\\) is statistically significantly less than unity. Instead, we can subtract \\(y_{t-1}\\) from both sides of the equation, and estimate: \\[\\Delta y_t = \\alpha+\\phi y_{t-1}+\\varepsilon_t,\\] where \\(\\phi=\\beta_1-1\\), and test whether \\(\\hat{\\phi}\\) is statistically significantly less than zero. This test is known as the Dickey-Fuller (DF) test. In practice, we apply the augmented Dickey-Fuller (ADF) test, which is the same test as above, except the lags of the dependent variable, \\(\\Delta y_t\\), are added to the regression to ensure that \\(\\varepsilon_t\\) is white noise. It is important to note that distribution of the test statistic is non-standard. That is, we can not use t distribution to test the null hypothesis of non-stationarity after obtaining the test statistic associated with \\(\\hat{\\phi}\\). Instead, we use the relevant version of the Dickey-Fuller table.14 To illustrate some of the foregoing, consider the USD/EUR exchange rates. Figure 6.1: USD/EUR exchange rates First, let’s observe the autocorrelation and partial autocorrelation functions of the series. Figure 6.2: Autocorrelation function Figure 6.3: Partial autocorrelation function That autocorrelations decrease gradually and eventually become statistically indistinguishable from zero is suggestive of a stationary process. That partial autocorrelations of the first two lags are statistically significantly different from zero is suggestive of the second-order autoregression.15 Let’s check formally, using augmented Dickey-Fuller test, whether the series are non-stationary. For that, estimate \\(\\Delta y_t=\\alpha+\\phi y_{t-1}+\\delta_1\\Delta y_{t-1}+\\varepsilon_t\\), and obtain test statistics associated with \\(\\hat{\\phi}\\). The test statistic turns out to be -2.757 which lies between the critical values of -2.88 for 5% statistical significance and -2.57 for 10% statistical significance. 6.3 Forecasting Making forecasts for some future period, \\(t+h\\), from an AR(p) model that has been fit to the data up to and including period \\(t+h-1\\) can be a straightforward exercise, so long as we have access to such data. That is the case for one-step-ahead forecasts, that is when \\(h=1\\), for which the information set is readily available. For multi-step-ahead forecasts, that is when \\(h&gt;1\\), this no longer it the case. There are two approaches or methods of multi-step-ahead forecasting that can allowe us to circumvent the issue. 6.3.1 Iterative Method of Multistep Forecasting One approach involves ‘coming up’ with the value of the variable that has not been realized yet. For example, when making a two-step-ahead forecast for period \\(t+2\\), we need data from period \\(t+1\\), which is not available at the time when the forecast is made. Instead, we need to use our forecast for period \\(t+1\\). The same applies to forecasts for any subsequent periods in the future. This approach is known as an iterative method of forecasting, wherein we make forecast for some period using the available data, then iterate forward by one period and use the most recent forecast to make the next period’s forecast, and so on and so forth. Consider an AR(p) model. A future realization of the random variable is \\[y_{t+h} = \\alpha + \\beta_1 y_{t+h-1} + \\cdots + \\beta_p y_{t+h-p}+\\varepsilon_{t+h}\\] Point forecast (ignoring parameter uncertainty) is: \\[y_{t+h|t} = E(y_{t+h}|\\Omega_t) = \\alpha + \\beta_1 y_{t+h-1|t} + \\cdots + \\beta_p y_{t+h-p|t},\\] where \\(y_{t+h-j|t}=y_{t+h-j}\\) when \\(h-j\\le 0\\). That is, for a given time period, we use the realization of the random variable if it is observed, otherwise we use the point forecast of the realization. Forecast error: \\[e_{t+h|t} = y_{t+h} - y_{t+h|t} = \\beta_1 e_{t+h-1|t} + \\cdots + \\beta_p e_{t+h-p|t} + \\varepsilon_{t+h},\\] where \\(e_{t+h-j|t}=0\\) when \\(h-j\\le 0\\). So, when \\(h=1\\), \\(e_{t+1|t}=\\varepsilon_{t+1}\\), which is the same as with previously described models (e.g., trend or seasonal models), but when \\(h&gt;1\\), forecast error becomes more complex. The forecast variance: \\[\\sigma_{t+h|t}^2 = \\sigma_{\\varepsilon}^2 + \\sum_{i=1}^{p}\\beta_i^2 Var(e_{t+h-i|t}) + 2\\sum_{i \\neq j}\\beta_i\\beta_j Cov(e_{t+h-i|t},e_{t+h-j|t})\\] These variance and covariances of forecast errors from preceding horizons are some functions of the in-sample error variance and model parameters. The 95% interval forecast is: \\[y_{t+h|t} \\pm 1.96 \\hat{\\sigma}_{\\varepsilon}.\\] To illustrate the foregoing, let’s revisit the USD/EUR exchange rate series, and obtain point and interval forecasts for periods from January 2011 onward based on parameter estimates of the second-order autoregression using data up to and including December 2010. Figure 6.4: Second-order autoregression 6.3.2 Direct Method of Multistep Forecasting The other approach entails directly obtaining multi-step-ahead forecasts. To illustrate, consider a first-order autoregression: \\(y_t=\\alpha+\\beta_1 y_{t-1}+\\varepsilon_t.\\) One-step-ahead point forecast is readily given by: \\(y_{t+1|t}=\\alpha+\\beta_1 y_{t}\\). That is, we observe all the variables on the right-hand side of the equation. To generate the two-step-ahead forecast in a similar manner, that is by ensuring the observed variables on the right-hand side of the equation, we can substitute \\(y_{t-1}=\\alpha+\\beta_1 y_{t-2}+\\varepsilon_{t-1}\\) into the original equation to obtain: \\[y_t=\\alpha(1+\\beta_1)+\\beta_1^2y_{t-2} + \\varepsilon_t + \\beta_1\\varepsilon_{t-1} = \\tilde{\\alpha} + \\tilde{\\beta}_1 y_{t-2} + u_t,\\] where \\(\\tilde{\\alpha}=\\alpha(1+\\beta_1)\\) and \\(\\tilde{\\beta}_1=\\beta_1^2\\), and \\(u_t=\\varepsilon_t + \\beta_1\\varepsilon_{t-1}.\\) Thus, we can obtain two-step-ahead forecast in a manner similar to that when we obtain one-step-ahead forecast by regressing \\(y_t\\) on \\(y_{t-2}\\), and then directly forecasting \\(y_{t+2}\\) from \\(y_{t}\\). This direct method of multi-step-ahead forecasting can be extended to higher order autoregression, as well as to any forecast horizon. In the direct method, error terms are serially correlated (by construction).16 For example, in the direct two-step-ahead forecast from a re-specified AR(1) model, as we saw: \\(u_t = \\varepsilon_t+\\beta_1\\varepsilon_{t-1}\\). It then follows that: \\[\\sigma^2_u = E\\left[(\\varepsilon_t+\\beta_1\\varepsilon_{t-1})^2\\right] = \\sigma^2_{\\varepsilon}(1+\\beta_1^2).\\] This is also the expression of the two-step-ahead forecast error variance under the iterated method. Thus, when applying the direct method of forecasting, interval forecasts for a given horizon are obtained ‘directly,’ based on the standard deviation of the residuals. The relative performance of forecasts from the considered two methods—iterative and direct—in terms of bias and efficiency depends on the bias and efficiency of the estimators of each method. Assuming the autoregressive model is correctly specified, both methods are consistent, but the iterative method is more efficient. Thus, in large samples, the iterative forecast can be expected to perform better than the direct forecast. In the case of a mis-specified model, however, the direct method may as well outperform the iterated method. The three versions of the test are (i) unit root, (ii) unit root with drift, and (iii) unit root with drift and trend.↩︎ More formally, the order of autoregression can be determined using an information criterion such as AIC or SIC.↩︎ Recall that multi-step-ahead forecast errors tend to be serially correlated. So, direct method merely maintains this feature of multistep forecasts.↩︎ "],["vector-autoregression.html", "Chapter 7 – Vector Autoregression 7.1 Dynamic Feedbacks Among Economic Variables 7.2 Modeling 7.3 Forecasting", " Chapter 7 – Vector Autoregression 7.1 Dynamic Feedbacks Among Economic Variables Economic variables are interrelated. For example, changes to household income impact their consumption levels; changes to interest rates impact investments in the economy. Often (albeit not always) the relationship between the variables goes in both directions. For example, growth in income results in higher prices (inflation), which in turn puts an upward pressure on wages.17 The foregoing implies that a shock to a variable may propagate a dynamic response not only of that variable, but also of related variables. The dynamic linkages between two (or more) economic variables can be modeled as a system of equations, represented by a vector autoregression (VAR). There are three general forms of vector autoregressions: structural, recursive, and reduced-form. The structural VAR uses economic theory to impose the ‘structure’ on correlations of the error terms in the system. The recursive VAR also introduces a structure of some sort, which primarily involves ordering the equations in the system in a specific way so that the error terms in each equation are uncorrelated with those in the preceding equations. To the extent that the ‘identifying assumptions’ are satisfied, some contemporaneous values (of other variables) appear in the equation of a given variable. The reduced-form VAR makes no claims of causality, at least not in the sense that this term is usually understood. The equations include only the lagged values of all the variables in the system. To the extent that these variables are, indeed, correlated with each other, the error terms of the reduced-form VAR (typically) are contemporaneously correlated. In what follows, VAR will refer to the reduced-form VAR, unless otherwise stated. 7.2 Modeling The simplest form of the VAR is a bivariate (two-dimensional) VAR of order one, VAR(1). Let \\(\\{X_{1,t}\\}\\) and \\(\\{X_{2,t}\\}\\) be the stationary stochastic processes. A bivariate VAR(1), is then given by: \\[\\begin{aligned} x_{1t} &amp;= \\alpha_1 + \\pi_{11}x_{1t-1} + \\pi_{12}x_{2t-1} + \\varepsilon_{1t} \\\\ x_{2t} &amp;= \\alpha_2 + \\pi_{21}x_{1t-1} + \\pi_{22}x_{2t-1} + \\varepsilon_{2t} \\end{aligned}\\] where \\(\\varepsilon_{1,t} \\sim iid(0,\\sigma_1^2)\\) and \\(\\varepsilon_{2,t} \\sim iid(0,\\sigma_2^2)\\), and the two can be correlated, i.e., \\(Cov(\\varepsilon_{1,t},\\varepsilon_{2,t}) \\neq 0\\). To generalize, consider a multivariate (\\(n\\)-dimensional) VAR of order \\(p\\), VAR(p), presented in matrix notation: \\[\\mathbf{x}_t = \\mathbf{\\alpha} + \\Pi^{(1)} \\mathbf{x}_{t-1} + \\ldots + \\Pi^{(p)} \\mathbf{x}_{t-p} + \\mathbf{\\varepsilon}_t,\\] where \\(\\mathbf{x}_t = (x_{1t},\\ldots,x_{nt})&#39;\\) is a vector of \\(n\\) (potentially) related variables; \\(\\mathbf{\\varepsilon}_t = (\\varepsilon_{1t},\\ldots,\\varepsilon_{nt})&#39;\\) is a vector of error terms, such that \\(E(\\mathbf{\\varepsilon}_t) = \\mathbf{0}\\), \\(E(\\mathbf{\\varepsilon}_t^{}\\mathbf{\\varepsilon}_t^{\\prime}) = \\Sigma_{\\mathbf{\\varepsilon}}\\), and \\(E(\\mathbf{\\varepsilon}_{t}^{}\\mathbf{\\varepsilon}_{s \\neq t}^{\\prime}) = 0\\). \\(\\Pi^{(1)},\\ldots,\\Pi^{(p)}\\) are \\(n\\)-dimensional parameter matrices: \\[\\Pi^{(j)} = \\left[ \\begin{array}{cccc} \\pi_{11}^{(j)} &amp; \\pi_{12}^{(j)} &amp; \\cdots &amp; \\pi_{1n}^{(j)} \\\\ \\pi_{21}^{(j)} &amp; \\pi_{22}^{(j)} &amp; \\cdots &amp; \\pi_{2n}^{(j)} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\pi_{n1}^{(j)} &amp; \\pi_{n2}^{(j)} &amp; \\cdots &amp; \\pi_{nn}^{(j)} \\end{array} \\right],\\;~~j=1,\\ldots,p\\] When two or more variables are modeled in this way, the implies assumption is that these variables are endogenous to each other; that is, each of the variables affects and is affected by other variables. Some of the features of the VAR are that: only lagged values of the dependent variables are considered as the right-hand-side variables (although, trend and seasonal variables might also be included in higher-frequency data analysis); each equation has the same set of right-hand-side variables (however, it is possible to impose a different lag structure across the equations, especially when \\(p\\) is relatively large, to preserve the degrees of freedom, particularly when the sample size is relatively small and when there are several variables in the system). The autregressive order of the VAR, \\(p\\), is determined by the maximum lag of a variable across all equations. The order of a VAR, \\(p\\), can be determined using system-wide information criteria: \\[\\begin{aligned} &amp; AIC = \\ln\\left|\\Sigma_{\\varepsilon}\\right| + \\frac{2}{T}(pn^2+n) \\\\ &amp; SIC = \\ln\\left|\\Sigma_{\\varepsilon}\\right| + \\frac{\\ln{T}}{T}(pn^2+n) \\end{aligned}\\] where \\(\\left|\\Sigma_{\\varepsilon}\\right|\\) is the determinant of the residual covariance matrix; \\(n\\) is the number of equations, and \\(T\\) is the effective sample size. We can estimate each equation of the VAR individually using OLS. 7.2.1 In-Sample Granger Causality A test of joint significance of parameters associated with all the lags of a variable entering the equation of another variable is known as the ‘Granger causality’ test. The use of the term ‘causality’ in this context has been criticized. That one variable can help explain the movement of another variable does not necessarily mean that the former causes the latter. To that end the use of the term can be seen as misleading, indeed. Rather, causality in Granger sense simply means that the former helps predict the latter. To illustrate the testing framework, consider a bivariate VAR(p): \\[\\begin{aligned} x_{1t} &amp;= \\alpha_1 + \\pi_{11}^{(1)} x_{1t-1} + \\cdots + \\pi_{11}^{(p)} x_{1t-p} \\\\ &amp;+ \\pi_{12}^{(1)} x_{2t-1} + \\cdots + \\pi_{12}^{(p)} x_{2,t-p} +\\varepsilon_{1t} \\\\ x_{2t} &amp;= \\alpha_1 + \\pi_{21}^{(1)} x_{1t-1} + \\cdots + \\pi_{21}^{(p)} x_{1t-p} \\\\ &amp;+ \\pi_{22}^{(1)} x_{2t-1} + \\cdots + \\pi_{22}^{(p)} x_{2t-p} +\\varepsilon_{2t} \\end{aligned}\\] We say that: \\(\\{X_2\\}\\) does not Granger cause \\(\\{X_1\\}\\) if \\(\\pi_{121}=\\cdots=\\pi_{12p}=0\\) \\(\\{X_1\\}\\) does not Granger cause \\(\\{X_2\\}\\) if \\(\\pi_{211}=\\cdots=\\pi_{21p}=0\\) So long as the variables of the system are covariance-stationarity, we can test the hypothesis using a F test. If \\(p=1\\), we can also equivalently test the hypothesis using a t test. 7.3 Forecasting Generating forecasts for each of the variable comprising the VAR(p) model can be a straightforward exercise, so long as we have access to the relevant information set. As was the case with autoregressive models, we make one-step-ahead forecasts based on the readily available data; and we make multi-step-ahead forecasts iteratively, using the forecast in periods for which the data are not present. 7.3.1 One-step-ahead forecasts In a VAR(p), the realization of a dependent variable \\(i\\) in period \\(t+1\\) is: \\[x_{it+1} = \\alpha_i + \\sum_{j=1}^{p}\\sum_{k=1}^{n}\\pi_{ik}^{(j)} x_{kt+1-j} + \\varepsilon_{t+1},\\;~~i=1,\\ldots,n.\\] Point forecast, ignoring parameter uncertainty, is: \\[x_{it+1|t} = \\alpha_i + \\sum_{j=1}^{p}\\sum_{k=1}^{n}\\pi_{ik}^{(j)} x_{kt+1-j},\\;~~i=1,\\ldots,n.\\] Forecast error is: \\[e_{it+1} = x_{it+1}-x_{it+1|t}=\\varepsilon_{it+1},\\;~~i=1,\\ldots,n.\\] Forecast variance is: \\[\\sigma_{it+1}^2 = E(\\varepsilon_{it+1}^2) = \\sigma_{i}^2,\\;~~i=1,\\ldots,n,\\] where \\(\\sigma_{i}^2\\) is the i\\(^{th}\\) element of the main diagonal of the error variance-covariance matrix of the VAR(p). The (95%) interval forecast is: \\[x_{it+1|t}\\pm1.96\\sigma_{i}^2,\\;~~i=1,\\ldots,n.\\] 7.3.2 Multi-step-ahead forecasts The realization of a dependent variables \\(i\\) in period \\(t+h\\) is: \\[x_{it+h} = \\alpha_i + \\sum_{j=1}^{p}\\sum_{k=1}^{n}\\pi_{ik}^{(j)} x_{kt+h-j} + \\varepsilon_{t+h},\\;~~i=1,\\ldots,n.\\] Point forecast is: \\[\\mathbf{x}_{t+h|t} = \\alpha_i + \\sum_{j=1}^{p}\\sum_{k=1}^{n}\\pi_{ik}^{(j)} x_{kt+h-j|t},\\;~~i=1,\\ldots,n.\\] where the iterative method is applied to generate forecasts, and where \\(x_{kt+h-j|t}=x_{t+h-j}\\) if \\(j\\ge h\\). Forecast error is: \\[e_{it+h} = x_{it+h}-x_{t+h|t}=\\sum_{j=1}^{p}\\sum_{k=1}^{n}\\pi_{ik}^{(j)} e_{kt+h-j} + \\varepsilon_{t+h},\\;~~i=1,\\ldots,n.\\] Note that the forecast error reduces to \\(\\varepsilon_{t+h}\\), when \\(h=1\\), which is what we saw above. More intriguingly, observe that the multi-step-ahead forecast error of a given variable is the function of preceding forecast errors of all variables in the system, each multiplied by some parameter of the model. Forecast variance, \\(\\sigma_{it+h}^2\\), is the function of error variances and covariances, and the model parameters. The (95%) interval forecast is: \\[x_{it+h|t}\\pm1.96\\sigma_{it+h},\\;~~i=1,\\ldots,n.\\] 7.3.3 Out-of-Sample Granger Causality The previously discussed (in sample) test of causality in Granger sense is frequently performed in practice. But as noted above, the term ‘causality’ may be misleading somewhat. Indeed, the ‘true spirit’ of such test is to assess the ability of a variable to help predict another variable in an out-of-sample setting. Consider a set of variables, \\(\\{X_1\\},\\ldots,\\{X_n\\}\\). Suppose there are two information sets, the unrestricted information set, \\(\\Omega_{t}^{(u)}\\), and the restricted information set, \\(\\Omega_{t}^{(r)}\\). The former consists of the current and lagged values of all the variables in the set. The latter consists of the current and lagged values of all but one variables in the set. Suppose the omitted variable is \\(\\{X_1\\}\\). Then, following Granger’s definition of causality: \\(\\{X_1\\}\\) is said to cause \\(\\{X_{i\\ne 1}\\}\\) if \\(\\sigma_{i}^2\\left(\\Omega_{t}^{(u)}\\right) &lt; \\sigma_{i}^2\\left(\\Omega_{t}^{(r)}\\right)\\), meaning that we can better predict \\(X_i\\) using the available information on \\(\\{X_1\\}\\) through \\(\\{X_n\\]\\), rather than that on \\(\\{X_2\\}\\) through \\(\\{X_n\\}\\) only. In practice, the test involves generating two sets of (one-step-ahead) out-of-sample forecasts from the unrestricted and restricted equations of the vector autoregression. The former is simply the forecast as presented above, that is: \\[x_{it+1|t}^{(u)} = \\alpha_i + \\sum_{j=1}^{p}\\sum_{k=1}^{n}\\pi_{ik}^{(j)} x_{kt+1-j},\\;~~i=2,\\ldots,n.\\] The latter is the forecast that doesn’t rely on information from the omitted variable (in our example, the first variable in the unrestricted system of equations): \\[x_{it+1|t}^{(r)} = \\tilde{\\alpha}_i + \\sum_{j=1}^{p}\\sum_{k=2}^{n}\\tilde{\\pi}_{ik}^{(j)} x_{kt+1-j},\\;~~i=2,\\ldots,n.\\] For these forecasts, the corresponding forecast errors are: \\[\\begin{aligned} &amp; e_{it+1}^{(u)} = x_{1t+1} - x_{1t+1|t}^{(u)}\\\\ &amp; e_{it+1}^{(r)} = x_{1t+1} - x_{1t+1|t}^{(r)} \\end{aligned}\\] The out-of-sample forecast errors are then evaluated by comparing the loss functions based on these forecasts errors. For example, assuming quadratic loss, and \\(P\\) out-of-sample forecasts: \\[\\begin{aligned} RMSFE^{(u)} &amp;= \\sqrt{\\frac{1}{P}\\sum_{s=1}^{P}\\left(e_{iR+s|R-1+s}^{(u)}\\right)^2} \\\\ RMSFE^{(r)} &amp;= \\sqrt{\\frac{1}{P}\\sum_{s=1}^{P}\\left(e_{iR+s|R-1+s}^{(r)}\\right)^2} \\end{aligned}\\] where \\(R\\) is the size of the (first) estimation window. \\(\\{X_1\\}\\) is said to cause in Granger sense \\(\\{X_{i\\ne 1}\\}\\) if \\(RMSFE^{(u)} &lt; RMSFE^{(r)}\\). The sequence of events in Australia, following the Covid-19 pandemic, is a notable case in point. Fearing economic crisis, the Australian government issued a suite of stimulus packages. A resulting increase in demand became one of the contributing factors to the subsequent price inflation. At that point, the Reserve Bank of Australia was left with no option but to hike interest rates to slow down the economy and, thus, release the inflationary pressure.↩︎ "],["threshold-autoregression.html", "Chapter 8 – Threshold Autoregression 8.1 Regime-Dependent Nonlinearity 8.2 Modeling 8.3 Forecasting", " Chapter 8 – Threshold Autoregression 8.1 Regime-Dependent Nonlinearity A linear model is a crude approximation of a true model. It may very well be possible that a nonlinear model offers a better fit to the data. Nonlinear models come in many flavours. Here we will consider a specific type of nonlinear models that belongs to the family of regime-dependent models. A regime-dependent model can be seen as a combination of linear specifications that are linked to each other in some (nonlinear) way. To that end, such nonlinear models are also referred to as the piece-wise linear models—i.e., each piece, or segment, of the model is linear, but when taken together we have a nonlinear model at hand. Here, we will consider two representative regime-dependent models: a time-varying threshold autoregression (TVAR) and a self-exciting threshold autoregression (SETAR). In both instances, we will assume that the switch between the regimes happens based on some threshold variable, and that it happens instantaneously.18 8.2 Modeling Consider an AR(p) process with a deterministic trend: \\[y_t = \\alpha_0 + \\alpha_1 t + \\sum_{i=1}^{p}\\beta_i y_{t-i} + \\varepsilon_t,\\] where \\(\\alpha_0 + \\alpha_1 t\\) is the time-specific deterministic component. This specification assumes a linear trend, but that doesn’t need to be the case. We can have quadratic or cubic trends, for example, or we can have no trend component at all. A simple augmentation of the foregoing model is an autoregressive model with a switching trend component: \\[y_t = \\delta_{0} + \\delta_{1} t + \\delta_{2}(t-\\tau)I(t&gt;\\tau) + \\beta y_{t-1} + \\varepsilon_t,\\] where \\(\\tau\\) is the threshold parameter. Such switch can be extended to the whole autoregression. For example, a two-regime AR(p) with drift can be given by: \\[y_t = \\delta_0 + \\delta_1 t + \\sum_{i=1}^{p}\\beta_{1i} y_{t-i} + \\left[\\delta_2(t-\\tau) + \\sum_{i=1}^{p}\\beta_{2i} y_{t-i}\\right]I(t&gt;\\tau) + \\varepsilon_t.\\] This equation implies that not only the trend, but also the autoregressive dynamics change around \\(\\tau\\). The foregoing nonlinear specifications assumes that the switch in the model occurs at some point in time, i.e. the regime-switching variable is a function of time. But the regime-switching variable can also be a function of the dependent variable or, indeed, any other (exogenous) variable: \\[y_t = \\alpha_0 + \\sum_{i=1}^{p}\\beta_{0i} y_{t-i} + \\left(\\alpha_1 + \\sum_{i=1}^{p}\\beta_{1i} y_{t-i}\\right)I(s_t&gt;\\kappa) + \\varepsilon_t,\\] where \\(s_t\\) is the regime-switching variable, and \\(\\kappa\\) is the threshold, such that \\(\\underline{s}_t &lt; \\kappa &lt; \\overline{s}_t\\), where \\(\\underline{s}_t\\) and \\(\\overline{s}_t\\) are the lower and upper quantiles of the regime-switching variable.19 This equation is referred to as a threshold autoregression, or TAR(p). More specifically, if in a TAR(p), \\(s_t = y_{t-d}\\), \\(d = 1,\\ldots,m\\), then it is a self-exciting threshold autoregression, or SETAR(p); alternatively, if \\(s_t = \\Delta y_{t-d}\\), \\(d = 1,\\ldots,m\\), then the model is referred to as a momentum threshold autoregression, or momentum-TAR(p). The latter is typically applied when \\(y_t\\) is an I(1) process. A TAR (any version of it) can take a multiple-regime form: \\[y_t = \\alpha_1 + \\sum_{i=1}^{p}\\beta_{1i} y_{t-i} + \\sum_{j=2}^{K}{\\left(\\alpha_j + \\sum_{i=1}^{p}\\beta_{ji} y_{t-i}\\right)I(s_t&gt;\\kappa_j)} + \\varepsilon_t,\\] where \\(K\\) depicts the number of regimes in the equation. When estimating TAR-type models, we usually have no a priori knowledge on the number of regimes, the autoregressive order in each regime, the regime-switching (or threshold) variable, and the value(s) of the threshold parameter(s). Joint estimation of model parameters requires some nonlinear optimization routine. Alternatively, we can approximate such optimization using a grid-search procedure. The procedure relies on the fact that once the threshold parameter is known, the model reduces to a linear model, and the least squares estimator can be applied then. That is, for example, \\[\\hat{\\tau} = \\arg\\min_{\\tau}\\hat{\\sigma}^2(\\tau),\\] where \\[\\hat{\\sigma}^2(\\tau) = \\frac{1}{T-k}\\sum_{t=1}^{T}\\hat{\\varepsilon}_t^2(\\tau)\\] for all candidate values of \\(\\tau\\). The candidate values of \\(\\tau\\) typically belong to a range between the lower and upper quantiles of the transition variable, which is simply the trend in the case of the time-varying TAR, and the lagged dependent variable in the case of the self-exciting TAR. 8.3 Forecasting In the case of time-varying shifting trend (mean) models, the most recent trend component is used to obtain forecasts. To that end, the forecasting routine is similar to that of linear trend models. In the case of regime-switching models (e.g., TAR), obtaining one-step-ahead forecasts is a straightforward exercise: \\[\\begin{aligned} y_{t+1|t} &amp;= \\alpha_0+\\beta_{01}y_{t}+\\beta_{02}y_{t-1}+\\ldots \\\\ &amp;+ (\\alpha_1+\\beta_{11}y_{t}+\\beta_{12}y_{t-1}+\\ldots)I(s_t&gt;c) \\end{aligned}\\] Obtaining h-step-ahead forecasts (where \\(h&gt;1\\)) is less trivial, however. Of the available options: The iterated method (or, the so-called skeleton extrapolation) is an easy but an inefficient option. The analytical method can be unbearably tedious. A numerical method is usually applicable and, moreover, it addresses the caveats of the previous two options. 8.3.1 Skeleton Extrapolation One-step-ahead forecast: \\[y_{t+1|t} = E(y_{t+1}|\\Omega_{t}) = g(y_{t},y_{t-1},\\ldots,y_{t+1-p};\\theta)\\] Two-step-ahead forecast: \\[y_{t+2|t} = E(y_{t+2}|\\Omega_{t}) = g(y_{t+1|t},y_{t},\\ldots,y_{t+2-p};\\theta)\\] h-step-ahead forecast: \\[y_{t+h|t} = E(y_{t+h}|\\Omega_{t}) = g(y_{t+h-1|t},y_{t+h-2|t},\\ldots,y_{t+h-p|t};\\theta)\\] This is fine for linear models; but not okay for nonlinear models. 8.3.2 Analytical Method One-step-ahead forecast is the same as before (no uncertainty about the observed data). Two-step-ahead forecast is: \\[\\tilde{y}_{t+2|t} = \\int_{-\\infty}^{\\infty}g(y_{t+1|t}+\\varepsilon_{t+1},y_{t},\\ldots,y_{t+2-p};\\theta)f(\\varepsilon_{t+1})d\\varepsilon_{t+1}\\] Unless the model is linear, \\(\\tilde{y}_{t+2|t} \\ne y_{t+2|t}\\). Longer horizon forecasts require multiple integrals. 8.3.3 Numerical Method: Bootstrap Resampling Bootstrap resampling helps approximate the optimal forecast from nonlinear models and circumvents the complexity of integration. Algorithm: Estimate the (nonlinear) time series model and store the residuals. From this set of residuals, sample (with replacement) a vector of shocks for a bootstrap iteration, \\(\\varepsilon^b = (\\varepsilon_{t+1}^b,\\varepsilon_{t+2}^b,\\ldots,\\varepsilon_{t+h}^b)&#39;\\). Use this sample of shocks, along with the estimated parameters and historical observations, to iteratively generate a forecast path for the given bootstrap iteration. Repeat steps 2-3 many times to generate an empirical distribution of forecast paths. Take the simple averages, for each horizon, across all bootstrap iterations to generate point forecasts. To better illustrate the algorithm, consider a \\(\\text{SETAR}(p,y_{t-1})\\) model, that is, a SETAR(p) with \\(y_{t-1}\\) as the regime-switching variable. One-step-ahead bootstrap forecast: \\[\\begin{aligned} y_{t+1|t}^b &amp;= (\\alpha_1 + \\beta_{11} y_{t} + \\ldots + \\beta_{1p} y_{t+1-p})I(y_{t} \\leq \\kappa) \\\\ &amp;+ (\\alpha_2 + \\beta_{21} y_{t} + \\ldots + \\beta_{2p} y_{t+1-p})I(y_{t} &gt; \\kappa)+\\varepsilon_{t+1}^b \\end{aligned}\\] Note that when \\(I(y_{t} \\leq \\kappa)=1\\) then \\(I(y_{t} &gt; \\kappa)=0\\), and vice versa. So, either the parameters \\(\\{\\alpha_1,\\beta_{11},\\ldots,\\beta_{1p}\\}\\) or the parameters \\(\\{\\alpha_2,\\beta_{21},\\ldots,\\beta_{2p}\\}\\) are used to generate the one-step-ahead fitted value, which is then ‘disturbed’ by the bootstrap error term, \\(\\varepsilon_{t+1}^b\\), thus resulting in the one-step-ahead bootstrap forecast. Two-step-ahead bootstrap forecast: \\[\\begin{aligned} y_{t+2|t}^b &amp;= (\\alpha_1 + \\beta_{11} y_{t+1|t}^b + \\ldots + \\beta_{1p} y_{t+2-p})I(y_{t+1|t}^b \\leq \\kappa) \\\\ &amp;+ (\\alpha_2 + \\beta_{21} y_{t+1|t}^b + \\ldots + \\beta_{2p} y_{t+2-p})I(y_{t+1|t}^b &gt; \\kappa)+\\varepsilon_{t+2}^b \\end{aligned}\\] The mechanism here is similar to that described above for the one-step-ahead bootstrap forecast. The key difference is in the regime-switching variable, which in this case is \\(y_{t+1|t}^b\\), which is a function of the boostrap error term. The sign and size of this error term can, in and of itself, have a regime-switching effect. That is, in different bootstrap iterations, the boostrap error term can determine whether the parameters \\(\\{\\alpha_1,\\beta_{11},\\ldots,\\beta_{1p}\\}\\) or the parameters \\(\\{\\alpha_2,\\beta_{21},\\ldots,\\beta_{2p}\\}\\) are used to generate two-step-ahead bootstrap forecast. In general, the aforementioned peculiarity illustrated with the two-step-ahead bootstrap forecast is relevant to any \\(h\\)-step-ahead forecast, so long as \\(h&gt;d\\), where \\(d\\) is the delay factor of the regime-switching variable, \\(y_{t-d}\\). Point forecast at horizon \\(h\\) is: \\[\\bar{y}_{t+h|t} = B^{-1}\\sum_{b=1}^{B}y_{t+h|t}^b,\\] where \\(B\\) is the total number of bootstrap iterations (usually many thousands of iterations). Forecast error at horizon \\(h\\) is: \\[e_{t+h|t}=y_{t+h}-\\bar{y}_{t+h|t}.\\] Measures of forecast accuracy measures (such as RMSFE, for example) can be obtained based on this forecast error. For interval forecasts, we will need to resort to the relevant percentiles of the empirical distribution of the bootstrapped forecast paths. This is because the multi-step forecast density from nonlinear models, usually, is no longer symmetric or normally distributed. Indeed, multi-step forecast densities from regine-dependent models, such as SETAR, usually are characterized by multiple modes (instead of a single mode). There are other members of the family of regime-dependent models, e.g., a smooth transition autregression (STAR), which relax this assumption of an instantaneous switch between the regimes.↩︎ Such restriction is usually needed to ensure that there are enough observations in each of the regimes of the model.↩︎ "],["comparing-forecasts.html", "Chapter 9 – Comparing Forecasts 9.1 The Need for the Forecast Evaluation 9.2 Relative Forecast Accuracy Tests 9.3 Forecasting Year-on-Year Monthly Inflation 12-steps-ahead", " Chapter 9 – Comparing Forecasts 9.1 The Need for the Forecast Evaluation Among several available models we can select one that offers the superior in-sample fit (e.g., based on the AIC or SIC), and then apply this model to generate the forecasts. A more sensible approach, at least from the standpoint of a forecaster, would be to assess goodness of fit in an out-of-sample setting. Recall that the model that offer the best in-sample fit doesn’t necessarily produce the most accurate out-of-sample forecasts, among the considered models. That is, for example, while a better in-sample fit can be achieved by incorporating additional parameters in the model, such more complex models extrapolate the estimated parameter uncertainty into the forecasts, thus sabotaging their accuracy. Thus far we have applied the following algorithm to identify ‘the best’ among the competing forecasts: decide on a loss function (e.g., quadratic loss); obtain forecasts, the forecast errors, and the corresponding sample expected loss (e.g., root mean squared forecast error) for each model; rank the models on their sample expected loss values; select the model with the lowest sample expected loss. But the loss function is a function of the random variable, and in practice we deal with sample information, so sampling variation needs to be taken into the account. Statistical methods of evaluation are, therefore, desirable (to say it mildly). 9.2 Relative Forecast Accuracy Tests Here we will cover two tests for the hypothesis that two forecasts are equivalent, in the sense that the associated loss differential is not statistically significantly different from zero. Consider a time series of length \\(T\\). Suppose \\(h\\)-step-ahead forecasts for periods \\(R+h\\) through \\(T\\) have been generated from two competing models \\(i\\) and \\(j\\): \\(y_{t+h|t,i}\\) and \\(y_{t+h|t,j}\\), with corresponding forecast errors: \\(e_{t+h,i}\\) and \\(e_{t+h,j}\\). The null hypothesis of equal predictive ability can be given in terms of the unconditional expectation of the loss differential: \\[H_0: E\\left[d(e_{t+h,ij})\\right] = 0,\\] where \\(d(e_{t+h,ij}) = L(e_{t+h,i})-L(e_{t+h,j})\\). 9.2.1 The Morgan-Granger-Newbold Test The Morgan-Granger-Newbold (MGN) test is based on auxiliary variables: \\(u_{1,t+h} = e_{t+h,i}-e_{t+h,j}\\) and \\(u_{2,t+h} = e_{t+h,i}+e_{t+h,j}\\). It follows that: \\[E(u_{1,t+h},u_{2,t+h}) = MSFE(i,t+h)-MSFE(j,t+h).\\] Thus, the hypothesis of interest is equivalent to testing whether the two auxiliary variables are correlated. The MGN test statistic is: \\[\\frac{r}{\\sqrt{(P-1)^{-1}(1-r^2)}}\\sim t_{P-1},\\] where \\(t_{P-1}\\) is a Student t distribution with \\(P-1\\) degrees of freedom, \\(P\\) is the number of out-of-sample forecasts, and \\[r=\\frac{\\sum_{t=R}^{T-h}{u_{1,t+h}u_{2,t+h}}}{\\sqrt{\\sum_{t=R}^{T-h}{u_{1,t+h}^2}\\sum_{t=R}^{T-h}{u_{2,t+h}^2}}}\\] The MGN test relies on the assumption that forecast errors (of the forecasts to be compared) are unbiased, normally distributed, and uncorrelated (with each other). These are rather strict assumptions that are, often, violated in empirical applications. 9.2.2 The Diebold-Mariano Test The Diebold-Mariano (DM) test, put forward by Diebold and Mariano (1995), relaxes the aforementioned requirements on the forecast errors. The DM test statistic is: \\[\\frac{\\bar{d}_h}{\\sqrt{\\sigma_d^2/P}} \\sim N(0,1),\\] where \\(\\bar{d}_h=P^{-1}\\sum_{t=R}^{T-h} d(e_{t+h,ij})\\), and where \\(P=T-R-h+1\\) is the total number of forecasts generated using a rolling or recursive window scheme, for example. A modified version of the DM statistic, due to Harvey, Leybourne, and Newbold (1997), addresses the finite sample properties of the test, so that: \\[\\sqrt{\\frac{P+1-2h+P^{-1}h(h-1)}{P}}DM\\sim t_{P-1},\\] where \\(t_{P-1}\\) is a Student t distribution with \\(P-1\\) degrees of freedom. In practice, the test of equal predictive ability can be applied within the framework of a regression model as follows: \\[d_{t+h} = \\delta + \\upsilon_{t+h}\\;~~t = R,\\ldots,T-h,\\] where \\(d_{t+h} \\equiv d(e_{t+h,ij})\\). The null of equal predictive ability is equivalent of testing \\(H_0: \\delta = 0\\) in the OLS setting. Moreover, because \\(d_{t+h}\\) may be serially correlated, autocorrelation consistent standard errors should be used for inference. 9.3 Forecasting Year-on-Year Monthly Inflation 12-steps-ahead Two methods for generating multi-step-ahead forecasts from autoregressions, for example, are the iterated and direct methods. Each has advantages and disadvantages, and when the model is mis-specified, which more often than not tends to be the case, either of the two methods can yield more accurate forecasts. We will examine this using the U.S. year-on-year monthly inflation data. Figure 9.1: Iterated and Direct Forecasts The foregoing figure illustrates the forecasts, obtained from iterated and direct multi-step forecasting methods. We examine the relative out-of-sample forecast accuracy of the two methods by regressing the loss differential \\(d_t=e_{t,d}^2-e_{t,i}^2\\) on a constant, where \\(t=R+h,\\ldots,T\\), and where the subscripts \\(d\\) and \\(i\\) indicate the direct and iterated multi-step forecasting method, respectively. In this example, as it turns out, we are unable to reject the null hypothesis of equal forecast accuracy as the DM test statistic happens to be -1.18. References "],["combining-forecasts.html", "Chapter 10 – Combining Forecasts 10.1 Benefits of Forecast Combination 10.2 Optimal Weights for Forecast Combination 10.3 Forecast Encompassing", " Chapter 10 – Combining Forecasts 10.1 Benefits of Forecast Combination Every model yields forecasts that are inaccurate in their own way. Taken together, two or more models can each contribute to an accurate forecast, on average. As the joke goes: a mathematician, a physicist, and a statistician went hunting. They stumbled upon a deer. The mathematician took a shot first but missed the target to the left by a metre. The physicist gave a shot next and missed the mark to the right by a metre. Having observed this, the statistician exclaimed: “We got it!” The joke works because the first two scientists missed the target by the same distance and in an opposite direction. Had the mathematician, instead, missed the target by a few metres, or had the physicist, like the mathematician, missed the target to the right, the statistician would have been less excited about the expected outcome. In accord with this analogy, the strategy of combining forecast to achieve a superior accuracy will best work if the models generate different enough forecasts with relatively similar expected loss functions. Intuitively, a combined forecast aggregates more information or more ways of processing the information. Practically, a method of forecast combination is akin to managing portfolio risk. 10.2 Optimal Weights for Forecast Combination Forecast combination implies mixing the forecasts from different models with some proportions. In other words, a combined forecast is a weighted sum of the individual forecasts. The weights need to be selected in a way that maximizes the combined forecast accuracy, i.e., minimizes the combined forecast loss. Consider two forecasting methods (or models), \\(a\\) and \\(b\\), each respectively yielding one-step-ahead forecasts \\(y_{a,t+1|t}\\) and \\(y_{b,t+1|t}\\), and the associated forecast errors \\(e_{a,t+1} = y_{t+1}-y_{a,t+1|t}\\) and \\(e_{b,t+1} = y_{t+1}-y_{b,t+1|t}\\). A combined forecast, \\(y_{c,t+1|t}\\), is the weighted sum of these two forecasts: \\[y_{c,t+1|t} = (1-w)y_{a,t+1|t} + w y_{b,t+1|t},\\] where the usual assumption about the weights apply, i.e., \\(0 \\leq w \\leq 1\\).20 A combined forecast error is the weighted sum of the individual forecast errors: \\[e_{c,t+1} = (1-w)e_{a,t+1} + w e_{b,t+1}\\] The combined forecast error is, on average, zero (assuming that the individual forecasts are all unbiased): \\[E\\left(e_{c,t+1}\\right) = E\\left[(1-w)e_{a,t+1} + w e_{b,t+1}\\right] = 0\\] The variance of a combined forecast error is: \\[Var\\left(e_{c,t+1}\\right) = (1-w)^2 \\sigma_a^2 + w^2 \\sigma_b^2 + 2w(1-w)\\sigma_{ab},\\] where \\(\\sigma_a\\) and \\(\\sigma_b\\) are the standard deviations of the forecast errors from models \\(a\\) and \\(b\\), and \\(\\sigma_{ab}=\\rho_{ab}\\sigma_a\\sigma_b\\) is the covariance of the two forecast errors, which can also be expressed as a product of the correlation between the two forecast errors, \\(\\rho_{ab}\\), and the standard deviations of each forecast error. By taking the derivative of the combined forecast error variance, and equating it to zero, we obtain the optimal weight (which minimizes the combined forecast error variance): \\[w^* = \\frac{\\sigma_a^2-\\sigma_{ab}}{\\sigma_a^2+\\sigma_b^2-2\\sigma_{ab}}\\] A couple of features of interest become immediately apparent. First, when the two forecast errors are uncorrelated, the optimal weight assigned to the individual forecast is inversely proportional to the forecast error variance. Second, when the two forecast errors have the same variance, the optimal weight assigned to each individual forecast is equal to 0.5. Substitute \\(w^*\\) in place of \\(w\\) in the formula for variance to obtain: \\[Var\\left[e_{c,t+1}(w^*)\\right] = \\sigma_c^2(w^*) = \\frac{\\sigma_a^2\\sigma_b^2(1-\\rho^2)}{\\sigma_a^2+\\sigma_b^2-2\\rho\\sigma_a\\sigma_b}\\] It can be shown that \\(\\sigma_c^2(w^*) \\leq \\min\\{\\sigma_a^2,\\sigma_b^2\\}\\). That is to say that by combining forecasts we are not making things worse (so long as we use optimal weights). In practice \\(\\sigma_a\\), \\(\\sigma_b\\), and \\(sigma_{ab}\\) are unknown. So, we estimate these using a pseudo-forecasting routine. Specifically, the sample estimator of \\(w^*\\) is: \\[\\hat{w}^* = \\frac{\\hat{\\sigma}_i^2-\\hat{\\sigma}_{ij}}{\\hat{\\sigma}_i^2+\\hat{\\sigma}_j^2-2\\hat{\\sigma}_{ij}},\\] where \\(\\hat{\\sigma}_i^2 = \\frac{1}{P-1}\\sum_{t=R}^{R+P-1}{e_{a,t+1}^2}\\) and \\(\\hat{\\sigma}_j^2 = \\frac{1}{P-1}\\sum_{t=R}^{R+P-1}{e_{b,t+1}^2}\\) are sample forecast error variances, and \\(\\hat{\\sigma}_{ij}=\\frac{1}{P-1}\\sum_{t=R}^{R+P-1}{e_{a,t+1}e_{b,t+1}}\\) is a sample forecast error covariance, where \\(P\\) denotes the number of out-of-sample forecasts. The optimal weight has a straightforward interpretation in a regression setting. Consider the combined forecast equation as: \\[y_{t+1} = (1-w)y_{a,t+1|t} + w y_{b,t+1|t} + \\varepsilon_{t+1},\\] where \\(\\varepsilon_{t+1}\\equiv e_{c,t+1}\\). We can re-arrange the equation so that: \\[e_{a,t+1} = w (y_{b,t+1|t}-y_{a,t+1|t}) + \\varepsilon_{t+1},\\] where \\(w\\) is obtained by estimating a linear regression with an intercept restricted to zero. Alternatively, we can estimate an unconstrained variant of the combined forecast equation: \\[y_{t+1} = \\alpha+\\beta_a y_{a,t+1|t} + \\beta_b y_{b,t+1|t} + \\varepsilon_{t+1},\\] which relaxes the assumption that the forecasts are unbiased, as well as that the weights need to add up to one. Indeed, this unconstrained variant for forecast combination allows for the possibility of negative weights. 10.3 Forecast Encompassing A special case of forecast combination is when \\(w=0\\). Such an outcome (of the optimal weights) is known as forecast encompassing. It is said that \\(y_{a,t+1|t}\\) encompasses \\(y_{b,t+1|t}\\), when given that the former is available, the latter provides no additional useful information. This is equivalent of testing the null hypothesis of \\(w=0\\) in the combined forecast error equation, which, after rearranging terms, yields the following regression: \\[e_{a,t+1} = w\\left(e_{a,t+1}-e_{b,t+1}\\right)+\\varepsilon_{t+1},\\;~~t=R,\\ldots,R+P-1\\] where \\(\\varepsilon_{t+1}\\equiv e_{c,t+1}\\), and where \\(R\\) is the size of the (first) estimation window, and \\(P\\) is the number of out-of-sample forecasts generated. We can test for the forecast encompassing by regressing the realized value on individual forecasts: \\[y_{t+1} = \\alpha + \\beta_1 y_{a,t+1|t} + \\beta_2 y_{b,t+1|t} + \\varepsilon_{t+1},\\] and testing the null hypothesis that \\(\\beta_2=0\\), given that \\(\\beta_1=1\\). More broadly, if we have \\(n\\) forecasts that we intend to combine, the associated weights, \\(w_i:i=1,\\ldots,n\\), will each be bounded by zero and one, and together they will add up to one.↩︎ "],["forecasting-using-r.html", "Forecasting Using R", " Forecasting Using R R is a programming language for data analysis and visualization. Here I introduce basic commands that should facilitate your understanding of R. You can further enhance your skillset using numerous online resources,21 as well as by applying a trial–and–error approach, which has been my own time-tested routine for mastering R. To the extent that new packages and features are added to R pretty much on a daily basis, there are virtually no limits to how far you can advance your knowledge of this programming language. These notes will get you started—you can go as far as you wish. We will work in RStudio—the go-to interface for R (available from CRAN). You will need to have installed both, R and RStudio on your devise. RStudio will seamlessly ‘find’ and connect with R. There are several ‘dialects’ of R coding language. We will primarily rely on data.table, which I find intuitive. The other popular dialect is tidyverse.22 Technically, you don’t necessarily need either of these dialects to get the job done—the base R language would be just enough. But these dialects make things easier, and they are not too difficult to master anyway. such as R for Data Science↩︎ Forecasting Principles and Practice by Hyndman and Athanasopoulos relies on tidyverse, for example.↩︎ "],["tutorial-1-introduction-to-r.html", "Tutorial 1: Introduction to R Base R and matrix manipulations Estimating parameters using OLS", " Tutorial 1: Introduction to R Base R and matrix manipulations There are a number of ways in which we can work with data in R. Here, we will primarily rely on data.table, the other prominent dialect being tidyverse. But to develop an intuition, let’s start off with base R, specifically—matrices. Consider a random sequence of observations:23 a &lt;- c(1,0,4,3,2,6) a ## [1] 1 0 4 3 2 6 We used c() function to combine the set of observations. Without this function, that is if we were to just list the observations separated by commas, we would have received an error message. This sequence, unlike a vector, has no dimensions.24 But we can transform it to a \\(n \\times 1\\) vector, and call it b, using the as.matrix() function: b &lt;- as.matrix(a) b ## [,1] ## [1,] 1 ## [2,] 0 ## [3,] 4 ## [4,] 3 ## [5,] 2 ## [6,] 6 dim(b) ## [1] 6 1 The result is a \\(6 \\times 1\\) vector, or a column matrix. To obtain a \\(1 \\times 6\\) vector, or a row matrix, we transpose the aforementioned vector using the t() function: bt &lt;- t(b) bt ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 0 4 3 2 6 dim(bt) ## [1] 1 6 We can create any \\(n \\times k\\) matrix, using the matrix() function. For example, consider a \\(3 \\times 2\\) matrix: B &lt;- matrix(a,nrow=3,ncol=2) B ## [,1] [,2] ## [1,] 1 3 ## [2,] 0 2 ## [3,] 4 6 Note that we included the earlier generated sequence of six values, which we assigned to a, as the elements of this matrix. We can add column names and row names to this matrix: colnames(B) &lt;- c(&quot;c1&quot;,&quot;c2&quot;) rownames(B) &lt;- c(&quot;r1&quot;,&quot;r2&quot;,&quot;r3&quot;) B ## c1 c2 ## r1 1 3 ## r2 0 2 ## r3 4 6 If, at this point, we would like to only work with, say, the first column of the matrix, we can call it using its column number or the column name as follows: B[,&quot;c1&quot;] ## r1 r2 r3 ## 1 0 4 Note that we indicated the column name after comma within the brackets placed immediately after the matrix. In general, the syntax goes as follows M[r,c], where M is the name of the matrix (in our example B), r is the row number(s) or the row name(s), and c is the column number(s) or column names(s). So, for example, if we want to extract a matrix element, say \\(b_{3,2}\\), we can do this by entering the respective indices in the brackets: B[3,2] ## [1] 6 Matrix multiplication is done using %*% command, granted that the two matrices are compatible. For example, we obtain a product of matrix \\(B\\) and a new \\(2 \\times 1\\) vector, \\(d\\), as follows: d &lt;- as.matrix(c(5,-2)) Bd &lt;- B%*%d Bd ## [,1] ## r1 -1 ## r2 -4 ## r3 8 We can add columns (and rows) to the existing matrix using a cbind() function: c3 &lt;- c(0,1,0) D &lt;- cbind(B,c3) D ## c1 c2 c3 ## r1 1 3 0 ## r2 0 2 1 ## r3 4 6 0 We can invert a(n invertible) matrix using the solve() function: Di &lt;- solve(D) Di ## r1 r2 r3 ## c1 -1.0000000 0 0.5000000 ## c2 0.6666667 0 -0.1666667 ## c3 -1.3333333 1 0.3333333 Estimating parameters using OLS By now, we have covered enough ground to obtain the least squares estimator. For that, we will generate vectors of dependent and independent variables, and then estimate the vector of parameters. Specifically, we will generate a sequence of 200 binary variables, which will serve as our independent variable x, and then we will construct the dependent variable y using the following formula: \\(y=2+0.5x+e\\), where e is a sequence of 200 standard normal random variables. set.seed(1) x &lt;- sample(c(0,1),200,replace=T) set.seed(2) e &lt;- rnorm(200) y &lt;- 2+0.5*x+e X &lt;- cbind(1,x) b &lt;- solve(t(X)%*%X)%*%t(X)%*%y b ## [,1] ## 1.8699318 ## x 0.7639315 Note that when generating the random samples of observations, we set seeds (in two instances). We did so to ensure that the results exactly replicate every time we re-run the model. Otherwise, owing to the random sampling, the parameter estimates that I observe will differ from those that you observe and, moreover, they will differ from each other every time you re-run the code. The foregoing “do it by hand” exercise can be easily replicated using the lm() function: ols &lt;- lm(y~x) ols ## ## Call: ## lm(formula = y ~ x) ## ## Coefficients: ## (Intercept) x ## 1.8699 0.7639 We can apply the summary() function to the lm() output to see the complete summary of the regression results: summary(ols) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.44493 -0.77746 -0.06555 0.80647 2.22089 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.8699 0.1058 17.674 &lt; 2e-16 *** ## x 0.7639 0.1511 5.054 9.8e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.069 on 198 degrees of freedom ## Multiple R-squared: 0.1143, Adjusted R-squared: 0.1098 ## F-statistic: 25.55 on 1 and 198 DF, p-value: 9.8e-07 In these tutorials, we will use the ‘assignment’ operator, &lt;-, which is, for all practical purposes, equivalent to the ‘equal’ operator, =.↩︎ You don’t need to take my word for it. You can check this for yourself by running the dim(a) in your R console, for example.↩︎ "],["tutorial-2-data-management-and-visualisation.html", "Tutorial 2: Data Management and Visualisation", " Tutorial 2: Data Management and Visualisation In this tutorial, we will introduce several functions of the the data.table and ggplot2 packages, which allow data management and plotting, respectively, in an efficient and aesthetically pleasing way. These two packages are not part of the base R, so we need to load them in the beginning of each session (for the very first time, we will need to download these packages from a repository). library(data.table) library(ggplot2) One of the comparative advantages of R is in its graphing aesthetics. Currently, the best graphs are plotted via the ggplot2 package. Notably, this package requires that the data are maintained in the data.frame or the data.table format (i.e., if your data are stored as a matrix, you will need to convert it to one of the two aforementioned formats; the conversion is pretty much seamless). Let’s create a data.table object and observe its few lines: set.seed(1) x &lt;- runif(120,0,2) y &lt;- 0.2+0.7*x+rnorm(120) dt &lt;- data.table(y=y,x=x) dt ## y x ## 1: 2.9733299 0.53101733 ## 2: 0.6817335 0.74424780 ## 3: 1.6917341 1.14570673 ## 4: 1.4994931 1.81641558 ## 5: -0.2609185 0.40336386 ## --- ## 116: 0.1835826 0.02615515 ## 117: 1.9894321 1.43113213 ## 118: 2.4197029 0.20636847 ## 119: 1.8521905 0.89256870 ## 120: 2.3040499 1.28020209 So, we have generated25 a dataset that contains two variables (x which is a realization of the uniformly distributed random variable, and y which consists of a deterministic component, function of x, and of an idiosyncratic component), stored in two columns, each of length 120. To get a feel whether there is any relationship between these two variables, we can generate a simple scatterplot. ggplot(dt,aes(x=x,y=y))+ geom_point() We can augment this plot in a number of different ways. Here we change the point color, add the fitted regression line to the plot, add labels to the figure, and change the background theme: ggplot(dt,aes(x=x,y=y))+ geom_point(color=&quot;coral&quot;)+ geom_smooth(method=&quot;lm&quot;,formula=y~x,se=F,color=&quot;darkgray&quot;)+ labs(title=&quot;A scatterplot with a fitted regression line&quot;, x=&quot;Treatment Variable&quot;, y=&quot;Outcome Variable&quot;, caption=&quot;Caption: in case if needed.&quot;)+ theme_classic() As another example, let’s generate a histogram (of y from the same data): ggplot(dt,aes(x=y))+ geom_histogram(color=&quot;white&quot;,fill=&quot;coral&quot;,binwidth=.5)+ labs(title=&quot;A basic histogram&quot;)+ theme_classic() It is customary to illustrate a time series via a line plot. So, we add a date column to our dataset, and then plot y in the chronological order: dt$date &lt;- seq(from=as.Date(&quot;2000-01-01&quot;),by=&quot;month&quot;,along.with=y) ggplot(dt,aes(x=date,y=y))+ geom_line(color=&quot;coral&quot;,size=1)+ labs(title=&quot;A basic time series plot&quot;, x=&quot;Year&quot;, y=&quot;Outcome Variable&quot;)+ theme_classic() Notice that prior to sampling we set seed to some value (to 1 in this instance). We do so to ensure that we can exactly replicate the sample in the future.↩︎ "],["tutorial-3-forecasting-methods-and-routines.html", "Tutorial 3: Forecasting Methods and Routines", " Tutorial 3: Forecasting Methods and Routines In this tutorial, we will introduce ‘for loop’, and illustrate its use by generating time series as well as by generating one-step-ahead forecasts. We will also perform forecast error diagnostics. To run the code, the data.table and ggplot2 packages need to be installed and loaded. Let’s generate a random walk process, such that \\(y_{t} = y_{t-1}+e_{t}\\), where \\(e_{t} \\sim N(0,1)\\), and where \\(y_{0}=0\\), for \\(t=1,\\ldots,120\\).26 n &lt;- 120 set.seed(1) r &lt;- rnorm(n) y &lt;- rep(NA,n) y[1] &lt;- r[1] for(i in 2:n){ y[i] &lt;- y[i-1] + r[i] } Store \\(y\\) in a data.table along with some arbitrary dates to the data (e.g., suppose we deal with the monthly series beginning from January 2011). dt &lt;- data.table(y) dt$date &lt;- seq(as.Date(&quot;2011-01-01&quot;),by=&quot;month&quot;,along.with=y) Plot the realized time series using the ggplot function. ggplot(dt,aes(x=date,y=y))+ geom_line(size=1)+ labs(x=&quot;Year&quot;,y=&quot;Random Walk&quot;)+ theme_classic() Generate a sequence of one-step-ahead forecasts from January 2017 onward by simply averaging the observed time series up to the period when the forecast is made.27 dt$f &lt;- NA R &lt;- which(dt$date==as.Date(&quot;2017-01-01&quot;))-1 P &lt;- n-R for(i in 1:P){ dt$f[R+i] &lt;- mean(dt$y[1:(R+i-1)]) } Obtain the RMSFE measure the forecast. dt$e &lt;- dt$y-dt$f rmsfe &lt;- sqrt(mean(dt$e^2,na.rm=T)) rmsfe ## [1] 5.665095 Perform the forecast error diagnostics of the forecast. Zero mean of the forecast errors: \\(E(e_{t+1|t})=0\\). We test this hypothesis by regressing the forecast error on the constant, and checking whether the coefficient is statistically significantly different from zero. summary(lm(e~1,data=dt))$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.421519 0.2396999 22.61794 6.604031e-27 We reject the null, which suggests that we are consistently underestimating (in this case) the one-step-ahead forecasts. No correlation of the forecast errors with the forecasts: \\(Cov(e_{t+1|t},y_{t+1|t})=0\\). We perform this test by regressing the forecast error on the forecast, and checking whether the slope coefficient is statistically significantly different from zero. summary(lm(e~f,data=dt))$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.1437252 1.5861275 -0.0906139 0.9281928187 ## f 0.9995059 0.2822414 3.5413157 0.0009247749 We reject the null, which suggests that there is some information in the data that we do not use well enough.28 No serial correlation in one-step-ahead forecast errors: \\(Cov(e_{t+1|t},y_{t|t-1})=0\\). We perform this test by regressing the forecast error on its lag, and checking whether the slope coefficient is statistically significantly different from zero.29 dt[,`:=`(e1=shift(e))] summary(lm(e~e1,data=dt))$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.7057751 0.40681156 1.734894 8.960477e-02 ## e1 0.8661231 0.07198491 12.032009 1.164311e-15 We reject the null, which again, suggests that the method that we have chosen for forecasting is far from ideal. The following code is deliberately done inefficiently to illustrate the use of the ‘for loop’. For reference, a much more efficient code, after setting the seed, would have been y &lt;- cumsum(rnorm(n))↩︎ This is equivalent to the expanding window scheme for generating forecasts.↩︎ Because, as we know, the true data generating process is random walk, a better use of information would involve assigning all the weights to the most recent observation rather than spreading them evenly across all observations in the estimation window.↩︎ Note: first we need to generate lagged forecast errors.↩︎ "],["tutorial-4-trends.html", "Tutorial 4: Trends", " Tutorial 4: Trends In this tutorial, we will generate trending series that, moreover, also exhibit some seasonal pattern. We will apply an information criterion to select the most suitable model, and we will obtain and evaluate one-step-ahead forecasts using a rolling window procedure. To run the code, the data.table, ggplot2 and fastDummies packages need to be installed and loaded. Let’s generate a quarterly time series that follows: \\(y_{t} = 0.05t+0.0002t^2+0.7d_{1,t}+e_{t}\\), where \\(d_{1,t}=1\\) if \\(t\\) is in quarter \\(1\\), and \\(d_{1,t}=0\\) otherwise; \\(e_{t} \\sim iid~\\text{N}(0,1)\\), for \\(t=1,\\ldots,80\\). n &lt;- 80 s &lt;- 4 set.seed(1) e &lt;- rnorm(n,0,1) trend &lt;- c(1:n) dum &lt;- rep(c(1,0,0,0),n/s) y &lt;- 0.05*trend-0.0002*trend^2+.7*dum+e Store \\(y\\) and \\(trend\\) in a data.table, call it ‘dt’. Add some arbitrary dates to the data (e.g., suppose we deal with a series beginning from quarter 1 of 2001). dt &lt;- data.table(y,trend) dt$date &lt;- seq(as.Date(&quot;2001-01-01&quot;),by=&quot;quarter&quot;,along.with=y) Plot the realized time series using the ggplot function. ggplot(dt,aes(x=date,y=y))+ geom_line(size=1)+ labs(x=&quot;Year&quot;,y=&quot;Simulated Series&quot;)+ theme_classic() Add quarterly dummy variables to the data, and calculate Akaike Information Criteria (AIC) for linear and quadratic trend models with and without quarterly seasonal dummy variables (so, four models in total). Observe that the linear trend model with the seasonal component offers the best in-sample fit, based on AIC. dt$q &lt;- quarter(dt$date) dt &lt;- dummy_cols(dt,select_columns=&quot;q&quot;) reg1 &lt;- lm(y~trend,data=dt) reg2 &lt;- lm(y~trend+I(trend^2),data=dt) reg3 &lt;- lm(y~trend+q_1+q_2+q_3,data=dt) reg4 &lt;- lm(y~trend+I(trend^2)+q_1+q_2+q_3,data=dt) aic1 &lt;- log(crossprod(reg1$residuals))+2*length(reg1$coefficients)/n aic2 &lt;- log(crossprod(reg2$residuals))+2*length(reg2$coefficients)/n aic3 &lt;- log(crossprod(reg3$residuals))+2*length(reg3$coefficients)/n aic4 &lt;- log(crossprod(reg4$residuals))+2*length(reg4$coefficients)/n Generate a sequence of one-step-ahead forecasts from the linear trend model with seasonal dummy variables, using the rolling window scheme, where the first rolling window ranges from period 1 to period 60 (i.e., 75% of the data). dt$f &lt;- NA R &lt;- 60 P &lt;- n-R for(i in 1:P){ reg3 &lt;- lm(y~trend+q_1+q_2+q_3,data=dt[i:(R-1+i)]) dt$f[R+i] &lt;- reg3$coef[1]+reg3$coef[2]*dt$trend[R+i]+reg3$coef[3]*dt$q_1[R+i]+reg3$coef[4]*dt$q_2[R+i]+reg3$coef[5]*dt$q_3[R+i] } Plot the original series and the one-step-ahead forecasts on the same graph. Note, for convenience we will first ‘melt’ the data into the ‘long table’ format, then plot the data. dl &lt;- melt(dt[,.(date,y,f)],id.vars=&quot;date&quot;) ggplot(dl,aes(x=date,y=value,color=variable,linetype=variable))+ geom_line(size=1,na.rm=T)+ scale_color_manual(values=c(&quot;darkgray&quot;,&quot;coral&quot;))+ scale_linetype_manual(values=c(1,5))+ labs(x=&quot;Year&quot;,y=&quot;Trending Seasonal Series and Forecasts&quot;)+ theme_classic()+ theme(legend.title=element_blank(),legend.position=c(.15,.85)) Test the hypotheses of forecast unbiasedness, efficiency, and no serial correlation of forecast errors. dt[,`:=`(f.e=y-f)] summary(lm(e~1,data=dt))$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.1061465 0.1007144 1.053936 0.2951249 summary(lm(e~f,data=dt))$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.1966728 1.8297135 1.200556 0.2454854 ## f -0.6922787 0.5997203 -1.154336 0.2634587 dt[,`:=`(e1=shift(e))] summary(lm(e~e1,data=dt))$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.11994115 0.1029770 1.1647377 0.2477210 ## e1 -0.03933125 0.1138245 -0.3455429 0.7306285 "],["tutorial-5-seasonality.html", "Tutorial 5: Seasonality", " Tutorial 5: Seasonality "],["tutorial-6-autoregression.html", "Tutorial 6: Autoregression", " Tutorial 6: Autoregression In this tutorial, we will generate autocorrelated series, we will apply an information criterion to select a suitable autoregressive model, we will obtain and compare one-step-ahead forecasts from competing models using a rolling window procedure, and we will generate one set of multi-step forecasts to illustrate the convergence to unconditional mean of the series. To run the code, the data.table and ggplot2 packages need to be installed and loaded. Let’s generate a time series that follows an AR(2) process: \\(y_{t} = 1.2y_{t-1}-0.3y_{t-2}+e_{t}\\), where \\(e_{t} \\sim N(0,1)\\), for \\(t=1,\\ldots,180\\). n &lt;- 180 set.seed(7) e &lt;- rnorm(n,0,1) y &lt;- rep(NA,n) y[1] &lt;- e[1] y[2] &lt;- 1.2*y[1]+e[2] for(i in 3:n){ y[i] &lt;- 1.2*y[i-1]-0.3*y[i-2]+e[i] } Generate a vector of arbitrary dates (e.g., suppose we deal with a monthly series beginning from January 2006), and store these dates along with \\(y\\) in a data.table, call it ‘dt’. Then plot \\(y\\) against \\(date\\) using a ggplot function. date &lt;- seq(as.Date(&quot;2006-01-01&quot;),by=&quot;month&quot;,along.with=y) dt &lt;- data.table(date,y) ggplot(dt,aes(x=date,y=y))+ geom_line(size=1)+ labs(x=&quot;Year&quot;,y=&quot;Autocorrelated series&quot;)+ theme_classic() Generate and plot the autocorrelation function and the partial autocorrelation function for lags 1 through 18. acf_vec &lt;- c(acf(dt$y,lag.max=18,plot=F)$acf[-1]) pacf_vec &lt;- c(pacf(dt$y,lag.max=18,plot=F)$acf) sd_rho &lt;- sqrt(1/n) acf_dt &lt;- data.table(lags=1:18,acf=acf_vec,pacf=pacf_vec) ggplot(acf_dt,aes(x=lags,y=acf)) + geom_hline(yintercept=0,color=&quot;darkgray&quot;,linetype=3,size=.6) + geom_hline(yintercept=c(-1.96*sd_rho,1.96*sd_rho),color=&quot;coral&quot;,linetype=2,size=.8) + geom_segment(aes(xend=lags,yend=0),color=&quot;powderblue&quot;,size=1)+ labs(x=&quot;k&quot;,y=expression(rho[k]))+ coord_cartesian(ylim=c(-1,1))+ theme_classic() ggplot(acf_dt,aes(x=lags,y=pacf)) + geom_hline(yintercept=0,color=&quot;darkgray&quot;,linetype=3,size=.6) + geom_hline(yintercept=c(-1.96*sd_rho,1.96*sd_rho),color=&quot;coral&quot;,linetype=2,size=.8) + geom_segment(aes(xend=lags,yend=0),color=&quot;powderblue&quot;,size=1)+ labs(x=&quot;k&quot;,y=expression(pi[k]))+ coord_cartesian(ylim=c(-1,1))+ theme_classic() Calculate Akaike Information Criteria (AIC) and Schwarz Information Criteria (SIC) for AR(1), AR(2), and AR(3) models, using all observations in the series,30 to decide on the optimal lag length. dt[,`:=`(y1=shift(y),y2=shift(y,2),y3=shift(y,3))] dt &lt;- dt[complete.cases(dt)] ic_dt &lt;- data.table(k=c(1:3),AIC=NA,SIC=NA) for(i in 1:nrow(ic_dt)){ fmla &lt;- as.formula(paste(&quot;y&quot;,paste0(&quot;y&quot;,c(1:i),collapse=&quot;+&quot;),sep=&quot;~&quot;)) reg &lt;- lm(fmla,data=dt) ic_dt$AIC[i] &lt;- log(sum(reg$residuals^2))+2*(i+1)/nrow(dt) ic_dt$SIC[i] &lt;- log(sum(reg$residuals^2))+log(nrow(dt))*(i+1)/nrow(dt) } ic_dt ## k AIC SIC ## 1: 1 5.196230 5.232119 ## 2: 2 5.012482 5.066315 ## 3: 3 5.019644 5.091422 Generate a sequence of one-step-ahead forecasts from random walk, as well as AR(1), AR(2), and AR(3), using the rolling window scheme, where the first rolling window ranges from the beginning of the sample to December 2015 so that the first forecast is made for January 2016. R &lt;- which(dt$date==as.Date(&quot;2015-12-01&quot;)) P &lt;- nrow(dt)-R dt$rw &lt;- NA dt$a1 &lt;- NA dt$a2 &lt;- NA dt$a3 &lt;- NA for(i in 1:P){ dt$rw[R+i] &lt;- dt$y[R+i-1] reg1 &lt;- lm(y~y1,data=dt[i:(R+i-1)]) reg2 &lt;- lm(y~y1+y2,data=dt[i:(R+i-1)]) reg3 &lt;- lm(y~y1+y2+y3,data=dt[i:(R+i-1)]) dt$a1[R+i] &lt;- reg1$coefficients[1]+reg1$coefficients[2]*dt$y[R+i-1] dt$a2[R+i] &lt;- reg2$coefficients[1]+reg2$coefficients[2]*dt$y[R+i-1]+reg2$coefficients[3]*dt$y[R+i-2] dt$a3[R+i] &lt;- reg3$coefficients[1]+reg3$coefficients[2]*dt$y[R+i-1]+reg3$coefficients[3]*dt$y[R+i-2]+reg3$coefficients[4]*dt$y[R+i-3] } Calculate the RMSFE measures for all considered models. dt[,`:=`(e_rw=y-rw,e_a1=y-a1,e_a2=y-a2,e_a3=y-a3)] rmsfe_rw &lt;- sqrt(mean(dt$e_rw^2,na.rm=T)) rmsfe_a1 &lt;- sqrt(mean(dt$e_a1^2,na.rm=T)) rmsfe_a2 &lt;- sqrt(mean(dt$e_a2^2,na.rm=T)) rmsfe_a3 &lt;- sqrt(mean(dt$e_a3^2,na.rm=T)) rmsfe_rw ## [1] 0.9650222 rmsfe_a1 ## [1] 0.9375063 rmsfe_a2 ## [1] 0.9205311 rmsfe_a3 ## [1] 0.9395883 Using the first rolling window as the information set, generate a single set of the multi-step-ahead point forecast for the remaining out-of-sample periods using the AR(2) model. dt[,`:=`(a2_multi=y)] reg2m &lt;- lm(y~y1+y2,data=dt[1:R]) for(i in 1:P){ dt$a2_multi[R+i] &lt;- reg2m$coefficients[1]+reg2m$coefficients[2]*dt$a2_multi[R-1+i]+reg2m$coefficients[3]*dt$a2_multi[R-2+i] } dt[1:R]$a2_multi &lt;- NA ggplot(dt,aes(x=date))+ geom_line(aes(y=y),color=&quot;darkgray&quot;,size=1)+ geom_line(aes(y=a2_multi),color=&quot;coral&quot;,na.rm=T,size=1,linetype=5)+ theme_classic() Note that after generating lagged dependent variables we discard first three rows of the data to ensure that information criteria apply the same \\(T\\) across the models.↩︎ "],["tutorial-7-vector-autoregression.html", "Tutorial 7: Vector Autoregression", " Tutorial 7: Vector Autoregression In this tutorial, we will generate bivariate autocorrelated series, we will apply a system-wide information criterion to select a suitable vector autoregressive model, we will perform an in-sample test of Granger causality, we will obtain and compare one-step-ahead forecasts from competing models using a rolling window procedure, and in so doing we will investigate evidence of out-of-sample Granger causality. To run the code, the data.table, ggplot2 and MASS packages need to be installed and loaded. Let’s generate a two-dimensional vector of time series that follow a VAR(1) process of the following form: \\[\\begin{aligned} x_{1t} &amp;= 0.3 + 0.7x_{1t-1} + 0.1x_{2t-1} + \\varepsilon_{1t} \\\\ x_{2t} &amp;= -0.2 + 0.9x_{1t-1} + \\varepsilon_{2t} \\end{aligned}\\] where \\(\\mathbf{e}_{t} \\sim N(\\mathbf{0},\\Sigma)\\), and where \\(\\Sigma\\) is the covariance matrix of the residuals such that \\(Cov(\\varepsilon_{1,t},\\varepsilon_{2,t}) = 0.3\\) for all \\(t=1,\\ldots,180\\). (Note: in the code, \\(x_1\\) is denoted by \\(y\\) and \\(x_2\\) is denoted by \\(x\\)). n &lt;- 180 R &lt;- matrix(c(1,0.3,0.3,1),nrow=2,ncol=2) set.seed(1) r &lt;- mvrnorm(n,mu=c(0,0),Sigma=R) r1 &lt;- r[,1] r2 &lt;- r[,2] x1 &lt;- rep(NA,n) x2 &lt;- rep(NA,n) x1[1] &lt;- r1[1] x2[1] &lt;- r2[1] for(i in 2:n){ x1[i] &lt;- 0.3+0.7*x1[i-1]+0.1*x2[i-1]+r1[i] x2[i] &lt;- -0.2+0.9*x2[i-1]+r2[i] } Generate a vector of some arbitrary dates (e.g., suppose we deal with the monthly series beginning from January 2006), and store these along with \\(y\\) in a data.table, call it ‘dt’. date &lt;- seq(as.Date(&quot;2006-01-01&quot;),by=&quot;month&quot;,along.with=x1) dt &lt;- data.table(date,x1,x2) Plot the realized time series using a ggplot function. Before plotting, first ‘melt’ the dataset into the ‘long’ format. dl &lt;- melt(dt,id.vars=&quot;date&quot;,variable.name=&quot;series&quot;,value.name=&quot;values&quot;) ggplot(dl,aes(x=date,y=values,color=series,linetype=series))+ geom_line(size=1)+ scale_color_manual(values=c(&quot;powderblue&quot;,&quot;coral&quot;))+ labs(x=&quot;Year&quot;,y=&quot;Series&quot;)+ theme_classic() Estimate VAR(1) and VAR(2) by running regressions on each equation separately. Collect residuals and obtain system-wide AIC for each of the two models. dt[,`:=`(x11=shift(x1,1),x12=shift(x1,2),x21=shift(x2,1),x22=shift(x2,2))] # VAR(1) p &lt;- 1 k &lt;- 2 var1.x1 &lt;- lm(x1~x11+x21,data=dt) var1.x2 &lt;- lm(x2~x11+x21,data=dt) res1 &lt;- cbind(var1.x1$residuals,var1.x2$residuals) cov1 &lt;- crossprod(res1)/(nrow(dt)-(p*k^2+k)) AIC1 &lt;- log(det(cov1))+2*(p*k^2+k)/nrow(dt) # VAR(2) p &lt;- 2 k &lt;- 2 var2.x1 &lt;- lm(x1~x11+x21+x12+x22,data=dt) var2.x2 &lt;- lm(x2~x11+x21+x12+x22,data=dt) res2 &lt;- cbind(var2.x1$residuals,var2.x2$residuals) cov2 &lt;- crossprod(res2)/(nrow(dt)-(p*k^2+k)) AIC2 &lt;- log(det(cov2))+2*(p*k^2+k)/nrow(dt) AIC1 ## [1] -0.1270596 AIC2 ## [1] -0.047212 Perform tests of (in-sample) Granger causality in each of the two models. Note, in the case of VAR(1), t tests and F tests are equivalently applicable. In the case of VAR(p), i.e., when \\(p&gt;1\\), the only appropriate test is an F test for joint significance of the parameters associated with the lags of the potentially causal (in Garanger sense) variable. # VAR(1) ## t test summary(var1.x1) ## ## Call: ## lm(formula = x1 ~ x11 + x21, data = dt) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.32279 -0.63680 -0.00953 0.68826 2.63468 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.34926 0.11146 3.134 0.00202 ** ## x11 0.68903 0.05877 11.725 &lt; 2e-16 *** ## x21 0.11304 0.04509 2.507 0.01308 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.976 on 176 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.5689, Adjusted R-squared: 0.564 ## F-statistic: 116.1 on 2 and 176 DF, p-value: &lt; 2.2e-16 summary(var1.x2) ## ## Call: ## lm(formula = x2 ~ x11 + x21, data = dt) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.38491 -0.68087 0.03216 0.66109 2.74762 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.22353 0.10805 -2.069 0.040 * ## x11 0.05814 0.05697 1.021 0.309 ## x21 0.85285 0.04371 19.511 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9461 on 176 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.7536, Adjusted R-squared: 0.7508 ## F-statistic: 269.2 on 2 and 176 DF, p-value: &lt; 2.2e-16 ## F test ar1.x1 &lt;- lm(x1~x11,data=dt) ar1.x2 &lt;- lm(x2~x21,data=dt) anova(var1.x1,ar1.x1) ## Analysis of Variance Table ## ## Model 1: x1 ~ x11 + x21 ## Model 2: x1 ~ x11 ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 176 167.64 ## 2 177 173.62 -1 -5.9866 6.2852 0.01308 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(var1.x2,ar1.x2) ## Analysis of Variance Table ## ## Model 1: x2 ~ x11 + x21 ## Model 2: x2 ~ x21 ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 176 157.54 ## 2 177 158.47 -1 -0.93231 1.0416 0.3089 ## VAR(2) ### t test (no longer applicable to test GC) summary(var1.x1) ## ## Call: ## lm(formula = x1 ~ x11 + x21, data = dt) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.32279 -0.63680 -0.00953 0.68826 2.63468 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.34926 0.11146 3.134 0.00202 ** ## x11 0.68903 0.05877 11.725 &lt; 2e-16 *** ## x21 0.11304 0.04509 2.507 0.01308 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.976 on 176 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.5689, Adjusted R-squared: 0.564 ## F-statistic: 116.1 on 2 and 176 DF, p-value: &lt; 2.2e-16 summary(var1.x2) ## ## Call: ## lm(formula = x2 ~ x11 + x21, data = dt) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.38491 -0.68087 0.03216 0.66109 2.74762 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.22353 0.10805 -2.069 0.040 * ## x11 0.05814 0.05697 1.021 0.309 ## x21 0.85285 0.04371 19.511 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9461 on 176 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.7536, Adjusted R-squared: 0.7508 ## F-statistic: 269.2 on 2 and 176 DF, p-value: &lt; 2.2e-16 ### F test ar2.x1 &lt;- lm(x1~x11+x12,data=dt) ar2.x2 &lt;- lm(x2~x21+x22,data=dt) anova(var2.x1,ar2.x1) ## Analysis of Variance Table ## ## Model 1: x1 ~ x11 + x21 + x12 + x22 ## Model 2: x1 ~ x11 + x12 ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 173 166.68 ## 2 175 172.78 -2 -6.0971 3.1641 0.04471 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(var2.x2,ar2.x2) ## Analysis of Variance Table ## ## Model 1: x2 ~ x11 + x21 + x12 + x22 ## Model 2: x2 ~ x21 + x22 ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 173 156.76 ## 2 175 158.03 -2 -1.2729 0.7024 0.4968 Generate a sequence of one-step-ahead forecasts from VAR(1) using the rolling window scheme, where the first rolling window ranges from period 1 to period 120. R &lt;- 120 P &lt;- nrow(dt)-R dt$f.a11 &lt;- NA dt$f.a12 &lt;- NA dt$f.v11 &lt;- NA dt$f.v12 &lt;- NA for(i in 1:P){ ar1.x1 &lt;- lm(x1~x11,data=dt[i:(R-1+i)]) ar1.x2 &lt;- lm(x2~x21,data=dt[i:(R-1+i)]) var1.x1 &lt;- lm(x1~x11+x21,data=dt[i:(R-1+i)]) var1.x2 &lt;- lm(x2~x11+x21,data=dt[i:(R-1+i)]) dt$f.a11[R+i] &lt;- ar1.x1$coefficients[&quot;(Intercept)&quot;]+ar1.x1$coefficients[&quot;x11&quot;]*dt$x1[R-1+i] dt$f.a12[R+i] &lt;- ar1.x2$coefficients[&quot;(Intercept)&quot;]+ar1.x2$coefficients[&quot;x21&quot;]*dt$x2[R-1+i] dt$f.v11[R+i] &lt;- var1.x1$coefficients[&quot;(Intercept)&quot;]+var1.x1$coefficients[&quot;x11&quot;]*dt$x1[R-1+i]+var1.x1$coefficients[&quot;x21&quot;]*dt$x2[R-1+i] dt$f.v12[R+i] &lt;- var1.x2$coefficients[&quot;(Intercept)&quot;]+var1.x2$coefficients[&quot;x11&quot;]*dt$x1[R-1+i]+var1.x2$coefficients[&quot;x21&quot;]*dt$x2[R-1+i] } Calculate the RMSFE measures for restricted and unrestricted models, and compare those to each other to make a suggestion about out-of-sample Granger causality. dt[,`:=`(e.a11=x1-f.a11,e.a12=x2-f.a12,e.v11=x1-f.v11,e.v12=x2-f.v12)] # calculate RMSFEs for restricted and unrestricted models rmsfe_x1.r &lt;- sqrt(mean(dt$e.a11^2,na.rm=T)) rmsfe_x1.u &lt;- sqrt(mean(dt$e.v11^2,na.rm=T)) rmsfe_x2.r &lt;- sqrt(mean(dt$e.a12^2,na.rm=T)) rmsfe_x2.u &lt;- sqrt(mean(dt$e.v12^2,na.rm=T)) rmsfe_x1.r ## [1] 1.134867 rmsfe_x1.u ## [1] 1.104268 rmsfe_x2.r ## [1] 1.00908 rmsfe_x2.u ## [1] 1.011653 "],["tutorial-8-threshold-autoregression.html", "Tutorial 8: Threshold Autoregression", " Tutorial 8: Threshold Autoregression In this tutorial, we will generate regime-dependent series, we will apply a grid-search method to obtain the threshold parameter, we will obtain and compare one-step-ahead forecasts from competing models using a rolling window procedure, and we will apply bootstrap resampling method to generate multi-step-ahead forecasts from a threshold regression. To run the code, the data.table and ggplot2 packages need to be installed and loaded. Let’s generate a time series that follow a TAR(2) process of the following form: \\[y_t = \\left\\{\\begin{array} {ll} 0.9y_{t-1} + \\varepsilon_t &amp; \\text{if}~~y_{t-1}\\ge0 \\\\ 1.2y_{t-1}-0.5y_{t-2} + \\varepsilon_t &amp; \\text{if}~~y_{t-1} &lt; 0 \\end{array}\\right. \\] where \\(e_{t} \\sim N(0,\\sigma^2)\\). This suggests that the dynamics of the time series change depending on the sign of the lagged dependent variable. n &lt;- 360 set.seed(8) e &lt;- rnorm(n,0,1) y &lt;- rep(NA,n) y[1] &lt;- e[1] if(y[1]&gt;=0){ y[2] &lt;- 0.9*y[1]+e[2] }else{ y[2] &lt;- 1.2*y[1]+e[2] } for(i in 3:n){ if(y[i-1]&gt;=0){ y[i] &lt;- 0.9*y[i-1]+e[i] }else{ y[i] &lt;- 1.2*y[i-1]-0.5*y[i-2]+e[i] } } Generate a vector of some arbitrary dates (e.g., suppose we deal with the monthly series beginning from January 1991), and store these along with \\(y\\) in a data.table, call it ‘dt’. Then plot the realized time series using ggplot function. date &lt;- seq(as.Date(&quot;1991-01-01&quot;),by=&quot;month&quot;,along.with=y) dt &lt;- data.table(date,y) ggplot(dt,aes(x=date,y=y))+ geom_line(size=1)+ labs(x=&quot;Year&quot;,y=&quot;Series&quot;)+ theme_classic() Obtain the AIC and SIC for AR(p), \\(p=1,\\ldots,4\\); decide on the optimal lag length based on AIC. dt[,`:=`(y1=shift(y),y2=shift(y,2),y3=shift(y,3),y4=shift(y,4))] dt &lt;- dt[complete.cases(dt)] ic_dt &lt;- data.table(k=c(1:4),AIC=as.numeric(NA),SIC=as.numeric(NA)) for(i in 1:nrow(ic_dt)){ fmla &lt;- as.formula(paste(&quot;y&quot;,paste0(&quot;y&quot;,c(1:i),collapse=&quot;+&quot;),sep=&quot;~&quot;)) reg_ar &lt;- lm(fmla,data=dt) ic_dt$AIC[i] &lt;- log(crossprod(reg_ar$residuals))+2*(i+1)/nrow(dt) ic_dt$SIC[i] &lt;- log(crossprod(reg_ar$residuals))+log(nrow(dt))*(i+1)/nrow(dt) } ic_dt ## k AIC SIC ## 1: 1 6.030794 6.052564 ## 2: 2 5.987318 6.019972 ## 3: 3 5.989787 6.033326 ## 4: 4 5.994844 6.049268 We now need to guess the threshold parameter (i.e., the value at which the switch between the regimes happens). For that, we perform a grid-search routine. We will consider a range of candidate thresholds that are within 10th and 90th percentiles of the lagged dependent variable. For each candidate threshold, we will run an OLS and calculate the residual sums of squares. A threshold parameter that yields the smallest residual sum of squares will be the estimate. Plot the residual sums of squares against the candidate threshold parameters to illustrate the optimal threshold selection routine. qy &lt;- round(quantile(dt$y,c(.1,.9)),1) tr &lt;- seq(qy[1],qy[2],by=.1) grid_dt &lt;- data.table(tr,ssr=NA) grid_dt[,`:=`(ssr=as.numeric(ssr))] for(i in tr){ dt[,`:=`(d=ifelse(y1&gt;=i,1,0))] tar &lt;- lm(y~(y1+y2):I(d)+(y1+y2):I(1-d),data=dt) grid_dt[tr==i]$ssr &lt;- crossprod(tar$residuals) } tr_hat &lt;- grid_dt[ssr==min(ssr)]$tr tr_hat ## [1] -0.1 ggplot(grid_dt,aes(x=tr,y=ssr))+ geom_line(size=1)+ geom_vline(xintercept=tr_hat,color=&quot;coral&quot;,linetype=5)+ labs(x=&quot;Threshold&quot;,y=&quot;RSS&quot;)+ theme_classic() Estimate the threshold autoregression and compare the parameter estimates with the true parameters of the model. dt[,`:=`(d=ifelse(y1&gt;=tr_hat,1,0))] tar &lt;- lm(y~(y1+y2):I(d)+(y1+y2):I(1-d),data=dt) summary(tar) ## ## Call: ## lm(formula = y ~ (y1 + y2):I(d) + (y1 + y2):I(1 - d), data = dt) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.07702 -0.69964 0.01969 0.65595 2.47130 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.01150 0.08554 0.134 0.893 ## y1:I(d) 0.70886 0.08895 7.969 2.26e-14 *** ## y2:I(d) 0.06424 0.07576 0.848 0.397 ## y1:I(1 - d) 1.15232 0.07632 15.099 &lt; 2e-16 *** ## y2:I(1 - d) -0.45200 0.06818 -6.630 1.27e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.02 on 351 degrees of freedom ## Multiple R-squared: 0.6447, Adjusted R-squared: 0.6407 ## F-statistic: 159.3 on 4 and 351 DF, p-value: &lt; 2.2e-16 Generate a sequence of one-step-ahead forecasts from TAR(2) using the rolling window scheme, where the first rolling window ranges from period 1 to period 240. For comparison, also generate the one-step-ahead forecasts from the AR(2) and the random walk models. Calculate the RMSFE measures for the three models. R &lt;- 240 P &lt;- nrow(dt)-R dt[,`:=`(rw=as.numeric(NA),ar=as.numeric(NA),tar=as.numeric(NA))] for(i in 1:P){ ar &lt;- lm(y~y1+y2,data=dt[i:(R-1+i)]) tar &lt;- lm(y~(y1+y2):I(d)+(y1+y2):I(1-d),data=dt[i:(R-1+i)]) dt$rw[R+i] &lt;- dt$y[R-1+i] dt$ar[R+i] &lt;- ar$coefficients[1]+ar$coefficients[2]*dt$y[R-1+i]+ar$coefficients[3]*dt$y[R-2+i] if(dt$y[R-1+i]&gt;=0){ dt$tar[R+i] &lt;- tar$coefficients[1]+tar$coefficients[2]*dt$y[R-1+i]+tar$coefficients[3]*dt$y[R-2+i] }else{ dt$tar[R+i] &lt;- tar$coefficients[1]+tar$coefficients[4]*dt$y[R-1+i]+tar$coefficients[5]*dt$y[R-2+i] } } dt[,`:=`(rw_e=y-rw,ar_e=y-ar,tar_e=y-tar)] rmsfe_rw &lt;- sqrt(mean(dt$rw_e^2,na.rm=T)) rmsfe_ar &lt;- sqrt(mean(dt$ar_e^2,na.rm=T)) rmsfe_tar &lt;- sqrt(mean(dt$tar_e^2,na.rm=T)) rmsfe_rw ## [1] 1.091629 rmsfe_ar ## [1] 1.046419 rmsfe_tar ## [1] 1.024177 Obtain the multi-step-ahead forecasts from period 241 onward using the so-called ‘skeleton extrapolation’ method (which yields biased forecasts) and the bootstrap resampling method (a numerical method that yields valid multi-step-ahead forecasts from nonlinear models). Plot the two forecasts along with the time series. dt[,`:=`(tar_skeleton=y)] tar &lt;- lm(y~(y1+y2):I(d)+(y1+y2):I(1-d),data=dt[1:R]) for(i in 1:P){ if(dt$tar_skeleton[R-1+i]&gt;=0){ dt$tar_skeleton[R+i] &lt;- tar$coefficients[1]+tar$coefficients[2]*dt$tar_skeleton[R-1+i]+tar$coefficients[3]*dt$tar_skeleton[R-2+i] }else{ dt$tar_skeleton[R+i] &lt;- tar$coefficients[1]+tar$coefficients[4]*dt$tar_skeleton[R-1+i]+tar$coefficients[5]*dt$tar_skeleton[R-2+i] } } dt[1:R]$tar_skeleton &lt;- NA B &lt;- 5000 # the number of bootstrap simulations boot_mat &lt;- replicate(B,dt$y) for(b in 1:B){ eps &lt;- sample(tar$residuals,P,replace=T) for(i in 1:P){ if(boot_mat[R-1+i,b]&gt;=0){ boot_mat[R+i,b] &lt;- tar$coefficients[1]+tar$coefficients[2]*boot_mat[R-1+i,b]+tar$coefficients[3]*boot_mat[R-2+i,b]+eps[i] }else{ boot_mat[R+i,b] &lt;- tar$coefficients[1]+tar$coefficients[4]*boot_mat[R-1+i,b]+tar$coefficients[5]*boot_mat[R-2+i,b]+eps[i] } } } dt$tar_boot &lt;- rowMeans(boot_mat) dt[1:R]$tar_boot &lt;- NA sub_lg &lt;- melt(dt[,.(date,y,tar_skeleton,tar_boot)],id.vars=&quot;date&quot;) ggplot(sub_lg,aes(x=date,y=value,color=variable,linetype=variable))+ geom_line(size=1,na.rm=T)+ scale_color_manual(values=c(&quot;darkgray&quot;,&quot;coral&quot;,&quot;steelblue&quot;))+ scale_linetype_manual(values=c(1,2,5))+ labs(x=&quot;Year&quot;,y=&quot;Series&quot;)+ theme_classic()+ theme(legend.position=&quot;top&quot;,legend.title=element_blank()) "],["tutorial-9-comparing-forecasts.html", "Tutorial 9: Comparing Forecasts", " Tutorial 9: Comparing Forecasts In this tutorial, we will generate a time series, we will obtain one-step-ahead forecasts from competing models using a rolling window procedure, and we will perform the Diebold-Mariano type regression-based test for equal predictive ability of the competing models. To run the code, the data.table, ggplot2, lmtest, and sandwich packages need to be installed and loaded. library(data.table) library(ggplot2) library(lmtest) library(sandwich) Let’s generate a time series that follow an AR(2) process of the following form: \\[y_t = 0.2+1.1y_{t-1}-0.3y_{t-2}+\\varepsilon_t\\] where \\(e_{t} \\sim N(0,\\sigma^2)\\). n &lt;- 240 set.seed(6) e &lt;- rnorm(n,0,1) y &lt;- rep(NA,n) y[1] &lt;- 0.2+e[1] y[2] &lt;- 0.2+1.1*y[1]+e[2] for(i in 3:n){ y[i] &lt;- 0.2+1.1*y[i-1]-0.3*y[i-2]+e[i] } Generate a vector of some arbitrary dates (e.g., suppose we deal with the monthly series beginning from January 2000), store these along with \\(y\\) in a data.table, call it ‘dt’, and plot the realized time series using ggplot function. date &lt;- seq(as.Date(&quot;2000-01-01&quot;),by=&quot;month&quot;,along.with=y) dt &lt;- data.table(date,y) ggplot(dt,aes(x=date,y=y))+ geom_line(size=1)+ labs(x=&quot;Year&quot;,y=&quot;Series&quot;)+ theme_classic() Suppose the candidate models are AR(1), AR(2), and AR(3), and that we want to compare forecasts obtained from these models to those from a random walk process. Generate a sequence of one-step-ahead forecasts using the rolling window scheme, where the first rolling window ranges from period 1 to period 180. Calculate the RMSFE measures for the canddidate models. dt[,`:=`(y1=shift(y,1),y2=shift(y,2),y3=shift(y,3))] R &lt;- 180 P &lt;- nrow(dt)-R dt[,`:=`(rw=as.numeric(NA),a1=as.numeric(NA),a2=as.numeric(NA),a3=as.numeric(NA))] for(i in 1:P){ dt$rw[R+i] &lt;- dt$y[R+i-1] ar1 &lt;- lm(y~y1,data=dt[i:(R+i-1)]) ar2 &lt;- lm(y~y1+y2,data=dt[i:(R+i-1)]) ar3 &lt;- lm(y~y1+y2+y3,data=dt[i:(R+i-1)]) dt$a1[R+i] &lt;- ar1$coefficients%*%as.numeric(c(1,dt[R+i,c(&quot;y1&quot;)])) dt$a2[R+i] &lt;- ar2$coefficients%*%as.numeric(c(1,dt[R+i,c(&quot;y1&quot;,&quot;y2&quot;)])) dt$a3[R+i] &lt;- ar3$coefficients%*%as.numeric(c(1,dt[R+i,c(&quot;y1&quot;,&quot;y2&quot;,&quot;y3&quot;)])) } dt$rw_e &lt;- dt$y-dt$rw dt$a1_e &lt;- dt$y-dt$a1 dt$a2_e &lt;- dt$y-dt$a2 dt$a3_e &lt;- dt$y-dt$a3 # RMSFEs sqrt(mean(dt$rw_e^2,na.rm=T)) ## [1] 0.9653331 sqrt(mean(dt$a1_e^2,na.rm=T)) ## [1] 0.9053279 sqrt(mean(dt$a2_e^2,na.rm=T)) ## [1] 0.8842908 sqrt(mean(dt$a3_e^2,na.rm=T)) ## [1] 0.8877883 Do the autoregressive models generate ‘statistically significantly’ more accurate forecasts than the random walk model? We will answer this question by performing the regression-based Diebold-Mariano tests. First we will generate the loss differentials; then we will run three separate regressions to assess predictive accuracy of AR(1), AR(2), and AR(3) relative to the random walk; and finally we will base our decision on the heteroskedasticity and autocorrelation consistent standard errors. dt$ld1 &lt;- dt$rw_e^2-dt$a1_e^2 dt$ld2 &lt;- dt$rw_e^2-dt$a2_e^2 dt$ld3 &lt;- dt$rw_e^2-dt$a3_e^2 reg_ld1 &lt;- lm(ld1~1,data=dt) reg_ld2 &lt;- lm(ld2~1,data=dt) reg_ld3 &lt;- lm(ld3~1,data=dt) lmtest::coeftest(reg_ld1,vcov.=sandwich::vcovHAC(reg_ld1)) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.112249 0.047525 2.3619 0.0215 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 lmtest::coeftest(reg_ld2,vcov.=sandwich::vcovHAC(reg_ld2)) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.149898 0.083086 1.8041 0.07632 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 lmtest::coeftest(reg_ld3,vcov.=sandwich::vcovHAC(reg_ld3)) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.143700 0.081309 1.7673 0.08235 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 "],["tutorial-10-combining-forecasts.html", "Tutorial 10: Combining Forecasts", " Tutorial 10: Combining Forecasts In this tutorial, we will generate a time series, we will obtain one-step-ahead forecasts from a set of models using a rolling window procedure, we will combine these forecasts and assess the accuracy of the combined forecast. To run the code, the data.table, ggplot2, lmtest, and sandwich packages need to be installed and loaded. library(data.table) library(ggplot2) library(lmtest) library(sandwich) Let’s generate a time series that follow an AR(2) process with the linear trend component as follows: \\[y_t = 0.02t-0.0001t^2+1.2y_{t-1}-0.5y_{t-2}+\\varepsilon_t\\] where \\(e_{t} \\sim N(0,\\sigma^2)\\). n &lt;- 180 set.seed(8) e &lt;- rnorm(n,0,1) y &lt;- rep(NA,n) y[1] &lt;- 0.02*1-0.0001*(1)^2+e[1] y[2] &lt;- 0.02*2-0.0001*(2)^2+1.2*y[1]+e[2] for(i in 3:n){ y[i] &lt;- 0.02*i-0.0001*(i)^2+1.2*y[i-1]-0.5*y[i-2]+e[i] } Generate a vector of some arbitrary dates (e.g., suppose we deal with the monthly series beginning from January 2000), store these along with \\(y\\) in a data.table, call it ‘dt’, and plot the realized time series using ggplot function. date &lt;- seq(as.Date(&quot;2000-01-01&quot;),by=&quot;month&quot;,along.with=y) dt &lt;- data.table(date,y) ggplot(dt,aes(x=date,y=y))+ geom_line(size=1)+ labs(x=&quot;Year&quot;,y=&quot;Series&quot;)+ theme_classic() Suppose the candidate models are AR(1), AR(2), and AR(1) with linear trend, and that we want to compare forecasts obtained from these models to those from a random walk process. Generate a sequence of one-step-ahead forecasts using the rolling window scheme, where the size of the rolling window is approximately 75% of the length of the series. dt[,`:=`(y1=shift(y,1),y2=shift(y,2),y3=shift(y,3),trend=c(1:nrow(dt)))] R &lt;- round(.75*nrow(dt)) P &lt;- nrow(dt)-R dt[,`:=`(rw=as.numeric(NA),a1=as.numeric(NA),a2=as.numeric(NA),tr=as.numeric(NA))] for(i in 1:P){ dt$rw[R+i] &lt;- dt$y[R+i-1] a1 &lt;- lm(y~y1,data=dt[i:(R+i-1)]) a2 &lt;- lm(y~y1+y2,data=dt[i:(R+i-1)]) tr &lt;- lm(y~y1+trend,data=dt[i:(R+i-1)]) dt$a1[R+i] &lt;- a1$coefficients%*%as.numeric(c(1,dt[R+i,c(&quot;y1&quot;)])) dt$a2[R+i] &lt;- a2$coefficients%*%as.numeric(c(1,dt[R+i,c(&quot;y1&quot;,&quot;y2&quot;)])) dt$tr[R+i] &lt;- tr$coefficients%*%as.numeric(c(1,dt[R+i,c(&quot;y1&quot;,&quot;trend&quot;)])) } Does either of the considered models ‘statistically significantly’ outperform the random walk? dt[,`:=`(e_rw=y-rw,e_a1=y-a1,e_a2=y-a2,e_tr=y-tr)] dt[,`:=`(ld1=e_rw^2-e_a1^2,ld2=e_rw^2-e_a2^2,ldt=e_rw^2-e_tr^2)] reg_ld1 &lt;- lm(ld1~1,data=dt) reg_ld2 &lt;- lm(ld2~1,data=dt) reg_ldt &lt;- lm(ldt~1,data=dt) lmtest::coeftest(reg_ld1,vcov.=sandwich::vcovHAC(reg_ld1)) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.23744 0.16521 1.4372 0.1577 lmtest::coeftest(reg_ld2,vcov.=sandwich::vcovHAC(reg_ld2)) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.49699 0.27820 1.7865 0.08091 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 lmtest::coeftest(reg_ldt,vcov.=sandwich::vcovHAC(reg_ldt)) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.27481 0.14690 1.8707 0.06805 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 All of the models, on average, generate more accurate forecasts than random walk. None of the models generate statistically significantly (at 5% significance level) more accurate forecast than random walk, based on Diebold-Mariano test applied on quadratic loss function. Might each model contain some useful information for improving forecast accuracy? Let’s combine the forecasts from the AR(2) and the AR(1) with linear trend using the equal weights scheme and assess the relative accuracy of the combined forecast. dt$co &lt;- dt$a2*.5+dt$tr*.5 dt$e_co &lt;- dt$y-dt$co dt$ldc &lt;- dt$e_rw^2-dt$e_co^2 reg_ldc &lt;- lm(ldc~1,data=dt) lmtest::coeftest(reg_ldc,vcov.=sandwich::vcovHAC(reg_ldc)) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.48226 0.22397 2.1532 0.03682 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
