<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Linear Autoregression | Introduction to Time Series Modeling and Forecasting with Applications in R</title>
  <meta name="description" content="Chapter 6 Linear Autoregression | Introduction to Time Series Modeling and Forecasting with Applications in R" />
  <meta name="generator" content="bookdown 0.25 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Linear Autoregression | Introduction to Time Series Modeling and Forecasting with Applications in R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Chapter 6 Linear Autoregression | Introduction to Time Series Modeling and Forecasting with Applications in R" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Linear Autoregression | Introduction to Time Series Modeling and Forecasting with Applications in R" />
  
  <meta name="twitter:description" content="Chapter 6 Linear Autoregression | Introduction to Time Series Modeling and Forecasting with Applications in R" />
  

<meta name="author" content="David Ubilava" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="dynamic-time-series-models.html"/>
<link rel="next" href="vector-autoregression.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Time Series Modeling and Forecasting</a></li>

<li class="divider"></li>
<li><a href="index.html#foreword">Foreword<span></span></a></li>
<li><a href="preliminaries.html#preliminaries">Preliminaries<span></span></a></li>
<li class="chapter" data-level="1" data-path="introduction-to-forecasting.html"><a href="introduction-to-forecasting.html"><i class="fa fa-check"></i><b>1</b> Introduction to Forecasting<span></span></a></li>
<li class="chapter" data-level="2" data-path="stochastic-process-and-time-series.html"><a href="stochastic-process-and-time-series.html"><i class="fa fa-check"></i><b>2</b> Stochastic Process and Time Series<span></span></a>
<ul>
<li class="chapter" data-level="2.1" data-path="stochastic-process-and-time-series.html"><a href="stochastic-process-and-time-series.html#stationarity"><i class="fa fa-check"></i><b>2.1</b> Stationarity<span></span></a></li>
<li class="chapter" data-level="2.2" data-path="stochastic-process-and-time-series.html"><a href="stochastic-process-and-time-series.html#serial-dependence"><i class="fa fa-check"></i><b>2.2</b> Serial Dependence<span></span></a></li>
<li class="chapter" data-level="2.3" data-path="stochastic-process-and-time-series.html"><a href="stochastic-process-and-time-series.html#transformations"><i class="fa fa-check"></i><b>2.3</b> Transformations<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="forecasting-methods-and-routines.html"><a href="forecasting-methods-and-routines.html"><i class="fa fa-check"></i><b>3</b> Forecasting Methods and Routines<span></span></a>
<ul>
<li class="chapter" data-level="3.1" data-path="forecasting-methods-and-routines.html"><a href="forecasting-methods-and-routines.html#optimal-forecast"><i class="fa fa-check"></i><b>3.1</b> Optimal Forecast<span></span></a></li>
<li class="chapter" data-level="3.2" data-path="forecasting-methods-and-routines.html"><a href="forecasting-methods-and-routines.html#measuring-forecast-accuracy"><i class="fa fa-check"></i><b>3.2</b> Measuring Forecast Accuracy<span></span></a></li>
<li class="chapter" data-level="3.3" data-path="forecasting-methods-and-routines.html"><a href="forecasting-methods-and-routines.html#evaluating-time-series-forecasts"><i class="fa fa-check"></i><b>3.3</b> Evaluating Time Series Forecasts<span></span></a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="forecasting-methods-and-routines.html"><a href="forecasting-methods-and-routines.html#unbiasedness"><i class="fa fa-check"></i><b>3.3.1</b> Unbiasedness<span></span></a></li>
<li class="chapter" data-level="3.3.2" data-path="forecasting-methods-and-routines.html"><a href="forecasting-methods-and-routines.html#efficiency"><i class="fa fa-check"></i><b>3.3.2</b> Efficiency<span></span></a></li>
<li class="chapter" data-level="3.3.3" data-path="forecasting-methods-and-routines.html"><a href="forecasting-methods-and-routines.html#no-autocorrelation"><i class="fa fa-check"></i><b>3.3.3</b> No Autocorrelation<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="deterministic-time-series-models.html#deterministic-time-series-models">Deterministic Time Series Models<span></span></a></li>
<li class="chapter" data-level="4" data-path="trends.html"><a href="trends.html"><i class="fa fa-check"></i><b>4</b> Trends<span></span></a>
<ul>
<li class="chapter" data-level="4.1" data-path="trends.html"><a href="trends.html#spurious-relationship"><i class="fa fa-check"></i><b>4.1</b> Spurious Relationship<span></span></a></li>
<li class="chapter" data-level="4.2" data-path="trends.html"><a href="trends.html#modeling"><i class="fa fa-check"></i><b>4.2</b> Modeling<span></span></a></li>
<li class="chapter" data-level="4.3" data-path="trends.html"><a href="trends.html#forecasting"><i class="fa fa-check"></i><b>4.3</b> Forecasting<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="seasonality.html"><a href="seasonality.html"><i class="fa fa-check"></i><b>5</b> Seasonality<span></span></a>
<ul>
<li class="chapter" data-level="5.1" data-path="seasonality.html"><a href="seasonality.html#modeling-1"><i class="fa fa-check"></i><b>5.1</b> Modeling<span></span></a></li>
<li class="chapter" data-level="5.2" data-path="seasonality.html"><a href="seasonality.html#forecasting-1"><i class="fa fa-check"></i><b>5.2</b> Forecasting<span></span></a></li>
</ul></li>
<li><a href="dynamic-time-series-models.html#dynamic-time-series-models">Dynamic Time Series Models<span></span></a></li>
<li class="chapter" data-level="6" data-path="linear-autoregression.html"><a href="linear-autoregression.html"><i class="fa fa-check"></i><b>6</b> Linear Autoregression<span></span></a>
<ul>
<li class="chapter" data-level="6.1" data-path="linear-autoregression.html"><a href="linear-autoregression.html#modeling-2"><i class="fa fa-check"></i><b>6.1</b> Modeling<span></span></a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="linear-autoregression.html"><a href="linear-autoregression.html#ar1"><i class="fa fa-check"></i><b>6.1.1</b> AR(1)<span></span></a></li>
<li class="chapter" data-level="6.1.2" data-path="linear-autoregression.html"><a href="linear-autoregression.html#ar2"><i class="fa fa-check"></i><b>6.1.2</b> AR(2)<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="linear-autoregression.html"><a href="linear-autoregression.html#forecasting-2"><i class="fa fa-check"></i><b>6.2</b> Forecasting<span></span></a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="linear-autoregression.html"><a href="linear-autoregression.html#one-step-ahead-forecast-from-ar1"><i class="fa fa-check"></i><b>6.2.1</b> One-step-ahead forecast from AR(1)<span></span></a></li>
<li class="chapter" data-level="6.2.2" data-path="linear-autoregression.html"><a href="linear-autoregression.html#two-step-ahead-forecast-from-ar1"><i class="fa fa-check"></i><b>6.2.2</b> Two-step-ahead forecast from AR(1)<span></span></a></li>
<li class="chapter" data-level="6.2.3" data-path="linear-autoregression.html"><a href="linear-autoregression.html#h-step-ahead-forecast-from-ar1"><i class="fa fa-check"></i><b>6.2.3</b> h-step-ahead forecast from AR(1)<span></span></a></li>
<li class="chapter" data-level="6.2.4" data-path="linear-autoregression.html"><a href="linear-autoregression.html#one-step-ahead-forecast-from-ar2"><i class="fa fa-check"></i><b>6.2.4</b> One-step-ahead forecast from AR(2)<span></span></a></li>
<li class="chapter" data-level="6.2.5" data-path="linear-autoregression.html"><a href="linear-autoregression.html#two-step-ahead-forecast-from-ar2"><i class="fa fa-check"></i><b>6.2.5</b> Two-step-ahead forecast from AR(2)<span></span></a></li>
<li class="chapter" data-level="6.2.6" data-path="linear-autoregression.html"><a href="linear-autoregression.html#one-step-ahead-forecast-from-ar2-1"><i class="fa fa-check"></i><b>6.2.6</b> One-step-ahead forecast from AR(2)<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="vector-autoregression.html"><a href="vector-autoregression.html"><i class="fa fa-check"></i><b>7</b> Vector Autoregression<span></span></a>
<ul>
<li class="chapter" data-level="7.1" data-path="vector-autoregression.html"><a href="vector-autoregression.html#modeling-3"><i class="fa fa-check"></i><b>7.1</b> Modeling<span></span></a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="vector-autoregression.html"><a href="vector-autoregression.html#in-sample-granger-causality"><i class="fa fa-check"></i><b>7.1.1</b> In-Sample Granger Causality<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="vector-autoregression.html"><a href="vector-autoregression.html#forecasting-3"><i class="fa fa-check"></i><b>7.2</b> Forecasting<span></span></a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="vector-autoregression.html"><a href="vector-autoregression.html#one-step-ahead-forecast-from-bivariate-var1"><i class="fa fa-check"></i><b>7.2.1</b> One-step-ahead forecast from bivariate VAR(1)<span></span></a></li>
<li class="chapter" data-level="7.2.2" data-path="vector-autoregression.html"><a href="vector-autoregression.html#two-step-ahead-forecast-from-bivariate-var1"><i class="fa fa-check"></i><b>7.2.2</b> Two-step-ahead forecast from bivariate VAR(1)<span></span></a></li>
<li class="chapter" data-level="7.2.3" data-path="vector-autoregression.html"><a href="vector-autoregression.html#out-of-sample-granger-causality"><i class="fa fa-check"></i><b>7.2.3</b> Out-of-Sample Granger Causality<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="threshold-autoregression.html"><a href="threshold-autoregression.html"><i class="fa fa-check"></i><b>8</b> Threshold Autoregression<span></span></a>
<ul>
<li class="chapter" data-level="8.1" data-path="threshold-autoregression.html"><a href="threshold-autoregression.html#nonlinear-models"><i class="fa fa-check"></i><b>8.1</b> Nonlinear Models<span></span></a></li>
<li class="chapter" data-level="8.2" data-path="threshold-autoregression.html"><a href="threshold-autoregression.html#modeling-4"><i class="fa fa-check"></i><b>8.2</b> Modeling<span></span></a></li>
<li class="chapter" data-level="8.3" data-path="threshold-autoregression.html"><a href="threshold-autoregression.html#forecasting-4"><i class="fa fa-check"></i><b>8.3</b> Forecasting<span></span></a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="threshold-autoregression.html"><a href="threshold-autoregression.html#skeleton-extrapolation"><i class="fa fa-check"></i><b>8.3.1</b> Skeleton Extrapolation<span></span></a></li>
<li class="chapter" data-level="8.3.2" data-path="threshold-autoregression.html"><a href="threshold-autoregression.html#analytical-method"><i class="fa fa-check"></i><b>8.3.2</b> Analytical Method<span></span></a></li>
<li class="chapter" data-level="8.3.3" data-path="threshold-autoregression.html"><a href="threshold-autoregression.html#numerical-method-bootstrap-resampling"><i class="fa fa-check"></i><b>8.3.3</b> Numerical Method: Bootstrap Resampling<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="forecast-assessment.html#forecast-assessment">Forecast Assessment<span></span></a></li>
<li class="chapter" data-level="9" data-path="forecast-evaluation.html"><a href="forecast-evaluation.html"><i class="fa fa-check"></i><b>9</b> Forecast Evaluation<span></span></a>
<ul>
<li class="chapter" data-level="9.1" data-path="forecast-evaluation.html"><a href="forecast-evaluation.html#the-need-for-the-forecast-evaluation"><i class="fa fa-check"></i><b>9.1</b> The Need for the Forecast Evaluation<span></span></a></li>
<li class="chapter" data-level="9.2" data-path="forecast-evaluation.html"><a href="forecast-evaluation.html#relative-forecast-accuracy-tests"><i class="fa fa-check"></i><b>9.2</b> Relative Forecast Accuracy Tests<span></span></a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="forecast-evaluation.html"><a href="forecast-evaluation.html#the-morgan-granger-newbold-test"><i class="fa fa-check"></i><b>9.2.1</b> The Morgan-Granger-Newbold Test<span></span></a></li>
<li class="chapter" data-level="9.2.2" data-path="forecast-evaluation.html"><a href="forecast-evaluation.html#the-diebold-mariano-test"><i class="fa fa-check"></i><b>9.2.2</b> The Diebold-Mariano Test<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="forecast-combination.html"><a href="forecast-combination.html"><i class="fa fa-check"></i><b>10</b> Forecast Combination<span></span></a></li>
<li><a href="tutorial-1-introduction-to-r.html#tutorial-1-introduction-to-r">Tutorial 1: Introduction to R<span></span></a>
<ul>
<li><a href="tutorial-1-introduction-to-r.html#data-management">Data Management<span></span></a></li>
<li><a href="tutorial-1-introduction-to-r.html#data-visualisation">Data Visualisation<span></span></a></li>
<li><a href="tutorial-1-introduction-to-r.html#regression-analysis">Regression Analysis<span></span></a></li>
</ul></li>
<li><a href="tutorial-2-some-r-functions.html#tutorial-2-some-r-functions">Tutorial 2: Some R Functions<span></span></a></li>
<li><a href="tutorial-3-forecasting-methods-and-routines.html#tutorial-3-forecasting-methods-and-routines">Tutorial 3: Forecasting Methods and Routines<span></span></a></li>
<li><a href="tutorial-4-deterministic-trends.html#tutorial-4-deterministic-trends">Tutorial 4: Deterministic Trends<span></span></a></li>
<li><a href="tutorial-5-seasonality.html#tutorial-5-seasonality">Tutorial 5: Seasonality<span></span></a></li>
<li><a href="tutorial-6-linear-autoregression.html#tutorial-6-linear-autoregression">Tutorial 6: Linear Autoregression<span></span></a></li>
<li><a href="tutorial-7-vector-autoregression.html#tutorial-7-vector-autoregression">Tutorial 7: Vector Autoregression<span></span></a></li>
<li><a href="tutorial-8-threshold-autoregression.html#tutorial-8-threshold-autoregression">Tutorial 8: Threshold Autoregression<span></span></a></li>
<li><a href="tutorial-9-forecast-evaluation.html#tutorial-9-forecast-evaluation">Tutorial 9: Forecast Evaluation<span></span></a></li>
<li><a href="tutorial-10-forecast-combination.html#tutorial-10-forecast-combination">Tutorial 10: Forecast Combination<span></span></a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Time Series Modeling and Forecasting with Applications in R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-autoregression" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">Chapter 6</span> Linear Autoregression<a href="linear-autoregression.html#linear-autoregression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Economic time series are often characterized by stochastic cycles. A cycle is a pattern of periodic fluctuations, not contained within a calendar year. A stochastic cycle is one generated by random variables. In general terms, the process is given by:
<span class="math display">\[Y_t = f(Y_{t-1},Y_{t-2},\ldots;\mathbf{\theta})+\varepsilon_t.\;~~t=1,\ldots,T\]</span></p>
<p>An autoregressive process (or, simply, an autoregression) is a regression in which the dependent variable and the regressors belong to the same stochastic process.</p>
<p>An autoregression of order <span class="math inline">\(p\)</span>, denoted as <span class="math inline">\(AR(p)\)</span>, has the following functional form:
<span class="math display">\[y_t = \alpha + \beta_1 y_{t-1}+\beta_2 y_{t-2}+ \cdots + \beta_p y_{t-p}+\varepsilon_t\]</span></p>
<p>The sum of the autoregressive parameters, <span class="math inline">\(\beta_1,\ldots,\beta_p\)</span>, depicts the persistence of the series. The larger is the persistence (i.e., closer it is to one), the longer it takes for the effect of a shock to dissolve. The effect will, eventually, dissolve so long as the series are covariance-stationary.</p>
<p>The autocorrelation, <span class="math inline">\(\rho\)</span>, and partial autocorrelation, <span class="math inline">\(\pi\)</span>, functions of the covariance-stationary <span class="math inline">\(AR(p)\)</span> process have the following distinctive features:</p>
<ul>
<li><span class="math inline">\(\rho_1 = \pi_1\)</span>, and <span class="math inline">\(\pi_p = \beta_p\)</span>.</li>
<li>The values of <span class="math inline">\(\beta_1,\ldots,\beta_p\)</span> determine the shape of the autocorrelation function (ACF); in any case, the smaller (in absolute terms) is the persistence measure, the faster the ACF decays toward zero.</li>
<li>The partial autocorrelation function (PACF) is characterized by “statistically significant” first <span class="math inline">\(p\)</span> spikes <span class="math inline">\(\pi_1 \neq 0,\ldots,\pi_p \neq 0\)</span>, and the remaining <span class="math inline">\(\pi_k = 0\)</span>, <span class="math inline">\(\forall k &gt; p\)</span>.</li>
</ul>
<div id="modeling-2" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> Modeling<a href="linear-autoregression.html#modeling-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="ar1" class="section level3 hasAnchor" number="6.1.1">
<h3><span class="header-section-number">6.1.1</span> AR(1)<a href="linear-autoregression.html#ar1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The first-order autoregression is given by: <span class="math display">\[y_t = \alpha + \beta_1 y_{t-1} + \varepsilon_t,\]</span> where <span class="math inline">\(\alpha\)</span> is a constant term; <span class="math inline">\(\beta_1\)</span> is the <em>persistence</em> parameter; and <span class="math inline">\(\varepsilon_t\)</span> is a white noise process.</p>
<p>A necessary and sufficient condition for an <span class="math inline">\(AR(1)\)</span> process to be covariance stationary is that <span class="math inline">\(|\beta_1| &lt; 1\)</span>. We can see this by substituting recursively the lagged equations into the lagged dependent variables:
<span class="math display">\[
\begin{aligned}
y_t &amp;= \alpha + \beta_1 y_{t-1} + \varepsilon_t \notag \\
y_t &amp;= \alpha + \beta_1 (\alpha + \beta_1 y_{t-2} + \varepsilon_{t-1}) + \varepsilon_t \notag \\
&amp;= \alpha(1+\beta_1) + \beta_1^2 (\alpha + \beta_1 y_{t-3} + \varepsilon_{t-2}) + \beta_1\varepsilon_{t-1} + \varepsilon_t \notag \\
&amp;\vdots  \notag \\
&amp;= \alpha\sum_{i=0}^{k-1}\beta_1^i + \beta_1^k y_{t-k} + \sum_{i=0}^{k-1}\beta_1^i\varepsilon_{t-i}
\end{aligned}
\]</span>
The end-result is a general linear process with geometrically declining coefficients. Here, <span class="math inline">\(|\beta_1| &lt; 1\)</span> is required for convergence.</p>
<p>Assuming <span class="math inline">\(|\beta_1| &lt; 1\)</span>, as <span class="math inline">\(k \to \infty\)</span> the process converges to: <span class="math display">\[y_t = \frac{\alpha}{1-\beta_1} + \sum_{i=0}^{\infty}\beta_1^i\varepsilon_{t-i}\]</span></p>
<p>The <em>unconditional mean</em> of this process is: <span class="math display">\[\mu = E\left(y_t\right) = E\left(\frac{\alpha}{1-\beta_1} + \sum_{i=0}^{\infty}\beta_1^i\varepsilon_{t-i}\right) = \frac{\alpha}{1-\beta_1}\]</span></p>
<p>The <em>unconditional variance</em> of this process is: <span class="math display">\[\gamma_0 = Var\left(y_t\right) = Var\left(\frac{\alpha}{1-\beta_1} + \sum_{i=0}^{\infty}\beta_1^i\varepsilon_{t-i}\right) = \frac{\sigma_{\varepsilon}^2}{1-\beta_1^2}\]</span></p>
<p>The <em>Autocovariance</em> is simply the covariance between <span class="math inline">\(y_t\)</span> and <span class="math inline">\(y_{t-k}\)</span>, that is: <span class="math display">\[\gamma_k = Cov(y_t,y_{t-k}) = E[(y_t - \mu)(y_{t-k} - \mu)] = E(y_t y_{t-k}) - \mu^2\]</span></p>
<p>Some algebraic manipulation can help us show that: <span class="math display">\[\gamma_k = \beta_1\gamma_{k-1},\]</span> and that: <span class="math display">\[\rho_{k} = \beta_1\rho_{k-1}\]</span> (recall, <span class="math inline">\(\rho_k = \gamma_k/\gamma_0\)</span> is the autocorrelation coefficient).</p>
<p>In fact, for AR(1), an autocorrelation coefficient of some lag can be represented as the autoregression parameter (which in this instance is equivalent to the persistence measure) to that power. That is:
<span class="math display">\[
\begin{aligned}
\rho_1 &amp;= \beta_1\rho_0 = \beta_1 \notag \\
\rho_2 &amp;= \beta_1\rho_1 = \beta_1^2 \notag \\
&amp;\vdots \notag \\
\rho_k &amp;= \beta_1\rho_{k-1} = \beta_1^k
\end{aligned}
\]</span></p>
<p>It follows that the autocorrelation function of a covariance stationary AR(1) is a geometric decay; the smaller is <span class="math inline">\(|\beta_1|\)</span> the more rapid is the decay.</p>
<p>By imposing certain restrictions, the AR(1) will reduce to other already known models:</p>
<ul>
<li>If <span class="math inline">\(\beta_1 = 0\)</span>, <span class="math inline">\(y_t\)</span> is equivalent to a white noise.</li>
<li>If <span class="math inline">\(\beta_1 = 1\)</span> and <span class="math inline">\(\alpha = 0\)</span>, <span class="math inline">\(y_t\)</span> is a random walk.</li>
<li>If <span class="math inline">\(\beta_1 = 1\)</span> and <span class="math inline">\(\alpha \neq 0\)</span>, <span class="math inline">\(y_t\)</span> is a random walk with drift.</li>
</ul>
<p>In general, a smaller persistence parameter results in a quicker adjustment to the <em>unconditional mean</em> of the process.</p>
<p>The autocorrelation and partial autocorrelation functions of the AR(1) process have three distinctive features:</p>
<ul>
<li><span class="math inline">\(\rho_1 = \pi_1 = \beta_1\)</span>. That is, the persistence parameter is also the autocorrelation and the partial autocorrelation coefficient.</li>
<li>The autocorrelation function decreases exponentially toward zero, and the decay is faster when the persistence parameter is smaller.</li>
<li>The partial autocorrelation function is characterized by only one spike <span class="math inline">\(\pi_1 \neq 0\)</span>, and the remaining <span class="math inline">\(\pi_k = 0\)</span>, <span class="math inline">\(\forall k &gt; 1\)</span>.</li>
</ul>
<p>To illustrate the foregoing, let’s generate a series of 100 observations that follow the process: <span class="math inline">\(y_t=0.8y_{t-1}+\varepsilon_t\)</span>, where <span class="math inline">\(y_0=0\)</span> and <span class="math inline">\(\varepsilon\sim N(0,1)\)</span>, and plot the ACF and PACF of this series.</p>
<p><img src="forecasting_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
</div>
<div id="ar2" class="section level3 hasAnchor" number="6.1.2">
<h3><span class="header-section-number">6.1.2</span> AR(2)<a href="linear-autoregression.html#ar2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now consider the second-order autoregression: <span class="math display">\[y_t = \alpha + \beta_1 y_{t-1} + \beta_2 y_{t-2} + \varepsilon_t\]</span></p>
<p>where <span class="math inline">\(\alpha\)</span> is a constant term; <span class="math inline">\(\beta_1+\beta_2\)</span> is the persistence measure; and <span class="math inline">\(\varepsilon_t\)</span> is a white noise process.</p>
<p>In what follows, the necessary (1 and 2) and sufficient (3 and 4) conditions for an <span class="math inline">\(AR(2)\)</span> process to be covariance stationary are:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(|\beta_2| &lt; 1\)</span></li>
<li><span class="math inline">\(|\beta_1| &lt; 2\)</span></li>
<li><span class="math inline">\(\beta_1 + \beta_2 &lt; 1\)</span></li>
<li><span class="math inline">\(\beta_2 - \beta_1 &lt; 1\)</span></li>
</ol>
<p>The autocorrelation functions of the AR(2) process have the following distinctive features:</p>
<ul>
<li><span class="math inline">\(\rho_1 = \pi_1\)</span> (which is true for any <span class="math inline">\(AR(p)\)</span> process), and <span class="math inline">\(\pi_2 = \beta_2\)</span>.</li>
<li>The autocorrelation function decreases toward zero. The path, however, varies depending on the values of <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span>. Nonetheless, the decay is faster when the persistence measure is smaller.</li>
<li>The partial autocorrelation function is characterized by only two spikes <span class="math inline">\(\pi_1 \neq 0\)</span> and <span class="math inline">\(\pi_2 \neq 0\)</span>, and the remaining <span class="math inline">\(\pi_k = 0\)</span>, <span class="math inline">\(\forall k &gt; 2\)</span>.</li>
</ul>
<p>Again, to illustrate, let’s generate a series of 100 observations that follow the process: <span class="math inline">\(y_t=1.1y_{t-1}-0.4y_{t-2}+\varepsilon_t\)</span>, where <span class="math inline">\(y_{-1}=y_0=0\)</span> and <span class="math inline">\(\varepsilon\sim N(0,1)\)</span>, and plot the ACF and PACF of this series.</p>
<p><img src="forecasting_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
</div>
</div>
<div id="forecasting-2" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> Forecasting<a href="linear-autoregression.html#forecasting-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Making forecasts for some future period, <span class="math inline">\(t+h\)</span>, from an AR(p) model that has been fit to the data up to and including period <span class="math inline">\(t+h-1\)</span> can be a straightforward exercise, so long as we have access to the relevant information set. For one-step-ahead forecasts, the information set is readily available. For multi-step-ahead forecasts, we need to ‘come up’ with the value of the variable that has not been realized yet. For example, when making a two-step-ahead forecast for period <span class="math inline">\(t+2\)</span>, we need data from period <span class="math inline">\(t+1\)</span>, which is not available at the time when the forecast is made. Instead, we need to use our forecast for period <span class="math inline">\(t+1\)</span>. The same applies to forecasts for any subsequent periods in the future. This approach is known as an <em>iterative</em> method of forecasting, wherein we make forecast for some period using the available data, then iterate forward by one period and use the most recent forecast to make the next period’s forecast, and so on and so forth.</p>
<div id="one-step-ahead-forecast-from-ar1" class="section level3 hasAnchor" number="6.2.1">
<h3><span class="header-section-number">6.2.1</span> One-step-ahead forecast from AR(1)<a href="linear-autoregression.html#one-step-ahead-forecast-from-ar1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The realization of the random variable in period <span class="math inline">\(t+1\)</span> is: <span class="math display">\[y_{t+1} = \alpha + \beta_1 y_{t} + \varepsilon_{t+1}\]</span></p>
<p>The optimal one-step-ahead forecast is: <span class="math display">\[y_{t+1|t} = E(y_{t+1}|\Omega_t) = E(\alpha + \beta_1 y_{t} + \varepsilon_{t+1}) = \alpha + \beta_1 y_{t}\]</span></p>
<p>The one-step-ahead forecast error is: <span class="math display">\[e_{t+1|t} = y_{t+1} - y_{t+1|t} = \alpha + \beta_1 y_t + \varepsilon_{t+1} - (\alpha + \beta_1 y_t) = \varepsilon_{t+1}\]</span></p>
<p>The one-step-ahead forecast variance: <span class="math display">\[\sigma_{t+1|t}^2 = Var(y_{t+1}|\Omega_t) = E(e_{t+1|t}^2) = E(\varepsilon_{t+1}^2) = \sigma_{\varepsilon}^2\]</span></p>
<p>The one-step-ahead (95%) interval forecast: <span class="math display">\[y_{t+1|t} \pm z_{.025}\sigma_{t+1|t} = y_{t+1|t} \pm 1.96\sigma_{\varepsilon}\]</span></p>
</div>
<div id="two-step-ahead-forecast-from-ar1" class="section level3 hasAnchor" number="6.2.2">
<h3><span class="header-section-number">6.2.2</span> Two-step-ahead forecast from AR(1)<a href="linear-autoregression.html#two-step-ahead-forecast-from-ar1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The realization of the random variable in period <span class="math inline">\(t+2\)</span> is: <span class="math display">\[y_{t+2} = \alpha + \beta_1 y_{t+1} + \varepsilon_{t+2}\]</span></p>
<p>The optimal two-step-ahead forecast is: <span class="math display">\[y_{t+2|t} = E(y_{t+2}|\Omega_t) = E(\alpha + \beta_1 y_{t+1} + \varepsilon_{t+2}) = \alpha(1+\beta_1) + \beta_1^2 y_t\]</span></p>
<p>Note, that here we substituted <span class="math inline">\(y_{t+1}\)</span> with <span class="math inline">\(\alpha + \beta_1 y_t + \varepsilon_{t+1}\)</span>.</p>
<p>The two-step-ahead forecast error is:
<span class="math display">\[\begin{aligned}
e_{t+2|t} &amp;= y_{t+2} - y_{t+2|t} \\
&amp;= \alpha(1+\beta_1) + \beta_1^2 y_t + \beta_1\varepsilon_{t+1} + \varepsilon_{t+2} - [\alpha(1+\beta_1) + \beta_1^2 y_t] \\
&amp;= \beta_1\varepsilon_{t+1} + \varepsilon_{t+2}
\end{aligned}\]</span></p>
<p>The two-step-ahead forecast variance is:
<span class="math display">\[\begin{aligned}
\sigma_{t+2|t}^2 &amp;= Var(y_{t+2}|\Omega_t) \\
&amp;= E(e_{t+2|t}^2) = E(\beta_1\varepsilon_{t+1} + \varepsilon_{t+2})^2 = \sigma_{\varepsilon}^2(1+\beta_1^2)
\end{aligned}\]</span></p>
<p>The two-step-ahead (95%) interval forecast: <span class="math display">\[y_{t+2|t} \pm z_{.025}\sigma_{t+2|t} = y_{t+2|t} \pm 1.96\sigma_{\varepsilon}\sqrt{1+\beta_1^2}\]</span></p>
</div>
<div id="h-step-ahead-forecast-from-ar1" class="section level3 hasAnchor" number="6.2.3">
<h3><span class="header-section-number">6.2.3</span> h-step-ahead forecast from AR(1)<a href="linear-autoregression.html#h-step-ahead-forecast-from-ar1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The realization of the random variable in period <span class="math inline">\(t+h\)</span> is: <span class="math display">\[y_{t+h} = \alpha + \beta_1 y_{t+h-1} + \varepsilon_{t+h}\]</span></p>
<p>The optimal h-step-ahead forecast: <span class="math display">\[y_{t+h|t} = E(y_{t+h}|\Omega_t) = E(\alpha + \beta_1 y_{t+h-1} + \varepsilon_{t+1}) = \alpha\textstyle\sum_{j=0}^{h-1}\beta_1^j + \beta_1^h y_t\]</span></p>
<p>The h-step-ahead forecast error: <span class="math display">\[e_{t+h|t} = y_{t+h} - y_{t+h|t} = \textstyle\sum_{j=0}^{h-1}\beta_1^j\varepsilon_{t+h-j}\]</span></p>
<p>The h-step-ahead forecast variance: <span class="math display">\[\sigma_{t+h|t}^2 = Var(y_{t+h}|\Omega_t) = E(e_{t+h|t}^2) = \sigma_{\varepsilon}^2\textstyle\sum_{j=0}^{h-1}\beta_1^{2j}\]</span></p>
<p>The h-step-ahead (95%) interval forecast: <span class="math display">\[y_{t+h|t} \pm z_{.025}\sigma_{t+h|t} = y_{t+1|t} \pm 1.96\sigma_{\varepsilon}\sqrt{\textstyle\sum_{j=0}^{h-1}\beta_1^{2j}}\]</span></p>
<p>If the series represent a covariance-stationary process, i.e. when <span class="math inline">\(|\beta_1| &lt; 1\)</span>, as <span class="math inline">\(h \to \infty\)</span>:</p>
<p>The optimal point forecast converges to: <span class="math display">\[y_{t+h|t} = \frac{\alpha}{1-\beta_1}\]</span></p>
<p>The forecast variance converges to: <span class="math display">\[\sigma_{t+h|t}^2 = \frac{\sigma_{\varepsilon}^2}{1-\beta_1^2}\]</span></p>
<p>The (95%) interval forecast converges to: <span class="math display">\[y_{t+h|t} \pm z_{.025}\sigma_{t+h|t} = \frac{\alpha}{1-\beta_1} \pm 1.96\frac{\sigma_{\varepsilon}}{\sqrt{1-\beta_1^2}}\]</span></p>
</div>
<div id="one-step-ahead-forecast-from-ar2" class="section level3 hasAnchor" number="6.2.4">
<h3><span class="header-section-number">6.2.4</span> One-step-ahead forecast from AR(2)<a href="linear-autoregression.html#one-step-ahead-forecast-from-ar2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The realization of the random variable in period <span class="math inline">\(t+1\)</span> is: <span class="math display">\[y_{t+1} = \alpha + \beta_1 y_{t} + \beta_2 y_{t-1} + \varepsilon_{t+1}\]</span></p>
<p>The optimal one-step-ahead forecast:
<span class="math display">\[\begin{aligned}
y_{t+1|t} &amp;= E(y_{t+1}|\Omega_t) \\
&amp;= E(\alpha + \beta_1 y_{t} + \beta_2 y_{t-1} + \varepsilon_{t+1}) = \alpha + \beta_1 y_{t} + \beta_2 y_{t-1}
\end{aligned}\]</span></p>
<p>The one-step-ahead forecast error:
<span class="math display">\[\begin{aligned}
e_{t+1|t} &amp;= y_{t+1} - y_{t+1|t} \\
&amp;= \alpha + \beta_1 y_t + \beta_2 y_{t-1} + \varepsilon_{t+1} - (\alpha + \beta_1 y_t + \beta_2 y_{t-1}) = \varepsilon_{t+1}
\end{aligned}\]</span></p>
<p>The one-step-ahead forecast variance: <span class="math display">\[\sigma_{t+1|t}^2 = Var(y_{t+1}|\Omega_t) = E(e_{t+1|t}^2) = E(\varepsilon_{t+1}^2) = \sigma_{\varepsilon}^2\]</span></p>
<p>The one-step-ahead (95%) interval forecast: <span class="math display">\[y_{t+1|t} \pm z_{.025}\sigma_{t+1|t} = y_{t+1|t} \pm 1.96\sigma_{\varepsilon}\]</span></p>
</div>
<div id="two-step-ahead-forecast-from-ar2" class="section level3 hasAnchor" number="6.2.5">
<h3><span class="header-section-number">6.2.5</span> Two-step-ahead forecast from AR(2)<a href="linear-autoregression.html#two-step-ahead-forecast-from-ar2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The realization of the random variable in period <span class="math inline">\(t+2\)</span> is: <span class="math display">\[y_{t+2} = \alpha + \beta_1 y_{t+1} + \beta_2 y_{t} + \varepsilon_{t+2}\]</span></p>
<p>The optimal two-step-ahead forecast:
<span class="math display">\[\begin{aligned}
y_{t+2|t} = E(y_{t+2}|\Omega_t) &amp;= E(\alpha + \beta_1 y_{t+1} + \beta_2 y_{t} + \varepsilon_{t+1}) \\
&amp;= \alpha(1+\beta_1) + (\beta_1^2+\beta_2) y_{t} + \beta_1\beta_2 y_{t-1}
\end{aligned}\]</span></p>
<p>The two-step-ahead forecast error:
<span class="math display">\[\begin{aligned}
e_{t+2|t} = y_{t+2} - y_{t+2|t} =&amp; \alpha + \beta_1 y_{t+1} + \beta_2 y_{t} + \varepsilon_{t+2} \\
&amp;- (\alpha + \beta_1 y_{t+1|t} + \beta_2 y_{t}) = \beta_1\varepsilon_{t+1} + \varepsilon_{t+2}
\end{aligned}\]</span></p>
<p>The two-step-ahead forecast variance:
<span class="math display">\[\sigma_{t+2|t}^2 = Var(y_{t+2}|\Omega_t) = E(e_{t+2|t}^2) = E(\beta_1\varepsilon_{t+1} + \varepsilon_{t+2})^2 = \sigma_{\varepsilon}^2(1+\beta_1^2)\]</span></p>
<p>The two-step-ahead (95%) interval forecast: <span class="math display">\[y_{t+2|t} \pm z_{.025}\sigma_{t+2|t} = y_{t+2|t} \pm 1.96\sigma_{\varepsilon}\sqrt{1+\beta_1^2}\]</span></p>
</div>
<div id="one-step-ahead-forecast-from-ar2-1" class="section level3 hasAnchor" number="6.2.6">
<h3><span class="header-section-number">6.2.6</span> One-step-ahead forecast from AR(2)<a href="linear-autoregression.html#one-step-ahead-forecast-from-ar2-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The realization of the random variable in period <span class="math inline">\(t+h\)</span> is: <span class="math display">\[y_{t+h} = \alpha + \beta_1 y_{t+h-1} + \beta_2 y_{t+h-2} + \varepsilon_{t+h}\]</span></p>
<p>The optimal h-step-ahead forecast (iterated method):
<span class="math display">\[\begin{aligned}
y_{t+1|t} &amp;= \alpha + \beta_1 y_t + \beta_2 y_{t-1} \\
y_{t+2|t} &amp;= \alpha + \beta_1 y_{t+1|t} + \beta_2 y_{t} \\
y_{t+3|t} &amp;= \alpha + \beta_1 y_{t+2|t} + \beta_2 y_{t+1|t} \\
&amp;\vdots \\
y_{t+h|t} &amp;= \alpha + \beta_1 y_{t+h-1|t} + \beta_2 y_{t+h-2|t}
\end{aligned}\]</span></p>
<p>The h-step-ahead forecast error: <span class="math display">\[e_{t+h|t} = y_{t+h} - y_{t+h|t} = \varepsilon_{t+h}+\beta_1 e_{t+h-1|t}+\beta_2 e_{t+h-2|t}\]</span></p>
<p>The h-step-ahead forecast variance:
<span class="math display">\[\begin{aligned}
\sigma_{t+h|t}^2 &amp;= Var(y_{t+h}|\Omega_t) = E(e_{t+h|t}^2) \\
&amp;= \sigma_{\varepsilon}^2+\beta_1^2 Var(e_{t+h-1|t})+\beta_2^2 Var(e_{t+h-2|t}) \\
&amp;+2\beta_1\beta_2Cov(e_{t+h-1|t},e_{t+h-2|t})
\end{aligned}\]</span></p>
<p>(Note: the formulas for <span class="math inline">\(\sigma_{t+1|t}^2,\sigma_{t+2|t}^2,\ldots,\sigma_{t+h|t}^2\)</span> are the same for any <span class="math inline">\(AR(p)\)</span>, <span class="math inline">\(p \geq h-1\)</span>).</p>
<p>The h-step-ahead (95%) interval forecast: <span class="math display">\[y_{t+h|t} \pm z_{.025}\sigma_{t+h|t} = y_{t+1|t} \pm 1.96\sigma_{t+h|t}\]</span></p>
<p>The optimal h-step-ahead forecast: <span class="math display">\[y_{t+h|t} = E(y_{t+h}|\Omega_t) = \alpha + \beta_1 y_{t+h-1|t} + \beta_2 y_{t+h-2|t} + \cdots + \beta_p y_{t+h-p|t}\]</span></p>
<p>The h-step-ahead forecast error: <span class="math display">\[e_{t+h|t} = \varepsilon_{t+h} + \beta_1 e_{t+h-1|t} + \beta_2 e_{t+h-2|t} + \cdots + \beta_p e_{t+h-p|t}\]</span></p>
<p>The h-step-ahead forecast variance:
<span class="math display">\[\begin{aligned}
\sigma_{t+h|t}^2 &amp; = Var(y_{t+h}|\Omega_t) = E(e_{t+2|t}^2) \\
&amp;= \sigma_{\varepsilon}^2 + \sum_{i=1}^{p}\beta_i^2 Var(e_{t+h-i|t}) + 2\sum_{i \neq j}\beta_i\beta_j Cov(e_{t+h-i|t},e_{t+h-j|t})
\end{aligned}\]</span></p>
<p>The h-step-ahead (95%) interval forecast: <span class="math display">\[y_{t+h|t} \pm z_{.025}\sigma_{t+h|t} = y_{t+h|t} \pm 1.96\sigma_{t+h|t}\]</span></p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="dynamic-time-series-models.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="vector-autoregression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["forecasting.pdf", "forecasting.epub", "forecasting.mobi"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
